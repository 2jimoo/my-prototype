{"doc_id": 281401, "author": "Craig Lowe", "text": "I think this is much more complex than a single answer. There are many different types of Bluetooth connections, not all of these connections are interchangeable. It also depends heavily upon which version of Bluetooth is being used in the devices, both the reciever for the PC and the controller. As you can see from the Bluetooth 4.2 specification, which is 2,772 pages, it is quite complex. Bluetooth 4.2 specification document: https://www.bluetooth.org/DocMan/handlers/DownloadDoc.ashx?doc_id=286439 I doubt a definitive answer can be given to this question as to all Bluetooth devices in all scenarios. I would however in this case as it's quite a simple scenario, purchase a single Bluetooth adapter and try to connect four controllers. I suspect that it will work from what I have deduced from the Bluetooth specification but I would ensure that you purchase a 4.2 Bluetooth adapter. Apple Website FAQ The official Bluetooth specifications state seven is the maximum number of Bluetooth devices that can be connected at once. However, three to four devices is a practical limit, depending on the types of devices and profiles are used. Some devices require more Bluetooth data, so they are more demanding than other devices. Data-intensive devices may reduce the total number of devices that can be active at the same time. If a Bluetooth device becomes slow to connect or does not perform reliably, reduce the total number of connected devices. Apple Website Bluetooth FAQ As can be seen from the official Apple website they state that the maximum number of connected devices is 7. The official Bluetooth spec says that up to 255 sleeping devices can be connected. This would indicate that the maximum number of parallel active connections is 7, however due to the complexity of the connections and amount of data to be transferred in most scenarios the practical limit is 3 or 4. Bluetooth signal can also be effected largely by other household devices such as Wifi, Microwaves and Household Phones. A true answer to this would be difficult to come by in relation to your exact scenario due to the large number of variables, however we can see that theoretically seven devices can have active connections in parallel and as reported by Apple the practical limit in the majority of scenarios is 3 or 4."}
{"doc_id": 281572, "author": "oldmud0", "text": "Before I begin answering the question, think about this: if DualShock uses Bluetooth, then how does the PlayStation establish a connection to all 4 controllers with a single radio? I use Scarlet.Crush's SCP drivers, and they seem to have great Bluetooth support (I use regular USB). They require sacrificing one Bluetooth dongle to install SCP drivers on it, which replace the original Bluetooth drivers/stack (meaning the dongle will be dedicated to connecting DualShock controllers and nothing more). The fact that plural in \"controllers\" is used frequently in the documentation confirms that you are able to connect all four controllers into a single dongle. Due to the controller's hardware limitations (4 lights) and SCP's own limitations as a result, you can only have up to four controllers connected at once. Since a concurrent connection to 4 devices are supported by a majority of Bluetooth radios (the forum post above lists them out) and this does not even come near to the spec's hard limit of 7, there is no doubt you will be able to connect all 4 controllers with a single dongle, regardless of whether they are DS3 or DS4, given that everything is correctly configured and paired up."}
{"doc_id": 282024, "author": "qasdfdsaq", "text": "The internet is filled with conflicting information because a lot of it applies to different scenarios. General background: Firstly, there's a paired device and a connected device and then an actively communicating device. Your links above confuse these and no distinction, but there is a difference in functionality. A device can be paired, but not connected. Similarly, it can be connected but not transmitting. Think of a paired device as a saved wireless network that you're not connected to. Then there are host devices and client devices. This is a generalization, but consider PCs, mobile phones, and tablets (and consoles) to be hosts, and headsets, controllers, mice, keyboards, etc. to be clients. Then there's profiles, a profile indicating a type of connection (e.g. audio, HID, etc.) With that in mind, the following apply: In general, host devices support up to 7 simultaneously connected devices and a practically unlimited number of paired. Host devices are like a wireless router - you can connect many different devices at a time. Client devices typically support a limited number of pairings, anywhere between 1 and 5, and only one single connection. They act like a wireless client - you can save many different networks but only connect to one at a time. Some profiles only support one connection at a time on some devices - for example, some Bluetooth speakers can only connect to one computer at a time. A phone for example usually can only connect to one HSP (headset) at a time but can connect multiple HIDs (keyboards, mice, etc.). Also, to explain/clarify some of your links/references: All Bluetooth adapters can support 7 devices, period. Mostly true - the standards only allow up to seven simultaneous connections on a normal device. But you can pair any number of devices. You can hook up 7 devices, but only if they're all different types of devices. False, but in some scenarios, you cannot simultaneously use two devices with the same Bluetooth profile. This generally applies to audio devices only (i.e. headsets) which may be all that some people are familiar with. For example, a phone can connect to one headset, or one music stream, or one of each, but not two headsets or two speakers. A PC can, however, connect to many phones at a time. You can hook up unlimited devices, no restrictions. (source - a Bluetooth Dongle tech-support) False. You can usually pair unlimited devices on a host, but you cannot connect to them all simultaneously (again, think of the number of saved wireless networks, that aren't all connected at the same time) Only devices that support \"multipoint functionality\" can have multiple hooked up at once, and the choice of adapter doesn't make a difference. Partly true. This only applies to client devices, such as headsets, controllers, keyboards, etc.. These limited devices can only connect to one host at a time if they do not have multipoint. A host can accept multiple clients even if those clients do not support multipoint. Only Bluetooth 4.1 adapters support multiple devices. False. No idea where he got this from, it's plain rubbish. You can only use one device per Bluetooth adapter. False. Pretty much everything in that entire statement is wrong. So the final answer is yes, you can connect four controllers (clients) to one adapter (host) at a time. Even without the above information, it's fairly obvious as per @oldmud0's answer, if [controller] uses Bluetooth, then how does the PlayStation establish a connection to all 4 controllers with a single radio [if Bluetooth didn't support it]?"}
{"doc_id": 281401, "author": "Craig Lowe", "text": "I think this is much more complex than a single answer. There are many different types of Bluetooth connections, not all of these connections are interchangeable. It also depends heavily upon which version of Bluetooth is being used in the devices, both the reciever for the PC and the controller. As you can see from the Bluetooth 4.2 specification, which is 2,772 pages, it is quite complex. Bluetooth 4.2 specification document: https://www.bluetooth.org/DocMan/handlers/DownloadDoc.ashx?doc_id=286439 I doubt a definitive answer can be given to this question as to all Bluetooth devices in all scenarios. I would however in this case as it's quite a simple scenario, purchase a single Bluetooth adapter and try to connect four controllers. I suspect that it will work from what I have deduced from the Bluetooth specification but I would ensure that you purchase a 4.2 Bluetooth adapter. Apple Website FAQ The official Bluetooth specifications state seven is the maximum number of Bluetooth devices that can be connected at once. However, three to four devices is a practical limit, depending on the types of devices and profiles are used. Some devices require more Bluetooth data, so they are more demanding than other devices. Data-intensive devices may reduce the total number of devices that can be active at the same time. If a Bluetooth device becomes slow to connect or does not perform reliably, reduce the total number of connected devices. Apple Website Bluetooth FAQ As can be seen from the official Apple website they state that the maximum number of connected devices is 7. The official Bluetooth spec says that up to 255 sleeping devices can be connected. This would indicate that the maximum number of parallel active connections is 7, however due to the complexity of the connections and amount of data to be transferred in most scenarios the practical limit is 3 or 4. Bluetooth signal can also be effected largely by other household devices such as Wifi, Microwaves and Household Phones. A true answer to this would be difficult to come by in relation to your exact scenario due to the large number of variables, however we can see that theoretically seven devices can have active connections in parallel and as reported by Apple the practical limit in the majority of scenarios is 3 or 4."}
{"doc_id": 281572, "author": "oldmud0", "text": "Before I begin answering the question, think about this: if DualShock uses Bluetooth, then how does the PlayStation establish a connection to all 4 controllers with a single radio? I use Scarlet.Crush's SCP drivers, and they seem to have great Bluetooth support (I use regular USB). They require sacrificing one Bluetooth dongle to install SCP drivers on it, which replace the original Bluetooth drivers/stack (meaning the dongle will be dedicated to connecting DualShock controllers and nothing more). The fact that plural in \"controllers\" is used frequently in the documentation confirms that you are able to connect all four controllers into a single dongle. Due to the controller's hardware limitations (4 lights) and SCP's own limitations as a result, you can only have up to four controllers connected at once. Since a concurrent connection to 4 devices are supported by a majority of Bluetooth radios (the forum post above lists them out) and this does not even come near to the spec's hard limit of 7, there is no doubt you will be able to connect all 4 controllers with a single dongle, regardless of whether they are DS3 or DS4, given that everything is correctly configured and paired up."}
{"doc_id": 282024, "author": "qasdfdsaq", "text": "The internet is filled with conflicting information because a lot of it applies to different scenarios. General background: Firstly, there's a paired device and a connected device and then an actively communicating device. Your links above confuse these and no distinction, but there is a difference in functionality. A device can be paired, but not connected. Similarly, it can be connected but not transmitting. Think of a paired device as a saved wireless network that you're not connected to. Then there are host devices and client devices. This is a generalization, but consider PCs, mobile phones, and tablets (and consoles) to be hosts, and headsets, controllers, mice, keyboards, etc. to be clients. Then there's profiles, a profile indicating a type of connection (e.g. audio, HID, etc.) With that in mind, the following apply: In general, host devices support up to 7 simultaneously connected devices and a practically unlimited number of paired. Host devices are like a wireless router - you can connect many different devices at a time. Client devices typically support a limited number of pairings, anywhere between 1 and 5, and only one single connection. They act like a wireless client - you can save many different networks but only connect to one at a time. Some profiles only support one connection at a time on some devices - for example, some Bluetooth speakers can only connect to one computer at a time. A phone for example usually can only connect to one HSP (headset) at a time but can connect multiple HIDs (keyboards, mice, etc.). Also, to explain/clarify some of your links/references: All Bluetooth adapters can support 7 devices, period. Mostly true - the standards only allow up to seven simultaneous connections on a normal device. But you can pair any number of devices. You can hook up 7 devices, but only if they're all different types of devices. False, but in some scenarios, you cannot simultaneously use two devices with the same Bluetooth profile. This generally applies to audio devices only (i.e. headsets) which may be all that some people are familiar with. For example, a phone can connect to one headset, or one music stream, or one of each, but not two headsets or two speakers. A PC can, however, connect to many phones at a time. You can hook up unlimited devices, no restrictions. (source - a Bluetooth Dongle tech-support) False. You can usually pair unlimited devices on a host, but you cannot connect to them all simultaneously (again, think of the number of saved wireless networks, that aren't all connected at the same time) Only devices that support \"multipoint functionality\" can have multiple hooked up at once, and the choice of adapter doesn't make a difference. Partly true. This only applies to client devices, such as headsets, controllers, keyboards, etc.. These limited devices can only connect to one host at a time if they do not have multipoint. A host can accept multiple clients even if those clients do not support multipoint. Only Bluetooth 4.1 adapters support multiple devices. False. No idea where he got this from, it's plain rubbish. You can only use one device per Bluetooth adapter. False. Pretty much everything in that entire statement is wrong. So the final answer is yes, you can connect four controllers (clients) to one adapter (host) at a time. Even without the above information, it's fairly obvious as per @oldmud0's answer, if [controller] uses Bluetooth, then how does the PlayStation establish a connection to all 4 controllers with a single radio [if Bluetooth didn't support it]?"}
{"doc_id": 63639, "author": "Marcin", "text": "Case fans usually use 1.8 Watts and are rated at 12 Volts. The molex connector that is usually used on the case fans is 12 Volts by standard (same as your Hard Drive and Optical Drives usually use). Unless you are adding 50 fans I doubt you have to worry about the power supply. You will be fine with the 500 Watt PSU."}
{"doc_id": 63652, "author": "user47298", "text": "Also, be sure of the type of connector ... not all use molex, some use 3-pin to the mobo. But overall, I agree with the 500W PSU. Be sure to plan for the future as well ... any high-end graphics or high-end audio coming? These can draw up 200W peak (and more if you SLI/Crossfire) ... nothing more frustrating than having to keep upgrading the PSU. Good luck!"}
{"doc_id": 314953, "author": "sawdust", "text": "Is DSL digital or analogue? The various flavors of xDSL, or Digital Subscriber Line, all employ sophisticated digital signal processing to transmit digital information over twisted-pair wires. Such signals are simply called \"digital\" signals for convenience. However this is an analog world (unless you're studing sub-atomic particles, where quantum physics take over). A \"digital signal\" is a misnomer; it really means digital information conveyed by an analog waveform. All waveforms have continuous rather than discrete values, and are therefore analog. A waveform cannot be at one discrete voltage level, and then instantly change to another discrete voltage level. A digital signal would only have two levels, e.g. 0 and 1. A state of 1/2 is never permitted. But it's impossible to generate such a signal in this analog world. Typically a combination of amplitude, phase, and frequency are employed to modulate digital information into an analog waveform. The simplest \"digital\" signal, the logic signal (e.g. as used in TTL), uses just amplitude modulation to represent logic levels/states 0 and 1. The logic states are represented by, not specific voltage levels as you would expect for a true digital values, but, as a concession to the analog world that the signal has to operate in, voltage ranges (i.e. a continuum) for each logic state. Every logic input is a very simple analog-to-digital converter. When the input is sampled (triggered by a clock signal), a sampled voltage in the low-voltage range is interpreted as logic 0. But if the sampled voltage is in the high-voltage range, then a logic 1 is \"read\". All of this analog-to-digital conversion is simply treated as as logic or digital input, and textbooks always use perfect square pulses to represent changes in logic states. ADSL uses both phase and amplitude modulation to transmit digital information as an analog signal. I would of thought you wouldn't be able to have both digital and analogue on the exact same line. Whether a cable/wire/channel can carry more than one signal depends on the signals themselves. \"Digital\" signals can time multiplex (using data frames) so that the medium can be shared; that's how several users can share a single ISP connection to the Internet. But a pure analog signal (e.g. AM radio) is continuous in time, so time multiplexing is not possible. Frequency-division multiplexing can be employed for (analog) signals to share a medium. The (open space) electromagnetic spectrum is divided up into television, radio and communication bands and channels. Channels are used for analog or \"digital\" signals. In the case of ADSL, the voice signal is restricted to 0 to 4 KHertz. The ADSL signal (which is modulated to carrier frequencies > 4KHz) is given the rest of the bandwidth of the twisted-pair cable. Note that the common \"digital\" logic signal (e.g. TTL) uses simple amplitude moulation, and is full bandwidth. (Use Fourier analysis to convert pulses and square waves to sine waves to determine frequency content.) So logic signals are not suitable for frequency-division multiplexing."}
{"doc_id": 314964, "author": "davidgo", "text": "The other answers are good, but complex. Here is a simple one. ADSL is using an analog carrier to encode a digital signal. AN ADSL device includes a Modem (but a much faster and more complex one then the old dial-up ones) - the input to the modem on the sending side is digital, the modem converts it to analog, the remote side converts it back to digital. As far as the splitter goes - think of a modem like a radio. The bottom most frequency (in the case of ADSL its a little over 0 - 64k) is split off by the splitter so it acts like a regular phone line. The ADSL Modem contains the equivalent of a large number of receivers, such that it can send and receive multiple stations simultaneously on the higher frequencies (and one of the reasons speeds differ is some frequencies will work better then others depending on the line) - each of these frequencies requires the digital signal is imposed on a carrier - modulating and turning it to analog, and then the reverse on the other side."}
{"doc_id": 154161, "author": "superuser", "text": "They are names for very different concepts. A VPN is a \"virtual private network\", a piece of software that creates an encrypted communication between two (potentially) far-away computers such that nobody in between can see the contents of the communication. An extranet is something of a made-up term, referring to a web site that a company publishes for the benefit of its vendors, partners, or customers. The name \"extranet\" was derived from \"intranet\", which is a web site that's intended for use only within a company. That term itself was sort of perverted from \"Internet\", which is literally a network of networks (web sites came around long after the Internet per se was created). So an extranet is really a web site with a specific purpose. It's not a technology for setting up communications between computers like a VPN is."}
{"doc_id": 154163, "author": "Logman", "text": "Extranet is outside the firewall, private not public (DMZ), but only open to certain people/companies/oranizations.... https, instead of a vpn client connection. Intranet is inside the firewall, and private, sealed off from the internet. You can use VPN to access the intranet securely though."}
{"doc_id": 271656, "author": "Hamza Mazhar", "text": "A virtual private network (VPN) is a private network of computers linked using a secure “tunnel” connection over a public network such as the Internet. An extranet is a private intranet based on Internet and World Wide Web technology and standards that is accessible to authorized outsiders. A VPN is a method of securing a network, whereas an extranet describes a type of network in terms of its users, in this case, a firm and authorized vendors or partners."}
{"doc_id": 355019, "author": "potatoman", "text": "Yep, having two completely different GPUs in one PC is possible, as long as there are enough PCI slots. However, if you are planning to use SLI, it requires two of the same cards. Furthermore, you should remember not all applications take advantage of the dual GPU setup. Source"}
{"doc_id": 355020, "author": "Journeyman Geek", "text": "Kind of, though it's not that simple. Practically, on more modern versions of Windows, it'll 'just work' for most tasks. For Windows 7, they need to be the same driver (so Nvidia only or AMD only). Windows 10 doesn't care. I've run a third monitor on onboard video with some tweaks on Windows 8 and 10 with the primary monitors being on a separate video card, and that's roughly the worst case scenario. So yeah, you'd be able to drag programs between monitors, and what you're thinking of would be doable. I'd suggest cards of the same brand and roughly similar era (so an 800-1000 series) simply for driver compatibility. With paired video cards, chances are your system will drop down to x8 PCIe lanes from x16, but there's little practical performance loss from this. With some software, you can use a secondary/lesser card as a co-processor for specific tasks or simply pick one as a primary card. Photoshop specifically suggests using one card for your output, and to use identical video cards. Adobe also says: Photoshop currently doesn't take advantage of more than one graphics processor. Using two graphics cards does not enhance Photoshop's performance. Multiple graphics cards with conflicting drivers can cause problems with graphics-processor accelerated features in Photoshop. So in this particular scenario, it's not that useful. You might see some advantages with some CUDA or OpenCL loads but YMMV."}
{"doc_id": 355073, "author": "chx", "text": "While previously you needed to have special, more expensive motherboards running two or more video cards from the same vendor (called Crossfire/SLI by their respective vendors), DirectX 12 allows for multi-display adapter (MDA) where the software (game, mostly) can explicitly work with, as the name suggests, multiple displays (CF/SLI is now called linked display adapter or LDA)). However, to quote one of the earliest benchmarks from 2016 February: Almost no games presently support Dx12, fewer still support this type of multi-GPU configuration, and then there still has to be some dev-side support and testing for that configuration. It's not all just about drivers and APIs; if the game's QA team hasn't run the configuration, there's a good chance more bugs present themselves. While obviously since then more games started supporting DirectX 12, it is still true that few supports MDA and you are more likely to face bugs. If there are N video cards the QA tested then to test all possible pairs they would need to test almost N squared, which very quickly becomes unfeasible. So: you can have two GPUs, in theory they will accelerate some games together, in practice it's a bit more dubious. If you have no need of that and just want to hang a lot of monitors off the same PC, you can just expect that to work to an extent: applications run a specific GPU so stretching one application between monitors hanging off a different card is something unlikely to work. This might be unnecessary as there are gaming cards with six video outputs at a reasonable cost -- much cheaper than workstation cards: the Asus 7970 / 280X (from 2013) and the Asrock Taichi 5700 XT (from 2019), the latter allowing for six 4K monitors."}
{"doc_id": 355075, "author": "bwDraco", "text": "Yes, this can technically work—both cards will give you graphical output. However, different cards cannot be linked together to function as a GPU array (CrossFire or SLI), so you generally won't be able to use them together to render graphics in games. The cards will operate independently of each other. In your case, the second card will be responsible for rendering the graphics on the additional monitors that are attached to it. However, there are other ways you can use the second graphics card: GPU-accelerated compute applications often allow you to select the specific graphics card to use. However, Photoshop doesn't support multiple GPUs for acceleration and (presumably) will only use the first card: Photoshop currently doesn't take advantage of more than one graphics processor. Using two graphics cards does not enhance Photoshop's performance. A few new DirectX 12-based games (e.g. Ashes of the Singularity: Escalation) are capable of using two independent graphics cards to enhance performance. This can, in some cases, enable cards from different vendors to be used together. Games which use NVIDIA PhysX technology can use the second card as a dedicated processor for physics computations, independently of the first card which handles the graphical rendering. This can improve performance in some games. A dedicated PhysX GPU can be configured in the NVIDIA Control Panel."}
{"doc_id": 358802, "author": "Abom", "text": "I use a GTX1070/750ti with two monitors. I just had to make sure I set my display that the 750ti was plugged into as primary or desktop applications wouldn't use my 750ti. I assumed that Chrome and such would use whichever GPU its window was on. Boy, was I wrong. My 750ti no takes care of gpu programs like chrome, twitch, bnet, and my games use my 1070. Noticed a 10-15 fps improvement."}
{"doc_id": 355019, "author": "potatoman", "text": "Yep, having two completely different GPUs in one PC is possible, as long as there are enough PCI slots. However, if you are planning to use SLI, it requires two of the same cards. Furthermore, you should remember not all applications take advantage of the dual GPU setup. Source"}
{"doc_id": 355020, "author": "Journeyman Geek", "text": "Kind of, though it's not that simple. Practically, on more modern versions of Windows, it'll 'just work' for most tasks. For Windows 7, they need to be the same driver (so Nvidia only or AMD only). Windows 10 doesn't care. I've run a third monitor on onboard video with some tweaks on Windows 8 and 10 with the primary monitors being on a separate video card, and that's roughly the worst case scenario. So yeah, you'd be able to drag programs between monitors, and what you're thinking of would be doable. I'd suggest cards of the same brand and roughly similar era (so an 800-1000 series) simply for driver compatibility. With paired video cards, chances are your system will drop down to x8 PCIe lanes from x16, but there's little practical performance loss from this. With some software, you can use a secondary/lesser card as a co-processor for specific tasks or simply pick one as a primary card. Photoshop specifically suggests using one card for your output, and to use identical video cards. Adobe also says: Photoshop currently doesn't take advantage of more than one graphics processor. Using two graphics cards does not enhance Photoshop's performance. Multiple graphics cards with conflicting drivers can cause problems with graphics-processor accelerated features in Photoshop. So in this particular scenario, it's not that useful. You might see some advantages with some CUDA or OpenCL loads but YMMV."}
{"doc_id": 355073, "author": "chx", "text": "While previously you needed to have special, more expensive motherboards running two or more video cards from the same vendor (called Crossfire/SLI by their respective vendors), DirectX 12 allows for multi-display adapter (MDA) where the software (game, mostly) can explicitly work with, as the name suggests, multiple displays (CF/SLI is now called linked display adapter or LDA)). However, to quote one of the earliest benchmarks from 2016 February: Almost no games presently support Dx12, fewer still support this type of multi-GPU configuration, and then there still has to be some dev-side support and testing for that configuration. It's not all just about drivers and APIs; if the game's QA team hasn't run the configuration, there's a good chance more bugs present themselves. While obviously since then more games started supporting DirectX 12, it is still true that few supports MDA and you are more likely to face bugs. If there are N video cards the QA tested then to test all possible pairs they would need to test almost N squared, which very quickly becomes unfeasible. So: you can have two GPUs, in theory they will accelerate some games together, in practice it's a bit more dubious. If you have no need of that and just want to hang a lot of monitors off the same PC, you can just expect that to work to an extent: applications run a specific GPU so stretching one application between monitors hanging off a different card is something unlikely to work. This might be unnecessary as there are gaming cards with six video outputs at a reasonable cost -- much cheaper than workstation cards: the Asus 7970 / 280X (from 2013) and the Asrock Taichi 5700 XT (from 2019), the latter allowing for six 4K monitors."}
{"doc_id": 355075, "author": "bwDraco", "text": "Yes, this can technically work—both cards will give you graphical output. However, different cards cannot be linked together to function as a GPU array (CrossFire or SLI), so you generally won't be able to use them together to render graphics in games. The cards will operate independently of each other. In your case, the second card will be responsible for rendering the graphics on the additional monitors that are attached to it. However, there are other ways you can use the second graphics card: GPU-accelerated compute applications often allow you to select the specific graphics card to use. However, Photoshop doesn't support multiple GPUs for acceleration and (presumably) will only use the first card: Photoshop currently doesn't take advantage of more than one graphics processor. Using two graphics cards does not enhance Photoshop's performance. A few new DirectX 12-based games (e.g. Ashes of the Singularity: Escalation) are capable of using two independent graphics cards to enhance performance. This can, in some cases, enable cards from different vendors to be used together. Games which use NVIDIA PhysX technology can use the second card as a dedicated processor for physics computations, independently of the first card which handles the graphical rendering. This can improve performance in some games. A dedicated PhysX GPU can be configured in the NVIDIA Control Panel."}
{"doc_id": 358802, "author": "Abom", "text": "I use a GTX1070/750ti with two monitors. I just had to make sure I set my display that the 750ti was plugged into as primary or desktop applications wouldn't use my 750ti. I assumed that Chrome and such would use whichever GPU its window was on. Boy, was I wrong. My 750ti no takes care of gpu programs like chrome, twitch, bnet, and my games use my 1070. Noticed a 10-15 fps improvement."}
{"doc_id": 355019, "author": "potatoman", "text": "Yep, having two completely different GPUs in one PC is possible, as long as there are enough PCI slots. However, if you are planning to use SLI, it requires two of the same cards. Furthermore, you should remember not all applications take advantage of the dual GPU setup. Source"}
{"doc_id": 355020, "author": "Journeyman Geek", "text": "Kind of, though it's not that simple. Practically, on more modern versions of Windows, it'll 'just work' for most tasks. For Windows 7, they need to be the same driver (so Nvidia only or AMD only). Windows 10 doesn't care. I've run a third monitor on onboard video with some tweaks on Windows 8 and 10 with the primary monitors being on a separate video card, and that's roughly the worst case scenario. So yeah, you'd be able to drag programs between monitors, and what you're thinking of would be doable. I'd suggest cards of the same brand and roughly similar era (so an 800-1000 series) simply for driver compatibility. With paired video cards, chances are your system will drop down to x8 PCIe lanes from x16, but there's little practical performance loss from this. With some software, you can use a secondary/lesser card as a co-processor for specific tasks or simply pick one as a primary card. Photoshop specifically suggests using one card for your output, and to use identical video cards. Adobe also says: Photoshop currently doesn't take advantage of more than one graphics processor. Using two graphics cards does not enhance Photoshop's performance. Multiple graphics cards with conflicting drivers can cause problems with graphics-processor accelerated features in Photoshop. So in this particular scenario, it's not that useful. You might see some advantages with some CUDA or OpenCL loads but YMMV."}
{"doc_id": 355073, "author": "chx", "text": "While previously you needed to have special, more expensive motherboards running two or more video cards from the same vendor (called Crossfire/SLI by their respective vendors), DirectX 12 allows for multi-display adapter (MDA) where the software (game, mostly) can explicitly work with, as the name suggests, multiple displays (CF/SLI is now called linked display adapter or LDA)). However, to quote one of the earliest benchmarks from 2016 February: Almost no games presently support Dx12, fewer still support this type of multi-GPU configuration, and then there still has to be some dev-side support and testing for that configuration. It's not all just about drivers and APIs; if the game's QA team hasn't run the configuration, there's a good chance more bugs present themselves. While obviously since then more games started supporting DirectX 12, it is still true that few supports MDA and you are more likely to face bugs. If there are N video cards the QA tested then to test all possible pairs they would need to test almost N squared, which very quickly becomes unfeasible. So: you can have two GPUs, in theory they will accelerate some games together, in practice it's a bit more dubious. If you have no need of that and just want to hang a lot of monitors off the same PC, you can just expect that to work to an extent: applications run a specific GPU so stretching one application between monitors hanging off a different card is something unlikely to work. This might be unnecessary as there are gaming cards with six video outputs at a reasonable cost -- much cheaper than workstation cards: the Asus 7970 / 280X (from 2013) and the Asrock Taichi 5700 XT (from 2019), the latter allowing for six 4K monitors."}
{"doc_id": 355075, "author": "bwDraco", "text": "Yes, this can technically work—both cards will give you graphical output. However, different cards cannot be linked together to function as a GPU array (CrossFire or SLI), so you generally won't be able to use them together to render graphics in games. The cards will operate independently of each other. In your case, the second card will be responsible for rendering the graphics on the additional monitors that are attached to it. However, there are other ways you can use the second graphics card: GPU-accelerated compute applications often allow you to select the specific graphics card to use. However, Photoshop doesn't support multiple GPUs for acceleration and (presumably) will only use the first card: Photoshop currently doesn't take advantage of more than one graphics processor. Using two graphics cards does not enhance Photoshop's performance. A few new DirectX 12-based games (e.g. Ashes of the Singularity: Escalation) are capable of using two independent graphics cards to enhance performance. This can, in some cases, enable cards from different vendors to be used together. Games which use NVIDIA PhysX technology can use the second card as a dedicated processor for physics computations, independently of the first card which handles the graphical rendering. This can improve performance in some games. A dedicated PhysX GPU can be configured in the NVIDIA Control Panel."}
{"doc_id": 358802, "author": "Abom", "text": "I use a GTX1070/750ti with two monitors. I just had to make sure I set my display that the 750ti was plugged into as primary or desktop applications wouldn't use my 750ti. I assumed that Chrome and such would use whichever GPU its window was on. Boy, was I wrong. My 750ti no takes care of gpu programs like chrome, twitch, bnet, and my games use my 1070. Noticed a 10-15 fps improvement."}
{"doc_id": 355019, "author": "potatoman", "text": "Yep, having two completely different GPUs in one PC is possible, as long as there are enough PCI slots. However, if you are planning to use SLI, it requires two of the same cards. Furthermore, you should remember not all applications take advantage of the dual GPU setup. Source"}
{"doc_id": 355020, "author": "Journeyman Geek", "text": "Kind of, though it's not that simple. Practically, on more modern versions of Windows, it'll 'just work' for most tasks. For Windows 7, they need to be the same driver (so Nvidia only or AMD only). Windows 10 doesn't care. I've run a third monitor on onboard video with some tweaks on Windows 8 and 10 with the primary monitors being on a separate video card, and that's roughly the worst case scenario. So yeah, you'd be able to drag programs between monitors, and what you're thinking of would be doable. I'd suggest cards of the same brand and roughly similar era (so an 800-1000 series) simply for driver compatibility. With paired video cards, chances are your system will drop down to x8 PCIe lanes from x16, but there's little practical performance loss from this. With some software, you can use a secondary/lesser card as a co-processor for specific tasks or simply pick one as a primary card. Photoshop specifically suggests using one card for your output, and to use identical video cards. Adobe also says: Photoshop currently doesn't take advantage of more than one graphics processor. Using two graphics cards does not enhance Photoshop's performance. Multiple graphics cards with conflicting drivers can cause problems with graphics-processor accelerated features in Photoshop. So in this particular scenario, it's not that useful. You might see some advantages with some CUDA or OpenCL loads but YMMV."}
{"doc_id": 355073, "author": "chx", "text": "While previously you needed to have special, more expensive motherboards running two or more video cards from the same vendor (called Crossfire/SLI by their respective vendors), DirectX 12 allows for multi-display adapter (MDA) where the software (game, mostly) can explicitly work with, as the name suggests, multiple displays (CF/SLI is now called linked display adapter or LDA)). However, to quote one of the earliest benchmarks from 2016 February: Almost no games presently support Dx12, fewer still support this type of multi-GPU configuration, and then there still has to be some dev-side support and testing for that configuration. It's not all just about drivers and APIs; if the game's QA team hasn't run the configuration, there's a good chance more bugs present themselves. While obviously since then more games started supporting DirectX 12, it is still true that few supports MDA and you are more likely to face bugs. If there are N video cards the QA tested then to test all possible pairs they would need to test almost N squared, which very quickly becomes unfeasible. So: you can have two GPUs, in theory they will accelerate some games together, in practice it's a bit more dubious. If you have no need of that and just want to hang a lot of monitors off the same PC, you can just expect that to work to an extent: applications run a specific GPU so stretching one application between monitors hanging off a different card is something unlikely to work. This might be unnecessary as there are gaming cards with six video outputs at a reasonable cost -- much cheaper than workstation cards: the Asus 7970 / 280X (from 2013) and the Asrock Taichi 5700 XT (from 2019), the latter allowing for six 4K monitors."}
{"doc_id": 355075, "author": "bwDraco", "text": "Yes, this can technically work—both cards will give you graphical output. However, different cards cannot be linked together to function as a GPU array (CrossFire or SLI), so you generally won't be able to use them together to render graphics in games. The cards will operate independently of each other. In your case, the second card will be responsible for rendering the graphics on the additional monitors that are attached to it. However, there are other ways you can use the second graphics card: GPU-accelerated compute applications often allow you to select the specific graphics card to use. However, Photoshop doesn't support multiple GPUs for acceleration and (presumably) will only use the first card: Photoshop currently doesn't take advantage of more than one graphics processor. Using two graphics cards does not enhance Photoshop's performance. A few new DirectX 12-based games (e.g. Ashes of the Singularity: Escalation) are capable of using two independent graphics cards to enhance performance. This can, in some cases, enable cards from different vendors to be used together. Games which use NVIDIA PhysX technology can use the second card as a dedicated processor for physics computations, independently of the first card which handles the graphical rendering. This can improve performance in some games. A dedicated PhysX GPU can be configured in the NVIDIA Control Panel."}
{"doc_id": 358802, "author": "Abom", "text": "I use a GTX1070/750ti with two monitors. I just had to make sure I set my display that the 750ti was plugged into as primary or desktop applications wouldn't use my 750ti. I assumed that Chrome and such would use whichever GPU its window was on. Boy, was I wrong. My 750ti no takes care of gpu programs like chrome, twitch, bnet, and my games use my 1070. Noticed a 10-15 fps improvement."}
{"doc_id": 52643, "author": "Oscar Gonzalez", "text": "It's up to your hardware. I have an older toshiba laptop that has a built-in ethernet and one that is on the docking station. They both work if you plug it in (depending on your network). and they each have their own MAC address. When I plug in one and wait, that computer gets one IP Address, when I unplug that and plug into the docking station, then it gets a different address because my DHCP server thinks its a new machine because the DHCP leases are based on the MAC address. It depends on the hardware really and whether the ethernet port on the docking station is a \"new\" device vs a \"passthrough\" device. On Windows, open up the command prompt and type \"getmac\" and it'll tell you the mac addressess. You can also dock your computer and type in \"ipconfig -all\" and it will show you all your ethernet ports and their mac addresses."}
{"doc_id": 118383, "author": "Shadok", "text": "You can find this through the following command: echo %LOGONSERVER% And you don't have to be admin or poweruser to use it. Have a look at the output of this command: set"}
{"doc_id": 118384, "author": "Supercereal", "text": "This only answers your question if you have Outlook: I found an interesting feature in Outlook. If you hold Ctrl and right click the icon in the task bar then click connection status it shows you the exchange server your connected to as well as what domain controller you are connected to. I actually found that one answering another question about Exchange connections, great way to recycle answers..."}
{"doc_id": 134752, "author": "Erwin", "text": "set L lists all environment variables that begin with the letter L"}
{"doc_id": 180328, "author": "Noel", "text": "set l will respond with the variables for both localappdata and for logonserver. However, logonserver is the only variable you are interested in, and the one which will tell you the name of the domain controller you authenticated against. To only get the logonserver information, type set log (which is simply an abbreviation of set logonserver). The name of the domain controller you authenticated against will be returned."}
{"doc_id": 213565, "author": "Michael Knox", "text": "To determine computer / server DC use NLTEST: nltest /dsgetdc:<domain_name> To list all DC's with their appropriate site, try: nltest /dclist:<domain_name> You don't have to use the FQDN of the domain name or server -- for example, instead of saying /dclist:services.microsoft.com, you can simply type /dclist:services (as long as you are an authenticated member of that domain, of course). For user authentication and group policy use LOGONSERVER variable: echo %logonserver%"}
{"doc_id": 361412, "author": "rupert160", "text": "Powershell provides (with no parameters): Get-ADDomainController Or specifically: (Get-ADDomainController).HostName"}
{"doc_id": 118383, "author": "Shadok", "text": "You can find this through the following command: echo %LOGONSERVER% And you don't have to be admin or poweruser to use it. Have a look at the output of this command: set"}
{"doc_id": 118384, "author": "Supercereal", "text": "This only answers your question if you have Outlook: I found an interesting feature in Outlook. If you hold Ctrl and right click the icon in the task bar then click connection status it shows you the exchange server your connected to as well as what domain controller you are connected to. I actually found that one answering another question about Exchange connections, great way to recycle answers..."}
{"doc_id": 134752, "author": "Erwin", "text": "set L lists all environment variables that begin with the letter L"}
{"doc_id": 180328, "author": "Noel", "text": "set l will respond with the variables for both localappdata and for logonserver. However, logonserver is the only variable you are interested in, and the one which will tell you the name of the domain controller you authenticated against. To only get the logonserver information, type set log (which is simply an abbreviation of set logonserver). The name of the domain controller you authenticated against will be returned."}
{"doc_id": 213565, "author": "Michael Knox", "text": "To determine computer / server DC use NLTEST: nltest /dsgetdc:<domain_name> To list all DC's with their appropriate site, try: nltest /dclist:<domain_name> You don't have to use the FQDN of the domain name or server -- for example, instead of saying /dclist:services.microsoft.com, you can simply type /dclist:services (as long as you are an authenticated member of that domain, of course). For user authentication and group policy use LOGONSERVER variable: echo %logonserver%"}
{"doc_id": 361412, "author": "rupert160", "text": "Powershell provides (with no parameters): Get-ADDomainController Or specifically: (Get-ADDomainController).HostName"}
{"doc_id": 173762, "author": "Dan", "text": "Right click your desktop, click Personalize and then on the bottom row of the Personalize window select Desktop Background, you will be able to set the individual backgrounds for your monitors there, as you can see in the example image below."}
{"doc_id": 173763, "author": "Joey", "text": "You can right-click a picture in the selection dialog and select on which monitor to show it."}
{"doc_id": 234207, "author": "Dave", "text": "Remove the laptop, turn off the docking station. Now, plug in the keyboard/mouse etc Now turn on the docking station, give it a little time, and now put on your machine. It appears many docking stations need to work in a certain order, it doesn't detect the keyboard/mouse (I guess in the same way a PC doesn't detect a PS2 mouse/keyboard without a reboot). Some have reported that, \"disable one of the two USB Root Hub from the Device Manager and refresh the list allowing the system to recognize the connected devices\". Source Others have claimed to roll back the driver to a generic driver Source Based upon your comments, since it doesn't recognize multiple devices (keyboards/external disks) then it sounds faulty. You could ask HP for advice directly, if it's in warranty then it's a no brainer (send it back) but you may need to get a new dock :( However, to test this, try a different laptop in the dock (if possible)."}
{"doc_id": 252101, "author": "rich", "text": "There are several of us in our office that have Folio 9470m/ultraslim docking stations. I know of at least two who have the same usb problem. What I found that worked was to pull the power cord out of the docking station for about 10 seconds, plug it back in and miracle of all miracles, it works. So that is the workaround I use. Lame but until HP comes up with a fix, I'm not sure what else to do."}
{"doc_id": 252125, "author": "pointbreakpjh", "text": "My docking station worked fine, but I needed my IT Department to re-image my laptop for an unrelated issue. When I got my laptop back, the USB ports were not connecting. It was driver based. With the laptop docked, go into device manager and update all your USB drivers. I updated the drivers and everything is now working perfectly."}
{"doc_id": 256107, "author": "straussj", "text": "I experienced this same issue -- dock was powered, but keyboard and mouse were not recognized. This worked for me: In your list of programs see if you have an Intel USB 3.0 eXtensible host controller (or any other USB host controller that may be preventing your laptop from recognizing the dock's usb ports). I removed it and my mouse lit up!"}
{"doc_id": 269357, "author": "HP newbie", "text": "After I realized I plugged my power into the laptop instead of the docking station, I did the following: Undock the laptop, power off docking station and re-connect mouse and keyboard - just plugged into different USB sockets. Power on docking station, wait for 10-15 seconds with that back light on and then plugged in laptop. Once screens lit up there was a message that USB devices were installed. Weird thing is that the charger for my phone worked on any USB ports on docking station but mouse and keyboard did not recognize. They both worked when plugged into laptop directly. On side notes, my docking station has 2 HDMIs and it supports two external monitors that way + analog one. Don't have a fourth monitor but guessing the HDMI port on laptop itself would support it."}
{"doc_id": 401867, "author": "Jay Vee", "text": "I have an HP 830 G3 from my company, and I have an HP 840 G3 as a personal machine. I bought the 840 because it will use the same 2013 Ultraslim dock as the 830 does. I use the dock with the 830 every day so I know there is nothing wrong with it. I had made some changes to the 840 and hadnt tried to use it on the docking station in a while. When I did decide to use the 840 G3 on the docking station, I noticed that the keyboard and mouse didnt work. In fact none of the USB ports on the docking station were working. I looked around on the various forums but couldnt find a fix. I knew there must be something on the 840 preventing the docking station from working properly. I found that in BIOS, under advanced, there was a box that said USB docking stations (or something like that) and it was unchecked. Once I checked that box and saved the settings, the USB ports work as intended using the 840 G3 with the docking station. I know this is an old conversation, but maybe this will be a fix for someone else down the road."}
{"doc_id": 2341, "author": "Ivo Flipse", "text": "Depending on the file transferring rate, you can easily benchmark this copying a file of 100 Mb and try to extrapolate that to 1 Tb With USB sticks I get about 4 Mb/s with large files from my MyBook I get 25 Mb/s"}
{"doc_id": 2342, "author": "phenry", "text": "USB 2.0 has a signaling rate of 480 Mbit/s. The same article says that typical real-world throughput is \"about two thirds of the maximum theoretical bulk data transfer rate of 53.248 MB/s.\" If my math is correct, and it probably isn't, that suggests that the best time you could hope to achieve is about 8.2 hours for 1 TB, assuming that the USB connection is the biggest bottleneck."}
{"doc_id": 2344, "author": "Ash", "text": "It's not just dependent on Disk speeds, but also the speed of the IDE/Sata controller on the external drive / USB key. I've found that many cheaper external drives are slower, they are still USB2 but have cheaper and slower IDE/Sata controllers. Of course USB 2 has a theoretical max throughput of 480 Mbit /s [Clarified to stop uninformed comments], so you could work out pretty easily the absolutte fastest time (but this time would never happen in real life of course)."}
{"doc_id": 2345, "author": "Jared Harley", "text": "Well, based on my rough calculations, somewhere around 7 hours and 15 minutes! USB 2.0 has a raw data rate of 480Mbps, but the fastest typical usage usually tops out around 40 MB/s. Given that a terabyte has 1,048,576 megabytes, you just do the math. If you could achieve the theoretical 60 MB/s transfer, you could do it in slightly over 4 hours and 45 minutes."}
{"doc_id": 2346, "author": "fretje", "text": "From experience, I know USB 2.0 copies about 10Mb/sec on average (on my system). So that would be 1TB == 1048576 Mb 1048576 / 10 ==> +/- 104857 secs 104857 / 60 ==> +/- 1747 mins 1747 / 60 ==> +/- 29 hours So a full day and 5 hours. Note that I use teracopy as the default copy handler of my windows (otherwise I never get the 10Mb/sec average over usb)."}
{"doc_id": 2348, "author": "Phillip Gibb", "text": "too many factors here. the write transfer rate to the disk will change depending on how much space you have used, I assume that RAID 0 is in play here and that you would be writing to a 1.5 TB drive considering that a 1 TB drive will not have 1 TB of free space available. Nevermind the same constraints for the source drives read transfer rate. I reakon that ashh's answer would be correct if the drives' read and write constraints were not an issue {edit: and USB was running at max - not likely}. but it would more likely slow down as the drive got fuller - taking about 30 - 45 mins longer. {edit: more real: 5-6 hrs longer}"}
{"doc_id": 2349, "author": "pavsaund", "text": "Given the variations of I/O handling by the operating system and the natural delay of starting and stopping copying (many vs few files) you are realisticly looking at approx 15 Mbit/s (from my experienve) Theoretical values: 1 TB @ 480 MBit/s = approx 4.6 hrs Realistic values: 1 TB @ 15 MBit/s = approx 148 hrs"}
{"doc_id": 27035, "author":  "Pierre", "text": "I just completed 2 of these transfers and I'm surprised at the slow USB 2.0 speed I'm getting. I copied 1 TB (terabyte) of video files and it took about 16 hours. Avg speed was in the 30's. The 2nd about a week later to a different external HD was 18 hours. And I was annoyed at the 1st transfer :) time for esata external storage box with 4 or 5 bays!!!!"}
{"doc_id": 77300, "author": "Dave Brown", "text": "I have seen about 4-5x faster performance with ESata vs. USB2.0 using the same external hard drive. I have a WD 1.5TB Essentials Drive, which I would back up using USB, but it was painful, taking about 4-5hrs per 100G, and running about 8MB/sec avg. I pulled the HD out of the plastic case, removed the USB to SATA board, and plugged the same HD into the SATA port from the mother board via an ESATA cable, and I can now backup 100G in less than an hour, and 250G in about 2.5hr using NovaBackup S/W. All I can say is that 2-3hrs is not that bad for a lot of data (running about 28-32MB/sec w/ESATA). Both are well/far below theoretical, but the comparison between the two is what counts."}
{"doc_id": 125771, "author": "Syclone0044", "text": "There is a lot of false information in these answers about \"theoretical\" performance from people who have evidently never benchmarked USB2 HD transfer rates. I have benchmarked many different USB2 transfers between 2.5\" laptop HDs both PATA and SATA, 3.5\" HDs both PATA and SATA, and USB Flash drives... ...and I have NEVER seen transfer rates exceed 35 MB/sec! In fact, any properly configured modern drive will transfer at 20-30 MB/sec, it's very rare to see the 30 MB/sec rate be surpassed. (I'm referring SPECIFICALLY to HDs transferring over USB2 here, to be clear.) Ignore this talk about theoretical transfer rates and \"60 MB/sec\", etc. Although I give credit to the guys who correctly converted bits into bytes and calculated a 35 MB/sec maximum, which falls in line with my REAL WORLD PERFORMANCE EXPERIENCE."}
{"doc_id": 318130, "author": "maja", "text": "As a real-world example, I just transferred 1.774TB worth of mixed files reading from USB2.0 and writing to USB3.0 on an old laptop with a 2nd generation i7, finishing in about 22 hours at a speed of about 22.6MB/s (which means about 12 hours for 1TB)."}
{"doc_id": 2341, "author": "Ivo Flipse", "text": "Depending on the file transferring rate, you can easily benchmark this copying a file of 100 Mb and try to extrapolate that to 1 Tb With USB sticks I get about 4 Mb/s with large files from my MyBook I get 25 Mb/s"}
{"doc_id": 2342, "author": "phenry", "text": "USB 2.0 has a signaling rate of 480 Mbit/s. The same article says that typical real-world throughput is \"about two thirds of the maximum theoretical bulk data transfer rate of 53.248 MB/s.\" If my math is correct, and it probably isn't, that suggests that the best time you could hope to achieve is about 8.2 hours for 1 TB, assuming that the USB connection is the biggest bottleneck."}
{"doc_id": 2344, "author": "Ash", "text": "It's not just dependent on Disk speeds, but also the speed of the IDE/Sata controller on the external drive / USB key. I've found that many cheaper external drives are slower, they are still USB2 but have cheaper and slower IDE/Sata controllers. Of course USB 2 has a theoretical max throughput of 480 Mbit /s [Clarified to stop uninformed comments], so you could work out pretty easily the absolutte fastest time (but this time would never happen in real life of course)."}
{"doc_id": 2345, "author": "Jared Harley", "text": "Well, based on my rough calculations, somewhere around 7 hours and 15 minutes! USB 2.0 has a raw data rate of 480Mbps, but the fastest typical usage usually tops out around 40 MB/s. Given that a terabyte has 1,048,576 megabytes, you just do the math. If you could achieve the theoretical 60 MB/s transfer, you could do it in slightly over 4 hours and 45 minutes."}
{"doc_id": 2346, "author": "fretje", "text": "From experience, I know USB 2.0 copies about 10Mb/sec on average (on my system). So that would be 1TB == 1048576 Mb 1048576 / 10 ==> +/- 104857 secs 104857 / 60 ==> +/- 1747 mins 1747 / 60 ==> +/- 29 hours So a full day and 5 hours. Note that I use teracopy as the default copy handler of my windows (otherwise I never get the 10Mb/sec average over usb)."}
{"doc_id": 2348, "author": "Phillip Gibb", "text": "too many factors here. the write transfer rate to the disk will change depending on how much space you have used, I assume that RAID 0 is in play here and that you would be writing to a 1.5 TB drive considering that a 1 TB drive will not have 1 TB of free space available. Nevermind the same constraints for the source drives read transfer rate. I reakon that ashh's answer would be correct if the drives' read and write constraints were not an issue {edit: and USB was running at max - not likely}. but it would more likely slow down as the drive got fuller - taking about 30 - 45 mins longer. {edit: more real: 5-6 hrs longer}"}
{"doc_id": 2349, "author": "pavsaund", "text": "Given the variations of I/O handling by the operating system and the natural delay of starting and stopping copying (many vs few files) you are realisticly looking at approx 15 Mbit/s (from my experienve) Theoretical values: 1 TB @ 480 MBit/s = approx 4.6 hrs Realistic values: 1 TB @ 15 MBit/s = approx 148 hrs"}
{"doc_id": 27035, "author":  "Pierre", "text": "I just completed 2 of these transfers and I'm surprised at the slow USB 2.0 speed I'm getting. I copied 1 TB (terabyte) of video files and it took about 16 hours. Avg speed was in the 30's. The 2nd about a week later to a different external HD was 18 hours. And I was annoyed at the 1st transfer :) time for esata external storage box with 4 or 5 bays!!!!"}
{"doc_id": 77300, "author": "Dave Brown", "text": "I have seen about 4-5x faster performance with ESata vs. USB2.0 using the same external hard drive. I have a WD 1.5TB Essentials Drive, which I would back up using USB, but it was painful, taking about 4-5hrs per 100G, and running about 8MB/sec avg. I pulled the HD out of the plastic case, removed the USB to SATA board, and plugged the same HD into the SATA port from the mother board via an ESATA cable, and I can now backup 100G in less than an hour, and 250G in about 2.5hr using NovaBackup S/W. All I can say is that 2-3hrs is not that bad for a lot of data (running about 28-32MB/sec w/ESATA). Both are well/far below theoretical, but the comparison between the two is what counts."}
{"doc_id": 125771, "author": "Syclone0044", "text": "There is a lot of false information in these answers about \"theoretical\" performance from people who have evidently never benchmarked USB2 HD transfer rates. I have benchmarked many different USB2 transfers between 2.5\" laptop HDs both PATA and SATA, 3.5\" HDs both PATA and SATA, and USB Flash drives... ...and I have NEVER seen transfer rates exceed 35 MB/sec! In fact, any properly configured modern drive will transfer at 20-30 MB/sec, it's very rare to see the 30 MB/sec rate be surpassed. (I'm referring SPECIFICALLY to HDs transferring over USB2 here, to be clear.) Ignore this talk about theoretical transfer rates and \"60 MB/sec\", etc. Although I give credit to the guys who correctly converted bits into bytes and calculated a 35 MB/sec maximum, which falls in line with my REAL WORLD PERFORMANCE EXPERIENCE."}
{"doc_id": 318130, "author": "maja", "text": "As a real-world example, I just transferred 1.774TB worth of mixed files reading from USB2.0 and writing to USB3.0 on an old laptop with a 2nd generation i7, finishing in about 22 hours at a speed of about 22.6MB/s (which means about 12 hours for 1TB)."}
{"doc_id": 353375, "author": "Aganju", "text": "If you open a .dot (or .dotx), there is no ‘save’, it is always interpreted as ‘Save as’. The main effect of this is that users cannot accidentially save their filled version over the template, which otherwise is quite annoying for the next template user. You are correct that otherwise there is little difference; after all, you can make a doc or docx to a template by simply renaming the file on disk."}
{"doc_id": 353380, "author": "Chaosu", "text": "Am I missing the purpouse of templates? What are the adventages of using them over copying and pasting pre-layouted .docx? Opening template may be faster and \"copying\" process is on Word, not on user. For example if you would like to programmatically create new Word files, instead of copying existing file (where you need to input location before you even begin working on document itself) or creating new file and applying formatting, you \"open\" template and after you're done you can decide on saving or discarding new file (instead of deleting existing copy). It is understandable that for some use, these advantages are not appealing."}
{"doc_id": 377760, "author": "prdesignermom", "text": ".dotx files are designed to be saved to a specific location on your computer, which will tell word that there is a template available each time you open a new document. You can have as many templates as you want, which comes in handy if you have multiple clients, or multiple reports that need to follow established formatting. (\"save as,\" *.dotx to: this PC > Documents > custom office templates folder) When you open a new document in Word, just under the search bar, are two tabs, 1) Featured (showing all of the templates available online) and 2) Personal. Clicking personal will show all of the templates that are installed on your computer. It pulls up a blank document that is pre-formatted and you proceed as normal. You never have to hunt for the original, or worry about overwriting previous work."}
{"doc_id": 258341, "author": "Matty J", "text": "When you updated your Path variable, did you close and then reopen the Command Prompt before you tried running those commands again? Because you need to (or possibly restart). It sounds like your Path variable may have been overwritten by accident. If you navigate to C:\\WINDOWS\\system32, and try to run ping from there, does it work? If so, it is your PATH variable that has the problem."}
{"doc_id": 259307, "author": "laleh1380", "text": "The path is now managed by Windows 7 and not the autoexec.bat or autoexec.nt files. To change the system environment variables, follow the below steps. From the desktop, right-click My Computer and click Properties. In the System Properties window, click on the Advanced tab. In the Advanced section, click the Environment Variables button. Finally, in the Environment Variables window (as shown below), highlight the Path variable in the Systems Variable section and click the Edit button. Add or modify the path lines with the paths you wish the computer to access. Each different directory is separated with a semicolon as shown below. This is the default PATH environment variable setting on fresh install of Windows 7 %SystemRoot%\\system32;%SystemRoot%;%SystemRoot%\\System32\\Wbem;%SYSTEMROOT%\\System32\\WindowsPowerShell\\v1.0\\"}
{"doc_id": 258341, "author": "Matty J", "text": "When you updated your Path variable, did you close and then reopen the Command Prompt before you tried running those commands again? Because you need to (or possibly restart). It sounds like your Path variable may have been overwritten by accident. If you navigate to C:\\WINDOWS\\system32, and try to run ping from there, does it work? If so, it is your PATH variable that has the problem."}
{"doc_id": 259307, "author": "laleh1380", "text": "The path is now managed by Windows 7 and not the autoexec.bat or autoexec.nt files. To change the system environment variables, follow the below steps. From the desktop, right-click My Computer and click Properties. In the System Properties window, click on the Advanced tab. In the Advanced section, click the Environment Variables button. Finally, in the Environment Variables window (as shown below), highlight the Path variable in the Systems Variable section and click the Edit button. Add or modify the path lines with the paths you wish the computer to access. Each different directory is separated with a semicolon as shown below. This is the default PATH environment variable setting on fresh install of Windows 7 %SystemRoot%\\system32;%SystemRoot%;%SystemRoot%\\System32\\Wbem;%SYSTEMROOT%\\System32\\WindowsPowerShell\\v1.0\\"}
{"doc_id": 241245, "author": "Paul", "text": "An IP address is not necessarily an \"identity on the internet\", it is just a unique address assigned to a specific network card on an IP network. The internet is one big network, and so anything on it (that has an IP address) has a unique address, and so the IP address can be considered its identity. Your router is on the internet, and so it has an IP address unique to it. Anything on the inside of your network is not on the internet, it is on your LAN, an entirely separate network, with its own address space. Anything on your LAN must have a unique (within the LAN) IP address to participate properly with your local network. Anyone else who has a LAN is also not part of the internet, and not part of your network, and so can use the same addresses you do. In order for your LAN devices to talk to the internet, their outgoing traffic has the source address changed to match that of the internet IP address of the router. The router keeps track of this, and makes sure any response traffic gets sent to the right internal machine. This is called Network Address Translation."}
{"doc_id": 241248, "author": "LPChip", "text": "Every IP Address is accompanied with the subnet mask. This subnetmask determines how large your network is from ranges you can access to ranges you can't access. Basically, the subnetmask in a home situation is 255.255.255.0 which means that with a 192.168.0.1-255 network, the first numbers must always be the same in order to access that portion of the network. So 192.168.0.1 can see 192.168.0.2 but not 192.168.1.1 because the 3rd octed of the subnetmask is 255, not anything else. So your computer can only access addresses of 192.168.0.x Your router is also in this range, so your computer can access this address as well. Your router however makes a 2nd connection through your modem to the internet, and gets an IP address from the internet, for example: 123.45.67.89 with subnetmask 255.255.254.0 which places your internet connection inside a group of computers on the internet. Your internet provider has a router just like you have at home (well they have more, but thats not the point) which connects their internet selection with hubs, and through hubs, the whole internet eventually reaches eachother. So long story short, by how the subnetmask is set, it determines what you can access, but changing your subnetmask to 0.0.0.0 does not mean you can reach the internet without needing an internet provider. But given that this is an advanced topic, I tried to keep it as simple as possible."}
{"doc_id": 241249, "author": "Samir", "text": "Imagine that your router at home has the address 192.168.1.1 and your PC has the address 192.168.1.100. Then another router at your friend's house also has the address 192.168.1.1 and his PC has the address 192.168.1.100. Addresses that begin with 10, 172.16 - 172.31 or 192.168 are referred to as private range addresses (class A, B and C). These addresses are used in a limited area of the Internet, namely within your own local network (LAN) at home which you have control over. They don't need to be registered. Internet service providers usually use different address range, and the addresses they use are registered with an Internet authority. In the example above your public address might be 109.240.120.13 and your friends public address might be 83.42.112.110. These are the addresses that you and your friend use on the Internet, and they are unique."}
{"doc_id": 241251, "author": "ju.kreber", "text": "Well, that's an interesting thing indeed. Let's go back from the 192.168.x.y for a while and back to the time when internet was mostly used by universities and institutes. There were unique ip addresses, like you said an \"indentity\". If you were the network admin of a university and though, \"well, i think we should be part of the internet\" then you could order some at the IANA (https://www.iana.org/). For example, a class-B-network, that means you get an address range like 34.172.0.0 to 34.172.255.255. Now you could assign them as you wished to the computers at your university and they could take part on the internet, everything was fine. More and more people got access to the internet, for example some businesses. ISPs (internet service providers) came up. They buy a certain range of IPs and sell them to other people. Not long time ago, the common approach if you wanted your business to be connected to the internet was: \"i need 20 ips for the marketing, 50 for r&d and 40 for the production, so let's order 110 ips\". And this yet worked fine. Okay, now let's say you have 4 computers at home. They create a small LocalAreaNetwork, a LAN. This is your personel network, and they are not yet connected to the internet whichs means they don't need world-wide unique addresses but local, unique addresses. There is a certain ip range for this purpose, and 192.168.x.y is commonly used for this. This means: In your own private network every pc has a local unique IP address. They now can communicate with each other, no problems. But now you'd like them to be connected to the internet. If we do it as described above, we would go to our ISP and order 4 world-wide unique IPs. Okay. But we have already our LAN and every computer there has its own local ip already. It would not be nice to change all these to world-wide ones. So there's a nice technique called \"NAT\", which means NetworkAddressTranslation. Every computer keeps his local ip. And in your router (which commonly also implements a NAT) you assign each local ip to a global ip in a NAT table. Now what is going on here? If PC1 with 192.168.0.101 would like to connect to google.com, it sends a request to its main gateway (\"routing\" is the keyword here, but i'll not explain this here) to be connected to that server. (Your router is connected to your local network with its LAN ip on the one side and to the internet with your 4 global IPs on the other side) Of course google.com is in the WAN, the internet. So the NAT now does the following: It takes the local ip and translates it to a global ip as once assinged in the NAT table. The translation could be like \"192.168.0.101 <-> 33.134.10.51\". So a request from 33.134.10.51 is sent to google.com. The answer comes back to 33.134.10.51 (to the NAT) and is translated back to 192.168.0.101 and thus sent to your computer. So now all your computers have local and a global IP (but this one is only stored in the NAT and used for world-wide communication). But with more and more people getting internet access, the pool of free IPs decreased rapidely. There were too many computers for too less IPs. So what can we do? The answer is PAT (port and network translation). It works like an extended NAT. Nowadays, you go to your ISP and order one IP address, eg 90.80.70.10. You have your small LAN at home with 4 PCs. They all got their unique LAN address. Now the magic happens: Your router (with the NAT / PAT in it) is as before connected to the LAN, but to the WAN not longer with 4, but with only this one IP address. You must know that most communication protocols nowadays use IPs and ports. A connection between a computer and a webserver over TCP ip is like the following: PC1:12345 -> Webserver:80 Webserver:80 -> PC1:12345 PC1:12346 -> Webserver2:80 Webserver2:80 -> PC1:12346 Note that through the use of ports two different connections can be managed (PC1 is connected to webserver one with port 12345 and to webserver2 with port 12346). So now what happens in your network? Your pc sends a request like 192.168.0.101:12345 -> google.com:80. The NAT/PAT translates this to 90.80.70.10:10001 -> google.com:80. It saves the translation (192.168.0.101:12345 <-> 90.80.70.10:10001) in its memory. The answer comes back: google.com:80 -> 90.80.70.10:10001. The NAT knows how to translate this back and the packet google.com:80 -> 192.168.0.101:12345 comes to your computer. Works fine. But the interesting thing is: You have a second computer: 192.168.0.102. With this, you also like to connect to google. The process again is 192.168.0.102:12345 -> google.com:80 which is translated by the NAT/PAT now to 90.80.70.10:10002 -> google.com:80. This translation is also saved, note that another port was used so the NAT can distinguish the answers: google.com:80 -> 90.80.70.10:10002 goes back to google.com:80 -> 192.168.0.102:12345 and to your second PC. To conclude, with this technique you can have a huge number of PC in your local network with 192.168.x.y - they can communicate in this LAN normally - and one external global unique IP used for the outer side of your router. And all of your PCs can communicate with the internet because their requests are seperately translated to different ports on the outer router side so the answer can be tranlated back. I hope this helped you, feel free to ask more! :) This theme is indeed very interesting!"}
{"doc_id": 241258, "author": "Frank Thomas", "text": "RFC 1918 defines 3 ranges of IP addresses that are \"private\", for use internally within any given organization. These ranges are 10.0.0.0/8, 172.16.0.0/16, and 192.168.0.0/16. You will note that while many routers use these address ranges for their internal LAN networks, the router WAN interface is assigned a unique publically-routable IP address. when users of the internal network access the internet, they masquerade as the router's public address, using NAT."}
{"doc_id": 155895, "author": "killermist", "text": "Part 1: There is none. If your screen resolution is 1024x768, and the image is 1024x768, then they are a perfect match. Part 2: Not counting menus or other things that permanently take up some of your screen, yes, if the image is the same size as your resolution, it should fit perfectly. Having said that, most \"desktops\" are smart enough to know that the \"start menu\", and its bar will take up a certain space and just overlap that area. Part 3: This doesn't take into account using a non-optimal resolution for the display you're using. If you're using a \"wide screen\" (usually 16:9 or 16:10) display with a \"regular\" (like 1024x768, which is 4:3) resolution stretched to wide screen, the entire desktop will appear distorted, possibly including a background geared toward whatever resolution you're using. Physical limits: Between the monitor, and the video card, there will be some maximum resolution the pair can do. If the highest resolution a monitor will support is (just for example) 1280x1024, then forcing the video card to send a higher resolution will most often get you a pretty \"signal out of range\" message from the monitor, and it will display nothing else until the resolution is back in its capable range. Video cards are good at polling a monitor (one of the things Plug & Play got right) to query for a capable resolution list. The OS will get this list (of which the video card might not be capable of some, if it lacks enough video memory), and should only offer resolutions that the video card and monitor are, in tandem, capable of. If the OS is picking up the wrong monitor, or the wrong video card or video card driver are used, this could limit you to less possible resolutions than the hardware is actually capable."}
{"doc_id": 155896, "author": "Nam Phung", "text": "Yes it will if they have the same resolution. If they have smaller resolution, the picture will be blur if you make it fullscreen. If the picture have different aspect ratio, it'll be stretch when make background."}
{"doc_id": 155897, "author": "fmanco", "text": "The screen resolution concerns the number of pixels in a display (check here). The image resolution can refer to multiple concepts, but is used to describe the image detail (check here). If your are refering to image resolution as the image size in number of pixels, and if you're seeing the image in full screen mode, so yes, the image will fit your screen. If you're refering to image resolution as the number of pixels-pre-inch, so no, this is not true."}
{"doc_id": 155895, "author": "killermist", "text": "Part 1: There is none. If your screen resolution is 1024x768, and the image is 1024x768, then they are a perfect match. Part 2: Not counting menus or other things that permanently take up some of your screen, yes, if the image is the same size as your resolution, it should fit perfectly. Having said that, most \"desktops\" are smart enough to know that the \"start menu\", and its bar will take up a certain space and just overlap that area. Part 3: This doesn't take into account using a non-optimal resolution for the display you're using. If you're using a \"wide screen\" (usually 16:9 or 16:10) display with a \"regular\" (like 1024x768, which is 4:3) resolution stretched to wide screen, the entire desktop will appear distorted, possibly including a background geared toward whatever resolution you're using. Physical limits: Between the monitor, and the video card, there will be some maximum resolution the pair can do. If the highest resolution a monitor will support is (just for example) 1280x1024, then forcing the video card to send a higher resolution will most often get you a pretty \"signal out of range\" message from the monitor, and it will display nothing else until the resolution is back in its capable range. Video cards are good at polling a monitor (one of the things Plug & Play got right) to query for a capable resolution list. The OS will get this list (of which the video card might not be capable of some, if it lacks enough video memory), and should only offer resolutions that the video card and monitor are, in tandem, capable of. If the OS is picking up the wrong monitor, or the wrong video card or video card driver are used, this could limit you to less possible resolutions than the hardware is actually capable."}
{"doc_id": 155896, "author": "Nam Phung", "text": "Yes it will if they have the same resolution. If they have smaller resolution, the picture will be blur if you make it fullscreen. If the picture have different aspect ratio, it'll be stretch when make background."}
{"doc_id": 155897, "author": "fmanco", "text": "The screen resolution concerns the number of pixels in a display (check here). The image resolution can refer to multiple concepts, but is used to describe the image detail (check here). If your are refering to image resolution as the image size in number of pixels, and if you're seeing the image in full screen mode, so yes, the image will fit your screen. If you're refering to image resolution as the number of pixels-pre-inch, so no, this is not true."}
{"doc_id": 111587, "author": "Michael Borgwardt", "text": "Wikipedia has a timeline of \"generations\", CPU models and new features introduced in them. But note that its generation count already disagrees with yours (it goes up to 9) and doesn't correlate with any clear technical property such as bus size or CMOS process size. IMO the word \"generation\" as applied to CPUs is poorly defined and marketing driven - just a fancy way to say \"this isn't just a new model, it's fundamentally better, so you must buy it!\""}
{"doc_id": 111588, "author": "KCotreau", "text": "This question is very broad, but the often the biggest difference is that they have been able to miniaturize the transistors, allowing for less energy usage, and less heat. Of course, they can also change features, but this will vary with every comparison you could put forward. Here is a comparison of the i3: Intel introduced the 1st generation Core i3 processors in 2010 and the 2nd generation Core i3 processors in 2011. The 2nd generation Core i3 processors are built on the Intel’s Sandy Bridge architecture, which is 32nm microarchitecture, while 1st generation Core i3 processors were built on Intel’s Nehalem architecture. Additionally, 2nd generation Core i3 processors include new features for improving the graphics performance of the processors such as Intel Quick Sync Video, Intel InTru 3D / Clear Video HD and WiDi 2.0 that were not available in 1st generation Core i3 processors. http://rapidhow.com/2011/06/12/intel-core-i3-vs-2nd-generation-intel-core-i3-processors/"}
{"doc_id": 111589, "author": "A Dwarf", "text": "The term Generation is loosely applied to Intel processors to mean new and significant developments in processor architecture or functions. A second generation Core processor is the family of processors known as Sandy Bridge, which among other things introduced shared cache and placed the memory controller, graphics and CPU on the same die. The first generation was composed the Core i3, i5 and i7 processors launched early last year. The term can also be seen sometimes applied to processor families. The Core family of processors can sometimes be talked about as having had 3 generations (Lyndfield, Clarkdale and Sandy Bridge). But some processors like the i7, had more or different families (the Bloomfield and Gulftown). Example. It can also be used to name different factory models within a similar architecture. Again, sticking to Core processors, The Core 2 Duo, Quad and Extreme being mentioned as one generation different than the i3, i5 and i7, while the sandy Bridge being the 3rd generation of Core processors. Example All in all the term is not officially connoted to the media. Intel does seem to favor the term as meaning significant architecture and factory processing changes within the same family of processors. And they are ultimately the ones deciding what is named 2nd, 3rd, 4th and so on generation. They do it often. But the term has come to mean other things too, since Intel itself has never tried to enforce it. Hence being a term that can have both official and loose connotations, depending on the context."}
{"doc_id": 159444, "author": "tim", "text": "the progressions from generations by my thought are just the difference in architecture.. bigger the architecture, higher the battery consumption and vice a versa. this could be a factor that could influence your decision in buying a laptop/PC. as would the clock speed of the processor, size of cache and no of cores the processor has.. My question would be whether to go for a Core i5 2nd gen or a core i3 3rd gen ignoring personal utility and factoring in cost ( a core i5 is on an avg abt 80 -100$ more than an i3)??"}
{"doc_id": 159445, "author": "Louis Waweru", "text": "Intel's processors have a Tick-Tock cycle. That is, processors released during a tock will have enough notably different design features from the previous tick to be considered a new microarchitecture. Those released during the tick are generally the same tock design but having undergone a die shrink, with some minor features added."}
{"doc_id": 166105, "author": "siddhant", "text": "Suppose 2nd generation is first step of intel,then 3rd generation will be the improved (fixes all bugs,errors, graphics improvement,clock speed improvement,and the overall processing speed improvement.)"}
{"doc_id": 111587, "author": "Michael Borgwardt", "text": "Wikipedia has a timeline of \"generations\", CPU models and new features introduced in them. But note that its generation count already disagrees with yours (it goes up to 9) and doesn't correlate with any clear technical property such as bus size or CMOS process size. IMO the word \"generation\" as applied to CPUs is poorly defined and marketing driven - just a fancy way to say \"this isn't just a new model, it's fundamentally better, so you must buy it!\""}
{"doc_id": 111588, "author": "KCotreau", "text": "This question is very broad, but the often the biggest difference is that they have been able to miniaturize the transistors, allowing for less energy usage, and less heat. Of course, they can also change features, but this will vary with every comparison you could put forward. Here is a comparison of the i3: Intel introduced the 1st generation Core i3 processors in 2010 and the 2nd generation Core i3 processors in 2011. The 2nd generation Core i3 processors are built on the Intel’s Sandy Bridge architecture, which is 32nm microarchitecture, while 1st generation Core i3 processors were built on Intel’s Nehalem architecture. Additionally, 2nd generation Core i3 processors include new features for improving the graphics performance of the processors such as Intel Quick Sync Video, Intel InTru 3D / Clear Video HD and WiDi 2.0 that were not available in 1st generation Core i3 processors. http://rapidhow.com/2011/06/12/intel-core-i3-vs-2nd-generation-intel-core-i3-processors/"}
{"doc_id": 111589, "author": "A Dwarf", "text": "The term Generation is loosely applied to Intel processors to mean new and significant developments in processor architecture or functions. A second generation Core processor is the family of processors known as Sandy Bridge, which among other things introduced shared cache and placed the memory controller, graphics and CPU on the same die. The first generation was composed the Core i3, i5 and i7 processors launched early last year. The term can also be seen sometimes applied to processor families. The Core family of processors can sometimes be talked about as having had 3 generations (Lyndfield, Clarkdale and Sandy Bridge). But some processors like the i7, had more or different families (the Bloomfield and Gulftown). Example. It can also be used to name different factory models within a similar architecture. Again, sticking to Core processors, The Core 2 Duo, Quad and Extreme being mentioned as one generation different than the i3, i5 and i7, while the sandy Bridge being the 3rd generation of Core processors. Example All in all the term is not officially connoted to the media. Intel does seem to favor the term as meaning significant architecture and factory processing changes within the same family of processors. And they are ultimately the ones deciding what is named 2nd, 3rd, 4th and so on generation. They do it often. But the term has come to mean other things too, since Intel itself has never tried to enforce it. Hence being a term that can have both official and loose connotations, depending on the context."}
{"doc_id": 159444, "author": "tim", "text": "the progressions from generations by my thought are just the difference in architecture.. bigger the architecture, higher the battery consumption and vice a versa. this could be a factor that could influence your decision in buying a laptop/PC. as would the clock speed of the processor, size of cache and no of cores the processor has.. My question would be whether to go for a Core i5 2nd gen or a core i3 3rd gen ignoring personal utility and factoring in cost ( a core i5 is on an avg abt 80 -100$ more than an i3)??"}
{"doc_id": 159445, "author": "Louis Waweru", "text": "Intel's processors have a Tick-Tock cycle. That is, processors released during a tock will have enough notably different design features from the previous tick to be considered a new microarchitecture. Those released during the tick are generally the same tock design but having undergone a die shrink, with some minor features added."}
{"doc_id": 166105, "author": "siddhant", "text": "Suppose 2nd generation is first step of intel,then 3rd generation will be the improved (fixes all bugs,errors, graphics improvement,clock speed improvement,and the overall processing speed improvement.)"}
{"doc_id": 1263, "author": "moshen", "text": "Unfortunately a splitter will just mirror the display on 2 monitors. You will need something like the dualhead2go devices from Matrox to do what you're talking about."}
{"doc_id": 1264, "author": "Brettski", "text": "You cannot split a single DVI port to two monitors. I have seen splitters on video cards (like the Dell I am on now), but it's not connected to a DVI (though it looks roughly the same size), but another style of connector to two DVI's. I do recall seeing a splitter from a DVI to two DVI's but it wasn't used for two monitors but for a monitor which required two DVI inputs."}
{"doc_id": 1266, "author": "NicJ", "text": "Your card has two Dual Link DVI connectors (see Wikipedia page here). However, a Dual Link DVI connector cannot output two independent displays -- it is used for driving higher-resolution monitors (2560 × 1600 as opposed to 1920 × 1200 on Single Link DVI)."}
{"doc_id": 1302, "author": "CesarB", "text": "It is quite possible that the cables you saw were not plugging into a normal DVI port, but into a DMS-59 port (especially if you saw it on a Dell). DVI and DMS-59 look quite similar unless you look close enough, but the DMS-59 connector has in fact the equivalent of two full DVI connectors, and thus needs the split cable (you cannot directly plug a DVI cable into a DMS-59 port; it will not fit). These ports are normally used so a smaller card can be used for two DVI or VGA monitors (Dell likes using half-height cards on some models)."}
{"doc_id": 77961, "author": "Pierre", "text": "I have an iMac and HP 2009m monitor. I just installed the mini DVI cable to VGA from imac to HP monitor. I didn't have to make any change on Mac side every thing is working perfectly. On Windows 7 I was having some issue where I have the duplicate image and could really use the mouse interdependently from a monitor to the other. Here is the solution: In Windows 7: Right click your Desktop Click Screen resolution Change the appearance of your desktop Display, I choose the imac Monitor Color LCD) Resolution, 1980 8 1080 depends what urs is. Orientation: Landscape Multiple Displays: Extend these displays Click Apply Ok..Done."}
{"doc_id": 146830, "author": "Pierre", "text": "You actually can split the DIV-I to two monitors. We do it for the wyse system here all the time. You can use the 920302-02L to split the DVI-I to DVI-D and VGA. there is a setting in the wise that we trun on for this to work. However You can not use a DVI-D to VGA converter to get output to two VGA computer all it is going to do is clone the screen. I am guessing because you can only send one analog and one digital not two analogs. Good luck."}
{"doc_id": 147065, "author": "MikeyLikesIT", "text": "You can use a DVI spitter and extend the desktop. I have been doing it with a Radion 6xx half space card for years on a Dell optiplex. But now that I had to reformat this box due to virus, I now can only get the monitor to clone. I failed to bookmark a page I found about 2 years ago with the solution. I am vague on memory on what needs to be done but the problem is in the monitor INF that windows generates automaticall where it is looking for vga instead of DMI because I have an onboar vga port. Even though an external DMI card is installed which automatically disables the onboard VGA port, windows is still looking for VGA. The trick was a process of uninstalling the monitor entry in device manager and be sure delete the driver too when prompted then let the system boot to write a new monitor INF. But there was a specific procedure to follow to get this to work. Once I figure it out, I wil post to this forum."}
{"doc_id": 147068, "author": "MikeyLikesIT", "text": "In my case I bought a used half space Radion X600 series DVI card with a Y DVI splitter from Ebay for my Optiplex that is running Vista. Vista ends up installing ATI Radion X300/x550/x1050 driver. I also installed the ATI Catalyst Control Center. Problem will be that extended monitor config will work but not after reboot or after computer is logged off until I FIX... Go into device manager, uninstall the two monitor entries and also check box to delete the driver which will appear after uninstalling the first monitor entry. Once both monitors are uninstalled the monitor entry in device manager is gone. Shut down the computer, restart...computer should come up with both monitors working but cloned by default. Configure the displays for extended desktop. Once it is working as desired, go into profile manager, delete any existing profile and create a new profile, name it...save it...activate and close. Reboot the box to confirm that the configuration sticks. Problem solved."}
{"doc_id": 10001615, "author": "Spidercat", "text": "Based on this dog food calculator. A full grown mastiff will grow to be about 150-200 pounds1. Which means a 180 pound mastiff2 with what the site considers a typical activity level, and food with a kcal amount of 320 per cup3. Youre looking at roughly 10-11 cups of food twice a day. Alternatively, that would be about 2 pounds of food each meal. Multiply 4 pounds of food with 7 days in a week and that means youre looking at 28 pounds of food each week. Now thats a pretty rough estimate since you dont have the dog (or at least not yet), but you can play around with the calculator to figure out if those numbers are anything that you think you should be worried about. Note that if your dog happens to be larger, or even just more active, than the average it could very well need more food. And, depending on the type of food you purchase, it might need to eat more in order to receive the calories it needs in a day. The quality of food makes a difference, and the prices of food is related to the quality. In my opinion, if you have any concerns about being able to afford a 30 pound bag of dog food each week, then you should avoid large breed dogs. Keep in mind that you should be prepared to be able to afford to take your pet to a veterinarian if need be. That is an expense on top of the others. You should never have to skip feeding your pet in order to afford a vet bill. Also, if it turns out its allergic to certain ingredients you should be prepared to buy more expensive brands of food that exclude those ingredients. 1 http://dogtime.com/dog-breeds/mastiff 2 My estimated average weight based on the breeds weight range. 3 Based on the contents of Purina One healthy weight adult dog food."}
{"doc_id": 10001641, "author": "TheCO2", "text": "It really all depends on what you feed the dog. A mastiff is going to eat a lot, no matter what you feed him, but the better the dog food, the less he is going to need to eat. People look at cheap dog food as less expensive, but that is only true at the register. If you are buying really good dog food, you are spending more, but buying it less, so it really equals out. They will not only eat less, but they will poop less and have healthier skin and coat, which will ultimately lead to less shedding and thats a definitely plus, with a Mastiff. Anything with corn, wheat and soy is not considered to be real good dog food. Feeding dog food with corn is like giving your dog a bowl of Fruit Loops, twice a day; its pure sugar. Also, think about the last time you ate corn and how it came back out. As Matt was saying, there are special needs dogs that do need things like grain free dog food, or something other than chicken, for the main protein source (lamb, beef, venison, fish, etc) and thats where it really starts getting expensive. A dog owner should be prepared to spend a little more to keep the dog healthy. Its worth it, in the end."}
{"doc_id": 10002435, "author": "Tazzy", "text": "You answered your own question,, you cant afford a lot, I have 5 dogs all different breeds my biggest boy is 47kg, Rottweiler cross ridgeback he eats a lot, I have a bullmastador puppy shes 15 weeks she eats a lot, shes weighing 20kg already the vet guesses she will be over 50kg when adult, they eat a lot, vets bills are expensive, so I would carefully think about it before taking one giant breed dog on"}
{"doc_id": 10002466, "author": "rey", "text": "I breed mastiffs so I will try to help you with this: Age Amount 4-8 weeks 3-4 cups per day spread between 3-4 meals 8-12 weeks 4-6 cups per day spread between 3-4 meals 12-16 weeks 6-8 cups per day spread between 3-4 meals 4 to 6 months 8-10 cups per day spread between 2-3 meals 6-18 months 8-12 cups per day spread between 2-3 meals These measurements are in another web site but I have been following this rule for years now. OK my point is, as a breeder if you have not even got the puppy yet and you are already saying that you cannot afford to spend too much in dog food do yourself a favor and dont get the dog. Why? English mastiffs need high quality food; their growth and life quality depends on it. Lets say you buy cheap dog food filled with byproduct stuff; now that dog is going to grow so fast and fat that, trust me, you are going to have an unhealthy dog for life. You already damaged the dogs joints etc. Now we are not talking about $5 or $15 dollars more on food. Now its about thousands of dollars on surgeries, therapies, and a life full of pain for your dog, you see is like a chain reaction. This is a good example: Blue Buffalo the biggest bag they sell is 30 pounds for $55 plus tax, so about $60. Each of my English mastiffs eat that in a 2 week period, so each mastiff will cost you just on food $120 a month. Now throw the vet bills etc. I am not saying that you have to feed them Blue Buffalo; there are other good quality dog foods out there, but this is the one I use and the one I recommend as the owner of 6 baby giants. I hope this has helped."}
{"doc_id": 10004619, "author": "Crystal", "text": "I have a 2 1/2 year old bull mastiff. I have been feeding him Blue Buffalo since I got him at 8 weeks old. He is very lean and active for a bull mastiff. I feed him about 3-4 cups twice a day which averages about 130.00 a month. It is very important that you can afford this breed. I just had to take mine to the vet because he developed some sort of allergies on his skin, eyes and ears and the vet bill was $210.00 with all his medication. This wasnt an option I had to take him an get all his meds or he would have gotten worse. Theyre just like your children, you can expect things to come up and you need to be able to care for them."}
{"doc_id": 10000619, "author": "GrandmasterB", "text": "Amano shrimp are good tank mates for community fish. Theyll ignore your fish altogether. And they eat algae 24x7, which never hurts. Amano shrimp require brackish water for breeding, so wont breed in most tanks. This also makes them difficult to find. Cherry shrimp (and their color varieties) will also be no threat to your fish. But, they are very small, so aggressive fish (barbs, for example) may go after them. They breed quite easily and rapidly, so if you want more of them make sure you have plenty of hiding places for the young shrimp. Cherry shrimp are also algae eaters, though being so small youll need huge quantities of them to have an real impact if algae controll is a goal. Really, for most shrimp in tanks, the issue isnt if they are a danger to the fish, but if the fish will be a danger to the shrimps. Even larger shrimp may find their extremities and tails the target of nipping. The tetras shouldnt be a problem for the shrimp, but the loaches may go after them. Edit: One heads up about shrimp (and most aquarium invertebrates, actually). A lot of medications and chemicals you might use in a tank are poisonous to them. So once they are in there, youll need to be extra careful with what you put into the tank, and check that it is safe for shrimp."}
{"doc_id": 10002053, "author": "Uicerainbow", "text": "I prefer ghost shrimp. You can get a load of them (depending on tank size) and theyre pretty much clear so its cool to watch food and stuff go throughtheir system. They are cheap (49c a shrimp at Petsmart) and can get pretty big and eat all extra food, algae and dead fish.plus I never see them get near my other fish(keep far away from predator fish)"}
{"doc_id": 10003811, "author": "Justin", "text": "The key word you used was Loaches. Any shrimp you put in that tank are immediately an expensive fancy dinner. Loaches will flip them and rip them out of their shells just like they were snails. Loaches and invertebrates just dont mix. Mollies will eat all but the largest shrimp as well. Ive had this experience personally as I watched one devour my red crystals."}
{"doc_id": 10004573, "author": "user7955", "text": "I actually have 4 Yoyo loaches, two cory dorus (SP) and 5 cherry shrimp. So far, for the past 3 months, my shrimp have been fine so far. My tank is heavily planted with moss and various other plants which obviously helps. Also, my Yoyos are well fed and there are plenty of pest snails to keep them happy - shrimp can move pretty fast when they want to so its easier for them to eat the snails than chase the shrimp. Good luck"}
{"doc_id": 10007323, "author": "Brett Sampson", "text": "I made the mistake of keeping Kouli Loaches in a 55 gal heavily planted tank with Amano shrimp. I added several shrimp 3 times and they would just disappear. I thought they were just finding good hiding places until late one night I was watching and one of my loaches attacked and ate an Amano shrimp. They might be compatible at first until the loaches grow large enough to eat the shrimp."}
{"doc_id": 10002837, "author": "ATG", "text": "Serpae tetras tend to be less aggressive if kept in a larger group, like more than 6-8. This does not ensure the occasional nip at a peaceful cory. Kindly do a bit of research before grouping together fish in a community tank. There are a number of online resources to find out compatibility between different fish. A community aquarium needs fish which can tolerate a similar range of water temperature, pH, and gH/kH. Once this particular need is matched, you need to know about aggressive fish species. Corydora catfish need non aggressive tank-mates, and they MUST be kept in groups of 6 or more. Cory cats get very very stressed if kept in a lower number."}
{"doc_id": 10002838, "author": "Quillion", "text": "Add 6 more tetras. This will increase your tanks population and thus reduce the new fish you can add, but unless tetras are in groups of at least dozens this will happen. Additionally try to increase frequency of water changes and when adding new water try to have it one degree colder than your tank water. Tetras are the type of fish that love rain and feel more calm after a rain. As a matter of fact tetras in the wild spawn only after good rain. Your tank is already planted, so you are covered there."}
{"doc_id": 10003457, "author": "Kaka", "text": "I have 8 serpeas in a 5 gallon tank. At first I had 4 and then they were very aggressive. Then when I added 4 more, it was only one that was aggressive. When I removed him, none of the others where getting hurt. You should remove the aggressive ones then that should stop the nipping. I hope this helped!"}
{"doc_id": 10008406, "author": "Speedmaster", "text": "Serpae tetras are by nature fin nippers. They are only second to tiger barbs. Although I always wanted to have them again, I restrained myself knowing what they are capable of. It is just their nature. The docile Neon tetras can become aggressive to each other if you have fewer than half a dozen of them. Ive seen aggression in Neons in my small 5 gallon tank where I kept 1 betta, 4 neons, 1 cory and 2 black neons. The aggression stopped when I add 2 more neons. My 5 gallon tank is mainly just for the betta and the tetras are just tank mates. Bottom line, if a normally docile neon tetra becomes aggressive with few numbers how much more to a naturally aggressive Serpae. Mind you my neon tetras came from a larger population in my larger tank."}
{"doc_id": 10005159, "author": "Diether", "text": "Neon tetras will indeed eat baby shrimp. So they can be used for population control. But keep in mind that you do have to try to keep some of the babies alive. Otherwise you might end up without any shrimp after a while. From your other post, it seems like your tank wont have many hiding places for the shrimp. So you might want to add a few real plants and moss (shrimp will love this) and a shrimp cave. Especially when shrimp are shedding their skin, they are more vulnerable and also need a more quiet place to hide."}
{"doc_id": 10005462, "author": "Christy B.", "text": "Here is a PDF link about dog and cat skin thickness. It states that dogs have a thickness of 2.6-5.2mm , while a cat has 0.4-3.6mm skin thickness. One should also take into consideration cats skin is more flexible and hangs off of the muscle, so that injuries are more superficial. Skin and Coat Jill Cline, Ph.D."}
{"doc_id": 10001944, "author": "Spidercat", "text": "Some people will say that there are ways to tell the sex of a bearded dragon based on its behaviour. Males in general might be more likely to show their beard, females in general might be more likely to wave their arms in greeting. Ive even heard people saying that female bearded dragons are smaller than males. While these may be true sometimes, its important to remember that these are all anecdotal, and isnt a definitive way of determining the sex of your bearded dragon. The only sure way is to look at what are called hemipenal bulges. While I wouldnt say that determining the sex by looking at the hemipenal bulges is 100% accurate, they are the most accurate method of sexing your dragon (Right after finding it laying eggs). Note: The male hemipenal bulges might be bulges might not be developed enough to see until the bearded dragon is grown 6-7 inches in length. Until then, it will probably look very similar to a females. The hemipenal bulges on a male bearded dragon will be two bulges, one on each side of the tail following the vent. (Source) The hemipenal bulges on a female bearded dragon will be one single bulge in the center of the tail following the vent. (Source) If you are having trouble determining which sex you have, it can actually help a lot to take a small flashlight and shine some light through the tail. You should see the shadows of the testes if its a male. (Source) Bearded dragons are one of the lizards who have Femoral Pores. Commonly referred to as preanal pores. These are used to secrete pheromones, especially during mating season. These pores are located on the underside of the hind legs, running from knee to knee, and are generally more prominent on males. Though anecdotal, if youre unsure of the hemipenal bulge it might be the best you can use. Now, lets talk about the elephant in the room. Its called probing. Probing is the practice of forcing open the vent, and pulling the sex organs out, either with a pair of tweezers or what is essentially a pair chopsticks, in order to view them. If anyone suggests that probing your bearded dragon is an acceptable way of sexing your dragon, the proper response is to repeatedly smack them over the head with a rolled up newspaper until they leave in shame. Probing bearded dragons, or any lizard really, carries a huge risk for causing damage. It is one thing to force open the vent and pull the organs out, and another to put everything back in and close the vent without causing any damage. If anyone were to be able to successfully probe your bearded dragon, it would be a veterinarian that has a long history of treating reptiles. While I normally place a pretty high value to breeders, I cannot condone anyone performing this method of sexing lizards who has not dedicated their lives to the medical care of reptiles. Common issues that arise from probing bearded dragons include but are not limited to: Internal bleeding from pinched sex organs and/or intestines. Bruising of the sex organs and/or intestines. Prolapse, and increased risk of prolapsing in the future, from damage to the vent and/or intestines. Infection(s) from exposing the tissue thats normally protected by the vent. Note: Probing snakes poses less risk than lizards, though I still disagree with it as a method personally. But because of it, probing kits are sold. I have seen people buy these kits thinking they can use them for lizards, and people selling them to people to use on their lizards. This should never be done for any reason."}
{"doc_id": 10006311, "author": "Emma evans ", "text": "Just a note, try not to hold the tail like that, they are a lizard and so can perform autotomy and deglove or drop their tale as a defence mechanism and it will not grow back as it used to be, it will be shorter and more like cartilage. Males tend to show more aggressive behaviour such as head bobbing and turning some areas of the body black. Though females have also been known to blacken their beard as well as bob their head in display of dominance."}
{"doc_id": 10002686, "author": "Jestep", "text": "Return the goldfish and get a beta. Thats realistically the only fish that can survive long term in a 2.5 gallon aquarium. Your typical comet goldfish will need at least 30 gallons of water each to stay healthy long term. Theyre very messy fish and even in a small but suitable tank, owners often have major water quality issues. Before you get another fish, read up on how to properly cycle a fish tank. If you take a fish tank, regardless of the size, and just put water and conditioner and fish in it, most fish will be dead in about a week due to ammonia poisoning. An aquarium must cycle before it is ready to house fish. Cycling is the process of building up beneficial bacteria. This bacteria is what actually filters the water in any fish tank, beit freshwater or saltwater. Cycling a tank typically takes around 4 weeks and is complete when ammonia and nitrite can no longer be detected in a tank, while nitrate is detected and stable or rising. Petsmart is notorious for employing untrained people in their aquatics departments, and they routinely sell customers fish and aquarium setups that are impossible to maintain. In our aquarium club its probably a weekly or bi-weekly event that someone shows up trying to keep their tank alive because petsmart or petco sold them a setup that was impossible to keep going, or even get established, and theyve lost all their fish multiple times as a result."}
{"doc_id": 10002688, "author": "Spidercat", "text": "Jestep has a good answer, but sometime it helps to be able to visualize things. With the exception of a few show goldfish, which will only grow to about 10 inches in length, an adult goldfish is going to be about 14-18 inches long. You didnt say what species of goldfish you had, so Im assuming its the common comet goldfish that all pet stores sell. They will easily be at the upper end of the scale as long as you can keep them alive. To put this into perspective, a standard 2.5 gallon tank is usually only 13 inches long1, although they can be shorter if you buy a special kit. This means that for a tank to be big enough for a goldfish, you pretty much need to be able to wave a 2.5 gallon tank around the inside of it. Most tanks, aside from a 40g breeder tank, are only 13 inches deep, meaning the goldfish wouldnt be able to turn around without bending its spine too much. Goldfish are active swimmers so they need to be able to swim back and forth as they feel like it. I would take the fish back to the pet store, they should have no problem with you returning it, although they might only give you a refund if its been within a certain time frame. You could probably get a betta for the tank, although in my opinion a 2.5 gallon tank is still a bit small for them. I would personally turn it into a nano tank with some plants/moss and some shrimp. Thats personal preference though. What youll really want to do is look around at your local pet store, and see what fish they have available. Then be sure to look up how large the fish grows to as an adult, and if its a schooling fish that will require tankmates. Dont be afraid to visit just to write down names to look up at home. 1For reference, here is a site with a list of tank dimensions based on their size in gallons/liters: http://www.anapsid.org/resources/tanksize.html"}
{"doc_id": 10002728, "author": "Dalton", "text": "There are a few factors in determining tank size. One of the biggest factors is surface area. The way it works is that fish pull oxygen from the water. The less surface area there is, the longer it takes for the oxygen to exchange from the surrounding air into the water. If you have too many fish breathing, then they use up all the oxygen. As an experiment, put your self completely under a heavy blanket. After a while, youll notice that it becomes harder to breath. This is because youre burning up the oxygen faster than it can come through the cloth. Thats what will happen to your fish. Theyll slowly asphyxiate if there isnt a good surface area to fish ratio. You can find many calculators to help you with that. Here is one: Fish Calculator Thats just the oxygen. The next is water quality. Both can be improved by circulation through a filter. Fish are messy animals. They waste large portions of their food, they pee and poo everywhere, and generally make a mess. Just like a person living in their own filth, it will eventually effect their health. They cant help but come into contact with their own waste, so they arent going to sicken and die like a person might when exposed to cess. However, its not as hard as it sounds. In the wild, there are all kinds of things helping to process that waste out of the water, from sun, to other animals, to plants (who not only clean waste, but produce oxygen). To fix this in a closed system, you have filter systems. These suck up the solid waste to a degree, filter the water to remove ammonia and other harmful chemicals, and return the water, which agitates it and helps put oxygen back into it. You can add bubblers and real plants to help deal with these issues as well. The fish will also continue to grow throughout its life, since fish have something called indeterminate growth. You can look up the adult size of your fish and determine if there is enough room in your tank. I found two good articles here: http://injaf.org/articles-guides/do-fish-grow-to-the-size-of-their-tank/ and http://injaf.org/articles-guides/understanding-fish-stocking-guides/ So to sum it up, there a various factors that determine the fish that can go in a tank. Factors to keep in mind are adult size, oxygen levels, waste removal, and aggression, though that probably doesnt apply to you. I think that at minimum youll want a tank with some type of filtration system and not one thats stagnant. You could also get away with a smaller tank if you traded in your fish when it started getting too big for the tank. I can tell you that Ive done that many times. A pet store will often give you smaller versions since they can make more money off of selling your larger fish. If not, you have a way to get rid of your larger fish and keep the smaller. Something Id recommend is getting a 5-10 gal tank (many retail stores and pet stores sell a 5gal hex with a built in filter, and buy some naturally smaller fish. I like the small colorful ones anyway. Guppys are great. Theyre super colorful and are small. They also breed easily, so you can have babies. Thats a different can of worms, though. You can also get neon tetras, platys, swordtails, mollies, etc... You can also get a cool pet like a crayfish or fresh water crab. They dont need much space, just a place to hide (theyll create one if you put an object like a hide in their tank), flowing water ( a filter return is fine), and food to eat. Mine ate fish food, fish waste, and the random muscle I would catch at the river. Id catch them at the creek and keep them for a couple of years. They molt and get bigger every couple of months. Another cool feature is that they turn colors to match their surroundings. Not like a chameleon, but I caught one that was a 1/4 long and it was jet black. I had white gravel in the tank. He got lighter at every molt and by the time he was 2-3 long, he was blue-white. He started killing my fish and had to go, but he was cool and I caught a smaller one to replace him. They clean fish waste and can be kept with fish if there is plenty of room. They are predators and will hunt them eventually. Good luck."}
{"doc_id": 10008194, "author": "virolino", "text": "I do not have experience with golden fish, so I cannot help there. However, you also state: Their water looks cloudy, and they keep going to the surface for air. in the following context: I recently purchased two small goldfish for my son. We were instructed by the employee at Petsmart that the 2.5 gal (ca. 10 l) tank would be sufficient. This means that the aquarium is not properly set, and you do not have much experience. I answered shortly what does it mean to have an aquarium here. The most important device that you need is the mechanical filter. The most important activity at the beginning: cycle the aquarium. If you do the things properly, in the end you will only need to add water when it evaporates, and to clear the excess dirt in the mechanical filter. What the other friends here told you is true: if the aquarium is too small / overpopulated, you will never achieve a good balance."}
{"doc_id": 10004924, "author": "PudgeBlossom", "text": "Ive gone through two snails with my betta over the last few years. Ive never seen the betta care about the snails presence. I would think the snail probably died of natural causes. I dont think snails are as robust as bettas!"}
{"doc_id": 10006590, "author": "user10731", "text": "I had a Betta and a snail living together peacefully for ages, then I decided to put the snail in my guppy fry tank, but later on I noticed they kept nipping at it. I think it was more out of curiosity rather than aggression since there wasnt much else to do for them as they were in a breeding container at the time. So I put it back in the Bettas tank and he tried to kill it - so all I could was use my hand to push the Betta away from the snail and take the snail out. I think most Bettas are fine with other fish, snails, etc., if they are fairly chill Bettas and that you add the Betta after you have other fish or in you case snail. Because from what Ive read online with Betta tank mates, that Bettas will create their own territory, so if you add something to that territory if gets territorial and attacks them. Whereas if you add a Betta to a tank that already has fish, the Betta if it chill enough (like by the sounds of your Betta, since he lived with the snail before it died) will just kind of accept the established fish. I dont know if your Betta attacked the snail, but it would be weirded if they have been fine all the other times, or if it just died of natural causes - Im no expert but just thought Id put my opinion out there, hope its helpful."}
{"doc_id": 10006593, "author": "Fish-in-Tea", "text": "Yes, they do. Some bettas really love to eat snails, some do it only if hungry, some do not eat them at all (but may learn it later). Betta fish are strong individuals and they differ in their behaviour and tastes. Obviously, they can attack only small snails. Sometimes they pull them out of the intact shells, sometimes they crush the shell in the process (which can make audible noises)."}
{"doc_id": 10007338, "author": "Pro Q", "text": "My betta fish was continuously bothering my snail, so I decided to do a quick Google search, which brought up these links: https://www.fishlore.com/aquariumfishforum/threads/my-betta-ate-my-snail.267901/ https://www.aquascapeaddiction.com/articles/do-bettas-eat-snails https://www.bettafish.com/102-betta-fish-compatibility/69520-my-betta-ate-my-mystery-snails-antennae-help.html https://www.myaquariumclub.com/will-bettas-eat-snails-24408217.html http://www.aquariumadvice.com/forums/f12/bettas-eat-snails-278502.html After reading them, my conclusion is that it really depends on the betta fish. Some are actively vicious and will attack snails and eat them. Others are totally fine around snails or particular types of snails. It also seemed like some of the cases were similar to yours, in which the betta seemed to live peacefully with the snail for a while, but then suddenly the snail was gone (in that case though, I believe the snail might have been sick and the betta only ate it when it was close to being dead.) In short, yes, bettas can attack and eat snails, but they dont always do so."}
{"doc_id": 10007546, "author": "Angela Sanders", "text": "I have recently added new plants to my 20 gallon (76 liters) betta tank, in which only the one Orange Devil Crowntail CT #1105 lives. He is one bad mojo and does not tolerate anything else in HIS tank. There are some small snails that came in with the plants and anytime he sees one, he is flaring at it. I suspect he will clear out the snails soon."}
{"doc_id": 10007794, "author": "Christy", "text": "Mine does! I had a betta for more than four years and he died a few months ago. He happily shared his 3.5 gallon (13 liters) habitat with some ghost shrimp and a zebra snail. I recently replaced the betta and the new one will not leave the zebra snail alone - always flaring, swimming around him and nipping at his shell. The snail (about 1.5 inch / 3.8 cm long) doesnt seem to care and will hopefully be fine. I dont know what to do to get the fish to leave him alone!"}
{"doc_id": 10007934, "author": "Jennifer", "text": "I had a tank with only mystery snails in it and while I loved watching them I decided to add a beta for a bit of colour. That beta keeps nipping at the snails tentacles and seems to enjoy harassing them. My snails definitely dont like it and have started hiding in their shells all the time. My snails do not like their new tank mate."}
{"doc_id": 10008054, "author": "Caitlin", "text": "I put a mystery snail in with my betta when I read they are good tank cleaners and I was having an algae problem. He lasted about a week before I saw the fish kill him. The snail was up on top of the little tank heater and the betta attacked and knocked him off. I’m not sure exactly what he did but it definitely caused a splash, which is why I looked over. The snail never moved after that, not sure if he died right away, but after a couple days of my usually very active snail just lying in the same spot I took him out since he was obviously dead, which is sad."}
{"doc_id": 10002174, "author": "Swansong", "text": "From what I have gathered the only way to be absolutely certain is by probing which should only be done by a herpetologist. However, that may not always be possible, so you can also determine based on the tail. Males have thicker and longer tails, while the females have short and thin tails. Other details can be the size of the snake altogether since males are normally larger, however some exceptions may be out there. Also, male corn snakes that are on the bigger side often have subtle dark streaks that travel down the length of their bodies -- four in total. These streaks do not generally appear in the smaller males of the species, however. Hope this helps"}
{"doc_id": 10002178, "author": "Spidercat", "text": "A nice way to tell the sex of a corn snake is by the shape of the tail. Reptiles have whats referred to as a hemipenis, and because of the way its positioned in their tail, the males tail will be thicker after the cloacal opening when compared with a female, whos tail will taper immediately after the cloacal. (Source) Another, potentially more reliable method, is by counting the scales after the cloacal. Simply count the rows from the cloacal to the tip of the tail. 130 or less is a good sign that you have a female. 140 or more is a good sign that you have a male. If you count somewhere in between 130 and 140 then its not going to be definitive to say. In this case I would suggest also looking at the shape of the tail for that thicker section. Note: Unless you have an extremely well behaved snake, its going to be hard to count the scales on it. So I would suggest taking a picture, or even using one if its shed skin. Of course these are all going to be reliable to a fault, and aside from putting two snakes together and waiting to see if they mate, the only way to know for sure is by probing or popping the cloacal opening to see the sex organs. As Ive mentioned in one of my other answers Im against these practices because I believe its an unnecessary risk considering the damages that can be caused by it. While its debatable about whether or not snakes are less at risk to the damage than lizards, I would insist that anyone wanting to perform the procedure themselves have hands-on training from someone with experience (and that you can trust) first. Only someone with training should probe a snake. My suggestion is to try the non-invasive methods first, then if youre not convinced after that, you can take it to a reptile vet to have it sexed. As a new pet, its a good idea to have a check-up from a vet anyways, so you can have it checked for parasites at the same time."}
{"doc_id": 10006674, "author": "Maiko Chikyu", "text": "Depends on whether your store imports food or buys food locally. Rabbits like all mammals already have several strains of e-coli in their stomach that is native to their area so they wont be harmed by it assuming the store you got the vegetables from gets their food locally which is often the case. If the vegetables have imported tag then you should be worried. As an exception baby rabbits have weak immunity systems and they may not have gotten the strains from their mothers yet so if your rabbit has babies then those babies might be at the risk of e-coli."}
{"doc_id": 10006676, "author": "James Jenkins", "text": "Will Can my rabbit be harmed by e-coli on Lettuce? Probably not; the majority of store bought foods are safe. While there are risks, your bunny and you have about the same risk (assuming developed country) they really are not any higher or lower risk from food contamination then you are. Both you and your bunny have a gut populated by helpful bacteria including E-coli. Can my rabbit be harmed by e-coli on Lettuce? Yes, there are several different strains of E-coli (Escherichia coli) most are harmless and can found in intestines of warm blooded animals (including people) in many cases these are helpful bacteria. BUT some strains like Escherichia coli O157:H7 which is currently and often in the news is harmful. Results of infection in rabbits is much like that in humans; starting with diarrhea in 1 to 10 days, with other complications that can and do lead to death, particularly in the very young and old. Rabbit can become infected with harmful strains of E-coli in the same ways as humans. As with humans they can also pass it to others. Rabbits and humans can pass the infection to each other. It is important for the health of you and your rabbit that you take the same precautions when feeding them leafy greens, as you would take if you were consuming them yourself. All the different strains of E-coli are spread by the same several ways; water, organic fertilizer, bird droppings, human handling, etc. The different strains are not regional, there are carriers of all them all around the world. While a particular regional source is often identified in news reports for mass events, every raw leafy green vegetable is potentially infected. References E.coli (Escherichia coli) at cdc.gov Reports of Selected E. coli Outbreak Investigations at cdc.gov Escherichia coli and Diarrhoea in the Rabbit, PDF at sagepub.com Enteropathogenic Escherichia coli Prevalence in Laboratory Rabbits at ncbi.nlm.nih.gov Bacterial enteritis in rabbits at medirabbit.com Wild Rabbits as potential carriers of E. coli, VTEC – Final Report at hse.gov.uk"}
{"doc_id": 10007767, "author": "romerojr", "text": "Himalayan salt is typically used for cooking.. I would recommend you go to your local pet store and buy some aquarium specific salt. Like this one. 3 bucks and you know youre getting exactly what you need! Please dontt use the cooking salt/seasoning salt, use fish salt!"}
{"doc_id": 10001152, "author": "Spidercat", "text": "While its sometimes normal to see your snakes skin between the scales immediately after feeding, thats because they swallow their food whole and its pressing on their skin. If youre seeing the skin between the scales otherwise, then your snake is definitely overweight. I wouldnt use the spine as a reference for whether your snake is over/under weight, as all snakes, except for the obese ones, will still show their spine. A more accurate method is too look at the shape of the snake. Despite how theyre portrayed, they arent actually round. A snake thats underweight will be more triangle shape, with the sides leading straight from the belly to the spine. A healthy snake will be rounded at the top, but flat on the bottom, kind of like a half circle. The purpose is to have as much surface area (for traction) on their stomach as possible. An overweight snake will be rounded, sometimes almost perfectly, with sides of the belly not able to touch the ground. Another thing you might be able to notice are hips/rolls on you snake. All snakes will have creases when theyre coiled up, but an overweight snake will have more fat in those creases and it can cause noticeable rolls, or even misshapen scales. The misshapen scales are generally the worst cases I think, they come from the pressure of the extra fat in the creases. (source) [Notice here too, the stomach doesnt lay flat] As far as feeding schedules go, you should always keep a strict feeding schedule for all pets. Its especially important for corn snakes since theyre opportunistic feeders, theyll eat even when they shouldnt. Common feeding schedules for adult corn snakes are more like once every 12-14 days. That might seem like a pretty open schedule, but that all depends on what youre feeding him. Larger or fattier food needs to be fed less often. I would say start by feeding him only once every 12 days, and give him some exercise. You can give him some exercise by having him climb up and down a set of stair if you have some, or if he doesnt hate baths, you could encourage him to swim every so often in the bathtub. One trick I really like, is giving them a box full of cardboard tubes, from paper towels and stuff. Just the new environment will encourage them to explore, and youll have fun watching him explore the maze of tubes. It will take quite a while to see some improvement. Just like with humans, snakes wont lose weight within a week. I know some cases where its taken a whole year for people to get their snakes back to a healthy weight. Stick to a longer feeding schedule, and encourage your snake to exercise as much as possible, and hell be fine."}
{"doc_id": 10004174, "author": "keshlam", "text": "Yes. Just like us, cats vary in size and shape and weight. And like us, some of that is diet, some is health, some is genetics, some is age. One of my ladys cats was the runt of the litter; it wasnt certain she would survive, and she has always been both small and skinny. It hasnt seemed to limit her climbing/jumping much, if at all; I think she benefits from square/cube law to be stronger relative to her weight than you would expect. Another cat in the family was not only longer/taller/broader but also more solidly muscled. I think he may have weighed twice what the small one did, without being overweight. (The simplest rule-of-thumb test for whether a cat is overweight: if you can count every vertibra as you run your hand down their back, theyre fine. If some or all of them are hard to feel through the skin, theres probably more fat there than there should be. The runt really is below ideal weight; not only can you feel every bone in her back, its somewhat hard to believe there are muscles and tendons there. Especially true now that shes a senior catizen.)"}
{"doc_id": 10004176, "author": "Dude", "text": "Is your cat sterilized? Non-sterilized cats are usually rather fit and even skinny because mating calls take a lot of energy. In addition I guess she runs a lot outside that helps her to keep fit. Also I suppose that she is still growing and all her energy goes into size not width, which would be hilarious. My third though: was the medicine for worming effective? As she goes outside she must be wormed every 4 months. Maybe its worth trying another medicine?"}
{"doc_id": 10004185, "author": "Amy", "text": "We have a long, tall and skinny tux-tabby boy who is also commented on for his slender appearance. Hes 12lbs, slender, and perfect. Might be the breed? Some are slinky! I was always told that feeling ribs is fine as long as its when you press gently, during a stroke for example. Neither of mine go outside due to personal choice given the traffic and diseases around my neighborhood. I could not cope with anything happening to them because I decided to let them go explore our crappy neighbourhood. I second peoples comments on the wet food, though, and would add that ingredients are everything, especially where cats are concerned. My 2 boys have been on good quality food their whole lives and at 3 years old, still play like crazed kittens. Keep dry food to a minimum, think fingerpinch not handfull as well. There is NO such thing as a dry mouse and cats are carnivores, they get little to no nutrition from plants and almost zilch from the ground up grains that brands like Hills(?) and Friskies love putting as a main ingredient in their food. Ground Yellow Corn... for example... nothing but industrial waste used to bulk up their slaughterhouse waste, something I would not feed to a pig but people put their hard-earned cash into these products that are worth less than the tin they come in! A lot of the dry food you see out there also has too much filler, and not enough rich protein sources, plus the manufacturing process is said to be appalling. People add water or wet food, allowing stagnant bacteria to flourish. Drinking cannot make up for this dry food diet -- its not going to help a species known for low-thirst drive and its not going to prevent those kidneys from working overtime to undo the damage dry food does. Perhaps if you keep this cat indoors more, you may notice some weight gain? There are plenty of ways to give a cat exercise indoors, a cat tree for example. Last note, try lightly cooking (or fully cooking) some chicken liver, hearts or even breast for a treat. I leave it a little pink in the middle for my lil guys and they love it! Also, seems basic but I had trouble adjusting as my boys both grew (both are now 12lbs) but make sure that you are feeding appropriate to the weight of your cat, that means nevermind the cans, look at the calories. My boys are on about 300 a day on average if that helps. p.s. High grade dry food is a misnomer. Its actually becoming known as Kidney disease in a bag. The worst wet food might just be better than the best dry food, with a few exceptions to that rule cough science-diet-hells-purinacough."}
{"doc_id": 10005814, "author": "Chris", "text": "I have an 8 yr old long-haired tabby who is very long and skinny. He eats, but is never really very interested in food. Hell always take a cuddle over grub if he has the choice. Hes indoor at night and in and out using a cat door during the day. Occasionally hunts and brings me a bird, but never eats one that Ive seen. Ive tried every kind and combo of food out there. He has only gained 40 grams since I adopted him 2 months ago. The vet did complete labs and a very thorough exam. She says his blood work and teeth are that of a much younger cat and hes very healthy. I guess hes just a naturally skinny guy. If youre concerned, a blood and urine test to show liver and kidney function could reassure you. Although theyre expensive, it Helped me quit stressing and buying umpteen types of cat food that he didnt really want anyway."}
{"doc_id": 10001925, "author": "toxotes", "text": "No. Definitely not at the moment, but possibly later on down the road. I assume this is the same tank you ask about here and here. This is a new tank that does not have an established biofilter yet: this means the more fish you put in now, the more unhealthy the water will get. Until you know for a fact that your biofilter is working -- your ammonia and nitrite levels stay at 0ppm and do not fluctuate -- there is no point in adding any more fish. If you do, the most likely result is that theyll die, or suffer permanent damage to their gills that greatly shortens their life. But can you add tetras once the biofilter is working? The answer to that is maybe. There are dozens of species of tetra out there. I dont know whats available to you or what kind of fish youre looking for, so Im not going to make a specific recommendation. But basically, you need to ask two questions to decide whether you can keep fish together: how will they interact with each other? And, can I provide environmental conditions that are healthy for both species? In terms of behavioral compatibility, tiger barbs are active, somewhat aggressive fish. Ive even heard some people say they dont believe theyre suited for a community tank at all. As Seriously Fish notes: P. tetrazona is notorious as an aggressive community inhabitant with a reputation for nipping the fins of tankmates though this behaviour only tends to be pronounced when insufficient numbers are purchased or space is limited... That said it is relatively boisterous and doesn’t make an ideal companion for very shy, slow-moving, or long-finned fishes such as many livebearers, cichlids, and anabantoids. Tetras are often relatively shy, so in your research youll want to make sure a species can handle being pushed around a bit. The flip side is that there are very few tetra species that will cause problems for the barbs. And as the SF article mentions, youd want an active, short-finned species. In terms of environmental compatibility, it looks like tiger barbs and most tetras prefer slightly acidic, soft water, so thats a plus. The SF article mentions the barbs come from clearwater streams and rivers with a sandy bottom and thick vegetation, with temperatures in the mid-20s (°C)/mid-70s (°F). I would rule out the tetras native to blackwater conditions, where the water is still and warm, dark with tannins, with little live vegetation, but there are still plenty of species to choose from. So thats what would help you pick out a tetra thats compatible in terms of water conditions and behavior. But theres another aspect to environmental compatibility: once you know what the fish need, can you provide it? The biggest limiting factor here is your tank size. Tiger barbs and most tetras are highly social and are best kept in large groups, which means you would need a good amount of tank space and filtration capacity. I dont think Ive seen you mention anywhere how large your tank is. The four barbs you have will get pretty large -- 3 (7.5 cm) or so. If you have a small tank, like a 10 gallon (38 liters), you probably dont have room for any more fish. Finally, consider alternatives as well. The barbs will spend most of their time in the middle of the water column. You can avoid a lot of aggression problems by choosing tankmates that stay towards the bottom -- I think loaches are common tankmate, for example."}
{"doc_id": 10003050, "author": "keshlam", "text": "Its not a responsible choice from either a pet-owner or ecological point of view. Whether any particular cat survives going feral, and for how long, is a crapshoot. Youd certainly be dumping them into a life of illness, hunger, competition with other cats and the injuries resulting from that, predation by coyotes and hawks and such, and abuse by humans who have no sympathy for feral cats. Life expectancy is nowhere near that of a pet. Its a life, but not a very good one. That is not an improvement over a shelter. Ecologically: Feral cats are already a serious threat to some native species. There are ongoing efforts to humanely reduce their population in many areas; youd still be dumping your responsibilities on someone else. If you insist on doing this, at least neuter the animals first so you arent dumping multiple generations into the street and actively making the problem worse. Yes, no-kill shelters are overloaded and underfunded. But if youre willing to make a donation to support them, they might be able to make a place for your kids. And frankly, Im not convinced even a shelter which does euthanize is crueler than abandoning them; shelters generally treat the animals well and if they must be put down its done painlessly. So the answer to the question is Maybe the cat will survive, for a while, if it happens to survive the transition and continues to be lucky. Its still significantly worse odds than a shelter, never mind finding them a real home. Its a bad thing to do to an animal you claim to care about. There are better alternatives."}
{"doc_id": 10003051, "author": "Zaralynda", "text": "Kittens in the Shelter The ASPCA states Of the cats entering shelters, approximately 37% are adopted, 41% are euthanized, and less than 5% of cats who came in as strays are returned to their owners. However, I couldnt find statistics about the fate of kittens (versus adult cats). Generally, kittens are considered more adoptable. Adult cats who are found in shelters sometimes have behavioral or health problems that makes them more difficult to adopt. I expect that kittens would be euthanized at a lower rate than the general cat population. Feral Kittens For kittens who were raised by a feral mother, a study available on the AMVA website reports: Survival data were available for 169 kittens. Overall, 127 of the 169 (75%) kittens died (n = 87) or disappeared (40) before 6 months of age. ...Eighty-one of the 169 (48%) kittens died or disappeared before they were 100 days old. Causes of death were determined for 41 of the 87 (47%) kittens reported to have died. Thirty-seven of the 41 (90%) died as a result of trauma, with attacks by stray and owned dogs (n = 18) and motor vehicle accident (10) being the most common types of trauma. Other types of trauma that resulted in > 1 death included falls from haylofts (n = 2), being stepped on by horses or people (3), and a suspected episode of infanticide (3). Cause of death was not determined for 46 of the 87 (53%) kittens reported to have died, but many reportedly had signs of disease, including upper respiratory tract disease and diarrhea, prior to death. The kittens who disappeared could have moved on to different territory, but its also likely that they were eaten by predatory animals (so their corpses could not be recovered) and/or they were ill and found a quiet place to hide while they died (which made their corpses difficult to find). Tame Kittens Released I couldnt find any data on the fate of tame kittens who are released (often called dumping). These kittens may fare better than feral kittens (because they have several weeks of good food/care before having to forage for themselves) but may also lack important skills (such as where to look for food and how to hunt). They are likely subject to the same sources of trauma as the feral kittens in the study, but may have different rates of disease. Summary We know that kittens placed at a shelter will be protected from other animals and from car strikes. We know that kittens who become ill at a shelter will be given veterinary treatment. We know that kittens who are placed at a shelter will be given enough food. We dont know any of these things about kittens who are released, and have good evidence to believe that many of these things do happen to released kittens. Its hard to draw conclusions about which situation is more deadly, but its clear which situation is more humane. Even when a shelter cat is put to sleep, its done without pain or suffering."}
{"doc_id": 10003437, "author": "SimonT", "text": "NO! They are domesticated animals who we have made dependent upon us for our pleasure - they are not things you can throw away when you dont want them. She would be guilty of criminal abuse to animals. The way to limit kittens is to get the males neutered and the females spayed. If you let your female cat get pregnant, you are TOTALLY responsible for the lives of the kittens she delivers. Once a cat is old enough, YOU MUST have them neutered or spayed (though it is technically correct to speak of neutering both males and females even though the surgery is totally different). It is irresponsible to allow a cat to breed unless you can absolutely, positively be certain that there will be good homes for the kittens. Female cats do not have to have a litter to feel fulfilled. And a male does not need to impregnate a female to be a true cat. It is nonsense. In a pinch, a pregnant female can be spayed and the embryos aborted. Sounds bad but it is far better than the kittens being dumped and meeting one of far too many forms of danger and horrible death. If I caught someone dumping a cat or kitten - well, it is not appropriate to describe what I would do to them here. I would, of course, make sure the cat or kittens are safe before expressing my displeasure in a clear and simple way. I wont even try to address some comments in other answers!!! Cats are not disposable. They are loving and lovable creatures which we humans have domesticated, making us responsible for their care and safety. As to the crack about tame kittens doing better than feral because they would have had good meals - you dont want to hear my real reaction. It is pure nonsense. Actually a feral kitten (a true feral kitten, not a stray) will do immeasurably better than a tamed dumped kitten. Feral kittens were born and raised with no contact with humans but they will have their mother to provide them with appropriate care, love, milk and then solid food. THERE IS NOTHING THAT JUSTIFIES DUMPING A CAT, REGARDLESS OF AGE! NOTHING! If they were near you Id take them in them in a heartbeat. Dumping an unwanted cat or kitten - or any domesticated animal - should be a severely punished crime. I dont think that a sentence of years in prison would be too much punishment It is ludicrous to dump an animal and expect them to survive and live a good life!"}
{"doc_id": 10003701, "author": "Jennifer Winkler", "text": "The AMVA study cited above (https://www.avma.org/News/Journals/Collections/Documents/javma_225_9_1399.pdf) was conducted on colonies of free-roaming cats that had human caretakers. These humans presumably fed them. Among real feral cats* (having no food supplied by a human) there is additionally death through starvation. Obviously, people with a cat they cant keep should make great efforts to place the cat with new people. It is common for people to think that cats (however much they have been pets) can survive in the wild, but the information we have come across shows that the odds of survival of an individual house-raised cat put in the wild are small. I live in the country, where unwanted cats are sometimes abandoned. I resent the off-loading of now fearful cats, that have no food (people rarely help them, and prey is insufficient) to live on, can become pests probing garbage containers, and are subject to predation by larger animals. Almost all of them suffer a lot. Yes, this is the way of the wild, but we have expected better for pet species. (For free-ranging cats, estimates of the reproductive capacity of female cats and the consequences of unabated reproduction are often extrapolated beyond scientific reliability, as they typically fail to use realis- tic litter sizes or ignore kitten mortality rates, states the AMVA paper. We have developed a great fear of sexual intactness of cats, but that level of fear is not called for in situations of very high mortality of kittens.) Communicating about feral cats is made more difficult when different people mean different things by feral."}
{"doc_id": 10002494, "author": "James Jenkins", "text": "No, Pine and Cedar are generally considered toxic to Rats (and many small mammals) pine should not be used as bedding by extension they should not be offered as chew toys. There are several home remedies (most dont work) for keeping needles on trees, and you have no idea what methods may have been used on the trees that have been disposed of. In fact many woods can be harmful for a rat to chew on, so making healthy choices on behalf of your rat is important. (still looking for a really good reliable reference, on best rat wood chews)"}
{"doc_id": 10003173, "author": "John Cavan", "text": "We had a deaf cat, sadly passed away only about a month ago, and to be honest we didnt handle her any differently than our other cat. The things to bear in mind are: Theyre easier to surprise and every cat might react a little differently if startled, particularly from sleep. Ours just kind of jumped up, but others might be a bit more aggressive. They wont react to shouts or loud noises. We used to clap our hands to get the attention of our cats when they were doing things like clawing furniture. That would usually get them to stop, but that was totally useless with her once she went deaf. At that point wed just pick her up and remove her from the area, which seemed to work. Deaf cats still like attention, grooming, and petting. I found that she became even more of a lap cat once she became deaf, possibly because of getting touch feedback from us. In any event, cats are pretty adaptable and deafness will probably be a minor impediment to them and you. I wouldnt be worried about it, barely something we noticed."}
{"doc_id": 10003189, "author": "anongoodnurse", "text": "We had a cat that was congenitally deaf. She was an indoor cat only. The major difference was that to call her, we would stomp on the floor. Other than that, she was a really active, curious, playful, normal kitty. No special attention needed."}
{"doc_id": 10003197, "author": "Vercy", "text": "As everyone points out, no difference really. To call attention you will have to change sound waves to another type (both clapping and stomping) and cause air pressure waves to which cats have relative sensitivity (stomps in my opinion are better). One precaution though, do not let the cat to go outside unattended, as he is far more likely to fall prey to accidents of all sort. Another case is panic; if something startles your cat really badly, he will panic with more lasting effect as hearing cannot tell him if he is being chased etc. so a little sensitivity on the subject is required As John Cavan wrote, deaf cat will be far more likely to be couch cat, as he will be loving sensory input from petting to replace missing sensory input from hearing."}
{"doc_id": 10005619, "author": "Christy B.", "text": "So, this question depends on the severity of the Myiasis and how much deep tissue has been damaged, so I will give you a broad answer, that may help you and others in the future. Usually a bacterial infection spreads quicker than the rapid spread of the larvae, and bacterial infection can definitely kill an animal. If your vet has said he/she thinks treatment is the best option for your pup (versus being in too much pain, too much suffering, or too far gone thus suggesting euthanasia) then I would try my best to keep the faith in his educated opinion. Since I dont have pictures, or am able to see a blood panel test, or blood/tissue culture to see what type of infection or bacteria is growing and how the body is reacting- I would suggest IF you are concerned enough you could always get a second opinion from another vet. Subcutaneous infections can get into the bloodstream, spread to the spine, brain, other organs etc. I have seen a dog go blind from Myiasis from the location of where the infestation and infection took place. I would take this seriously either way, no matter the severity, because even if death (as you asked) isnt the outcome, there could be other outcomes that affect your pup forever. I would try your vets treatment plan. You did the right thing by taking him to the vet after topical solutions didnt work."}
{"doc_id": 10014414, "author": "Hobbes", "text": "Add sugar. A spoonful of sugar will bring a lot of CO2 out of solution at once. The beverage will fizz furiously, so dont fill the container to the brim or youll spill some. Adding surface area will help too. Ive done this by inserting a teaspoon, but thats too slow. I havent tried this, but maybe using a tea infuser (an egg-shaped strainer made to contain tea leaves) is better (more surface area): stir your drink with the (empty) tea infuser."}
{"doc_id": 10014415, "author": "Tetsujin", "text": "Not certain about the practicality of this solution, but as a child I discovered that putting the chewed end of a stick of liquorice [the real stuff, made of wood] into a fizzy drink would flatten it in seconds. The reality is likely not the liquorice itself, but the very large surface area it would present. As an adult Ive never actually tried to reproduce this, but perhaps something like a paint brush [clean of course] would reproduce that large surface area."}
{"doc_id": 10014416, "author": "w25r", "text": "I despise carbonated beverages so I often flatten my drinks. In order to meet all your acceptance criteria (namely, portable and works for cans) the solution I suggest is simple. Carry around a plastic sports bottle... must be strong enough to handle pressure but have an easy to operate lid. Transfer contents of carbonated beverage into bottle Slightly shake, to build up pressure. Open lid to release pressure. Repeat step 2, shaking a bit more each time, until all carbonation (or as much as needed) is gone. This process usually only takes a few minutes. With beverages in plastic bottles you can just use the bottle itself. With glass bottles (and clean hands) you can form a seal with your finger or thumb and do the same process. But for cans the easiest way would be a separate bottle."}
{"doc_id": 10016555, "author": "Feran", "text": "As children, we used to put a strip of clean folded tissue/paper towel in the glass. You might have to repeat a few times."}
{"doc_id": 10016562, "author": "Stan", "text": "Nothing needs to be added to soda to remove the fizz. The gas can be removed mechanically. Loosen the top of the container until you hear the characteristic woosh of the pressure being released from the bottle. Drive the carbon dioxide from a soft drink by striking the side of the [plastic] container with a heavy spoon/butterknife handle. The sudden jolt will immediately drive the gas from the liquid. Start with a light tap so the soft drink will remain in the container without overflowing. Repeat, gradually increasing the force of the taps. After a few knocks, the drink will be nearly flat. This will work better if the soda/pop is at room temperature. A warm liquid cannot hold as much gas than if its cold. Chill the flat liquid for consumption if desired. Try it. It works."}
{"doc_id": 10016747, "author": "MN H2O", "text": "PV = nRT So - Use a vacuum pump. ultrasonic bath will do it, outside of this equation Other suggestions touch on the portable but not automated methods. Last one: Maybe take a big syringe thats attached to a flat rubber cover for drinks. Cover your glass and pull back on the syringe several times. There are several YouTubes on how to do this."}
{"doc_id": 10010363, "author": "Pobrecita", "text": "You should lubricate the hinge and doorknob mechanism with oil. GT-85 or other teflon-containing lubricants are the best. If youre not in a position to do that: WikiHow: Take light, slow steps as you are opening the door. That way, no one will hear the squeak of the floorboards and/or tile as you enter the room. Be careful when turning the doorknob because some doorknobs can be very noisy as you turn and let go. If it is noisy, turn very slowly and let go slowly, only releasing when it is back in its original position. Turn the doorknob so that the least rotation is needed. If the hinges are on the right, rotate the doorknob clockwise. If the hinges are on the left, rotate the doorknob counterclockwise. This holds true whether the door opens toward you or away from you. After you turn the doorknob, press the door upward and then toward the hinges in rapid succession as you open the door. This will help to reduce the noise the door makes as it separates from the jamb. Jamming the door upward and toward the hinges immobilizes it and makes the chance of squeaking unlikely. Additional Info Some people say spit on the hinges. Doors are sticky and noisy when opened? For obstructions near the door. sand down or strip the offending parts until they clear each other without touching (and possibly repaint with a single coat if you need to). Id expect sanding only to rough it up to increase the friction, not make it better. Hone Guides"}
{"doc_id": 10010366, "author": "J. Musser", "text": "The squeaking noise is often made by friction as the hinges rub. If thats the case, I will turn the doorknob and pull straight up, basically lifting the door so that it presses against the top of the hinges, which are less likely to squeak."}
{"doc_id": 10014614, "author": "kim", "text": "Often, lifting up the door and pushing toward the hinges keeps the hinges from squeaking."}
{"doc_id": 10014617, "author": "bigbadmouse", "text": "Use a lipsalve aka chapstick on the hinges, push it into the gaps between the two hinge leaves with your fingers. It also works on swinging shower screens. Oil runs out, lipsalve just stays put."}
{"doc_id": 10027517, "author": "Bulrush", "text": "Yes there are different plugs for different engines. Usually you need the engine model on your mower, and take in the old spark plug to the hardware store. Ask for someone to help you find the right spark plug and tell them your engine model and mower model. Usually near the spark plugs at the store there is a booklet showing your old model of spark plug and what model the brand in front of you is compatible with. If not, someone behind the counter might have that book. They get used and abused a lot."}
{"doc_id": 10026741, "author": "Bamboo", "text": "Im not sure who you mean when you say they had a fire all winter long... wood ashes do contain small amounts of potash, but the actual potash content varies hugely on what type of wood/plant material is burnt to produce it. It also does not keep - if it becomes damp, any potash content leaches out quickly. The term pot-ash originally applied to the substance left behind after soaking ashes from burnt plants in a pot with water - after evaporation, any white coating round the pot was potash and used as such. The sort of potash most commonly used by gardeners is Sulphate of Potash, which is created chemically, and its used primarily around fruiting plants. Mined potash has chloride in it, and this is used much more in agriculture, often for cereal crops. More info here http://acceleratingscience.com/mining/potash-a-look-at-the-worlds-most-popular-fertilizer/"}
{"doc_id": 10030574, "author": "Bamboo", "text": "Blossom end rot is a physiological disorder primarily caused by irregular or insufficient water supply, which disables calcium uptake. The fruits, or the bits that arent affected, are not dangerous to eat, so if the part thats left after you cut off the brown bits is tasty, then yes, you can eat them. You might want to reconsider if the brown parts are oozing, have fungal growth, or smell nasty."}
{"doc_id": 10030811, "author": "Brōtsyorfuzthrāx", "text": "Since you havent accepted Bamboos answer, yet, Ill add my own, in case it offers something new. This answer is just based on my experience (not on studies, books, etc.) So, if you apply this information, do so at your own risk. Ive eaten plenty of BER tomatoes, and Ive never gotten sick from them. I just cut off the black/brown/tan part (I dont eat that, since it looks kind of scary, although it may or may not be edible), and the rest of the tomato is fine. Actually, they taste exceptional, in my experience. The flavor and brix seem to be concentrated for some reason. They also ripen faster than regular tomatoes, although the seeds can seem to be less mature. I live in a semi-arid climate. Im not sure if there would be more dangers of eating them in a more humid climate where different kinds of pathogens might flourish, but Ive never heard of anyone getting sick from eating a BER tomato. Secondary infections with disease are probably the concern there (not the BER itself). If you know what those secondary infections are, youll be safer for the knowledge. If its just anthracnose, you should be fine (Ive eaten lots of tomatoes with mild anthracnose lesions and havent had noticeable issues, although they do seem to have less flavor; same for a couple other pathogens sans the seeming flavor loss). Noticeable is the keyword, though. You never know what you dont notice. If youre really concerned, just cook the tomatoes. It should kill the pathogens and neutralize toxins produced by at least some kinds of pathogens. There might be toxins produced by some bacteria/fungi that might survive that, but I dont know of any in particular that would. Anyway, as long as the good part of the tomato isnt rotten, too, Id think it would be okay. So, if your definition of edible is that they taste good, then yes, theyre very edible. If your definition is that they wont kill you, Im pretty sure they wont. If youre worried about subtle toxic effects, or subtle pathogenic infections that might cause issues down the road or for people susceptible to illness, I really cant say, but Ive never had any noticeable problems, personally. I do notice some subtle stuff like that in other areas of life, and with some food situations, though (a lot more than your average person would claim to notice), but I havent noticed anything bad here, personally."}
{"doc_id": 10038279, "author": "John", "text": "I always have trouble with BER, despite regular watering. Ill try lime supplementation this year. Ive noticed that cherry, grape, and other small varieties seem to be immune. Its just my bigger tomatoes that are affected. Ive never tried to grow beefsteaks."}