{"doc_id": 147577, "author": "", "text": "It seems plausible. According to the Wikia entry on Trigedasleng: Trigedasleng is desended from a heavily-accented dialect of American English Trigedasleng developed partially due to natural linguistic drift, but also because there was a pressure for the Grounders to develop code-terms and euphemisms that their enemies, particularly the Mountain Men of Mount Weather, could not readily understand Trigedasleng underwent extreme phonological simplification during its descent from English, resulting in numerous homonyms. Combined with the relative isolation, it's quite plausible that the language would have changed as it did. Imagine that you started with a group of people who speak African Vernacular (Ebonics) who develop additional slang terms to hide meaning from others and also start to blur together the phonetics of similar words such that they knew from context both others from outside might not. Isolate them for a hundred years and it's very plausible that, after that time, it's very likely we'd have some trouble. Just look at how unintelligible some of the slang of the 50s is to people of today. David Peterson, the linguist who created the language, speaks more at length about it in his blog."}
{"doc_id": 147578, "author": "", "text": "There may be a real-world example on Quora from Don Grushkin, Professor of Deaf Studies (Ph.D. in Language, Reading and Culture). I've added the bold. I'm not sure anybody's ever conducted any research on rate of linguistic drift. As Joachim Pense notes, languages can be created in one or two generations. But you're talking about linguistic change from one form, say, \"Old English\" to a newer form, say \"New English\". One example that we can look at to see linguistic drift occurring is in the case of American Sign Language. ASL was created in 1817, with the establishment of the American School for the Deaf, where French Sign Language was imported and likely blended with Martha's Vineyard Sign Language, and possibly some American Indian Signs, or some local Deaf community signs (there were a few Deaf communities emerging in the early NorthEastern United States). In 1880, the use of signed language was officially banned within Deaf educational settings, and the language started to go underground, to some degree. In 1913, the National Association of the Deaf, fearing the death of signed language, commissioned a series of films (around 6 or 7 films) in order to preserve the history and the language. Most of these films survived today. You can watch one of these films here: To modern signers of ASL, it is difficult to understand the signs (I can, but only because I've seen a translation and have watched the film many times). Our difficulty shows that the language has undergone a significant drift in about 100 years, or about 5 generations. This drift might have been accelerated by other factors such as invented signed systems, the banning of ASL in education (causing some areas to invent their own sign language or variation of ASL), and the lack of a written form of ASL. Given a small, isolated community after a catastrophe, I think it is quite plausible to find a new, unintelligible language evolving in that short a time."}
{"doc_id": 144938, "author": "", "text": "OK, the fact of the matter is that everybody learns their own languages, in their own ways, in their own times, places, and circumstances. It is normal for kids to have several languages at home, and to pick up others as needed, by playing with other kids. Those languages either flourish through use, or wither and get forgotten by disuse, like any human skill. Plus, people vary not only in their unique language experience, but in their skill at apprehending and using it. Also like any human skill. That's a vast amount of individual variation. By contrast, labels like Native language, First language, Mother tongue L1 L2 .. etc. are invented by people who need abbreviations for commonly-referenced groups of characteristics, usually characteristics that are common only in monolingual places like the USA, where almost everybody speaks only English, and often finds multilingualism threatening. They are not terms defined in the Qur'an or the APA Style Manual; they are just abbreviations, which may be useful in certain contexts among certain kinds of professional. That's all, really. These terms, and others, may or may not be applicable to the situations you mention. Or to others one can easily imagine. I repeat, they're just nonce forms, with localized definition and localized utility. They are Not Ready For Prime Time, in other words, so you shouldn't take them too seriously. And they certainly don't cover every possibility."}
{"doc_id": 144939, "author": "", "text": "I wrote a blog post about this very topic last week, on the International Mother Language Day. http://multilingualparenting.com/2014/02/21/mother-tongue/ There is unfortunately no clear-cut answer if you speak more than one language. The different terms are used in different contexts and for varying purposes. For me 'mother tongue' and 'native language' are more or less interchangeable. The term 'first language' is as far as I understand not the chronologically first language, but the one a speaker is fluent in and feels most comfortable to speak. This means your 'first language' can change depending on where you live and which language you speak the most. Your first question is intriguing and I also wrote about the scenario in my blog. If you no longer speak the language(s) you learnt as a child, based on the monolingual research terminology, you would be \"mother-tongue-less\" - however you would have a L1. If you have become fluent in a language later in life, you have a 'native level fluency'. It's difficult to draw the line with regards to accents - different \"native\" speakers have vastly varying accents as well, so why would an accent from an other area prevent you from being called fluent? If you have learnt a language as a child and you are fluent in it, it is one of your 'native languages' - the notion that there could only be one 'mother tongue' or 'native language' comes from a monolingual perspective and doesn't apply to bilingual people."}
{"doc_id": 148368, "author": "", "text": "This is a very good question because it highlights the multiple terms used to describe what appears to be similar if not the same phenomena; However, as it has been pointed out above, there are contextual differences in the terms. As far as a clarification for the terms: First language and L1 are the same. L1 is the abbreviated form of first language. And mother tongue and Native language are interchangeable. Essentially, these two terms are socio-cultural constructs. Meaning, the terms native and/or mother language are a way to conjure a transportation of a language from one culture and geography into another geography/culture. Their use trigger the counterpart, foreign. Thus, in addition to declaring the order of language acquisition, these two terms also reveal an immigration component to the language. In contrast, the term L1/First language are clinical terms to describe language acquisition in individuals who have acquired one or more languages. As far as the scenarios listed in the original post: I would describe this scenario as a non dominant L1 individual, i.e. underscoring language acquisition and subsequent language shift. L2 adquisition post Critical period. This scenario needs to be further qualified because the critical period may include for some a wide time frame. For example, I would call an individual acquiring a second language up to age 4 or 5 a sequential L2, and/or maybe depending on the input exposure opportunity in both languages, a simultaneous bilingual, known as 2L1."}
{"doc_id": 148465, "author": "", "text": "Just take myself for an example. It might be subjective though. I'm a Cantonese living in Canton(or Guangzhou), China. Cantonese is my mother tongue and first language. It's a local language in Guangzhou and Hong Kong SAR. Mandarin is my native language. It's the official language of China. So almost all kindergartens and primary schools in mainland China will teach Mandarin as compulsory. Accents and dialects appear in all different parts of the nation but Mandarin is the only one that used nationwide freely. English is my 2nd language then."}
{"doc_id": 152301, "author": "", "text": "Historically, the two used to be the same. In other words, the English \"TO-infinitive\" started out as the preposition \"to\" plus a verbal noun; compare the Latin infinitive, which is derived from a noun in the locative case. Nowadays, though, the \"to\" used to form the \"TO-infinitive\" does not act like other prepositions. You can't say \"I wanted to come to see you, and now I am *at see you, and later I will go home *from see you\". In fact, there's no other English word with the same syntactic properties: it's in a class of its own. So, traditionally, it's called a \"particle\", which is the syntax term for \"word that doesn't fit into any larger class\" and/or \"word we don't know what to do with\". You might also hear it called a \"subordinator\" or \"complementizer\" or \"marker\", depending on your particular theory. But whatever it is, it's not a preposition."}
{"doc_id": 147088, "author": "", "text": "Phonemic and phonetic transcriptions are created with different purposes in mind. Phonemic transcriptions are often kind of a proto-orthography for spoken languages, and they are used to describe languages (in example sentences or grammars) that lack a proper orthography. Phonetic transcriptions are often used by dialectologists to describe how the sounds of dialects differ in space and time. Sub-phonemic sound shifts are a typical research topic in this area."}
{"doc_id": 147091, "author": "", "text": "You characterisation is basically right, but could be emended a bit, to reflect different ideologies of phonology (since there isn't just one). The most important is whether or not there are \"phones\" (or whether phones have a radically different status). A \"phone\" might be a real mental unit (not physically tangible), or it might be a conventionalised representation of an part of an acoustic waveform. Substance-free theories especially favor the latter interpretation, and SPE-era generativists favor the former (though neither particularly favors the use of the term \"phone\"). SPE theoreticians do hold that \"phones\" are comparable across languages and you can say that this \"m\" and that one \"are \"the same\", but the substance-free response to that claim is the same as a phonetician's response would be, namely producing a graph of the differences between the waveforms. I don't think the \"change the meaning of words\" characterization of phoneme is right, in that it misses the essential property of phonemes and focuses on a possible consequence. Phoneme are whatever underlying sounds exist in a language, which can be the building blocks of lexical and grammatical formatives. Other sounds come into existence by application of rules. That is it. The question arises as to how we know what those underlying sounds are, and a number of operational tests have been called on: one of them is asking whether there are any two words that differ in just the choice of a particular pair of sounds (i.e. a \"minimal pair\"). Typically, people conceptualize \"two words\" as being about having different meanings. So, the \"change meaning\" characterization is sufficient for phoneme identification, but it isn't necessary. I would not agree with your second interpretation. Actual sound, which is studied in phonetics, is both produced and perceived in real life. Perception involves hearing, and perception is broader than the concept of \"phoneme\" (e.g. also applies to cognitive states in humans that arise from being exposed to non-linguistic and non-human sound). Interpretation likewise is something that we do to anything that we have perceived. In ordinary language, we would talk of how speakers \"interpret\" a given acoustic waveform as referring to \"do\" vs. \"two\". What you are saying in point 2 most closely resembles the distinction between phonetics and phonology, but that's more at the level of introductory linguistics where we teach sound bites that are memorable but not really true. Phonologists and phoneticians generally (though not universally) recognize a distinction between the discrete, symbolic vs. continuous physical aspects of speech: formant measurments are an example of the latter, and the vowel distinction [i] vs. [ɪ] is an example of the former. Any transcription is, by necessity, a discrete symbolic representation. Up to a point, a transcription can give you more details about the physical sound, but there is a huge limit on the precision possible with a transcription, which numeric measurements go far beyond. Use of square vs. slash brackets is also problematic. I will disregard the problem that many people just don't care what brackets are used and use them indiscriminately. Square brackets represent something that is closer to the physical output, and slash brackets are used to represent something that is further from the physical output. The standard generative convention is that slash brackets go around underlying forms and square brackets go around surface forms. Underlying is not the same as phonemic (note that generative phonology a la Halle rejects the concept of \"phoneme\" as a distinct representational level). This does lead to a quandry in representing intermediate forms, for example /apa/ → aba → [ab], where aba is not the underlying form and not the surface form. You may have notices that in American English there is a difference in the pronunciation of /t/ in \"militaristic\" vs. in \"capitalistic\", where it is aspirated in \"militaristic\" and flapped in \"capitalistic\". The governing factor is stress and syllable position, and reduces to the fact that \"militaristic\" is from \"military\" vs. \"capitalistic\" being from \"capital\" -- the stress patterns of those words differ. So the distinction is derived by applying the relevant rules to get ˈmɪlɪˌtɛri+ɪstɪk whence ˈmɪlɪˌtʰɛri+ɪstɪk, vs. ˈkæpɪtl̩+ɪstɪk whence ˈkʰæpɪtl̩+ɪstɪk. But -istic also causes a stress shift, and thus you get a surface contrast in aspiration vs. flapping. The phonetic outputs are [ˌkʰæpɪtl̩ˈɪstɪk] and [ˌmɪlɪtʰɛˈrɪstɪk]. The intermediate form contains a non-phoneme so shouldn't be in slash brackets, but it isn't an actually pronounced form, so shouldn't be in square brackets either. If \"phones\" or \"allophones\" i.e. actual physical outputs are the things in square brackets, then rules of grammar cannot refer to them, because rules of grammar are mental operations on mental objects. A reasonable amount of experimental evidence is emerging to show that the notion of \"allophone\" or \"phone\" is suspect, in that it conflates two different things. One is that languages can have distinct sets of sounds which are not yet exploited to make lexical distinctions (so that one sound is a rule-derived variant of the other). Vowel nasalization in Sundanese is a prime example. The other is that languages can have physical variations of sounds which are gradient in degree and timing, which resemble categorial distinctions in sounds in other languages -- patterns of nasal airflow in English are a prime example. Nasal airflow patterns in English really require physiological investigation because you can't hear the point at which air flows through the nose, or when it reaches its peak. You can, however, hear in Sundanese whether a vowel is nasal or oral, just as you can hear that distinction in French (the difference being that in French, vowel nasalization is phonemic, but it is not in Sundanese)."}
{"doc_id": 147092, "author": "", "text": "Different linguists have different ideas about phonetic versus phonemic. Mine is one that I think is close to the original conception of Baudouin de Courtenay and his student Kruszewski (to whom we owe the term phoneme). My version is a little dumbed down. Phonemes are what we hear. Phonetics is what we say. At least, naive native speakers hear phonemes, not phonetics. After a course in articulatory phonetics, speakers start to hear some phonetic detail. Recognition of dialects requires some ability to hear phonetics, and that doesn't depend on schooling. This is why traditional alphabetic writing systems are mostly phonemic, rather than phonetic, why poetic rhyme is phonemic, and why secret languages like pig latin are based on phonemics."}
{"doc_id": 151289, "author": "", "text": "Their transcriptions (p. 58) probably do not represent any actual dialect. For example, [ˈælfa] probably isn't an actual pronunciation in North American English. [ˈtʃɑ:li] indicates a non-rhotic dialect (thus excluding General American); the contrast [noˈvembə] vs. [ho:ˈtel] suggests that they are randomly using phonetic notations since there's no difference in the initial vowels. Noting that ICAO is located in Montreal and they transcribe Quebec as [keˈbek], you may suspect an Eastern Canadian substratum. I can't say that I've ever encountered a speaker of GAE who pronounces the words the same. Ogden reports that they are homophones for some speakers of Melbourne English, and there is Wiki chat that suggests that it's an Australian thing. This (unnamed) person claims that they are homophones, so it's a credible claim. According to Cambridge, golf is pronounced differently from gulf. If you think the words are more like [gɔlf] rather than [gʌlf], I would conjecture that you're from the Midwest (I'm from the West). There is a lowering going on in the Midwest where my [tʊr] \"tour\" is pronounced [tɔr]."}
{"doc_id": 151290, "author": "", "text": "It’s quite common to distinguish the pronunciations of \"golf\" and \"gulf\". Historically, they had the different vowels that the spelling would suggest: a “short o” (\"lot\") sound in golf and a “short u” (\"strut\") sound in gulf. In American English, the “short o” sound has typically been rounded to the “cloth” vowel when it comes before /l/ followed by another consonant (or followed by the end of a word). According to Wikipedia, a general merger of /ʌl/ and /ɔːl/ (“strut” + l and “cloth~thought” + l, as in \"hull\" and \"hall\") is known to occur in some accents of American English, but it is not a mandatory feature of “standard” pronunciation and I don’t think it’s very common. I've heard of speakers having the merged vowel be /ɔ/, in which case this could be seen as a kind of rounding change, maybe similar to the one that produced /ɔl/ from what would have otherwise been /ɑl/. For me, in the specific context of a following voiceless obstruent (which causes the preceding syllable nucleus to become shortened or \"clipped\"), \"strut\" + l does sound somewhat confusable with \"lot~cloth~thought\" + l, but not to the extent that I would say that I have a merger. For context, I have the lot-cloth merger, so phonemically I have /ɑlf/ in \"golf\" and /ʌlf/ in \"gulf\". It seems plausible that some speakers with accents similar to mine might have something like \"Canadian raising\" here that turns former /ɑlf/ into something that sounds more like /ʌlf/, just as /aɪf/ in words like \"wife\" is turned into [ʌɪf]. There was a post on ELU relatively recently by a speaker who perceived /ʌ/ rather than /ɑ/ in the word \"scarf\" (but not in \"scarves\"), which seems similar."}
{"doc_id": 145704, "author": "", "text": "This a metaphor. Both terms refer to plants, but words are not plants. Metaphors are rarely exact, so there's no reason to expect the difference between root and stem to be consistent for all languages. The distinction is only useful in a highly inflected language like Latin; in English both words are used in the same way -- to indicate what one adds an affix to. Since there are very few affixes in English, it really doesn't matter. In Latin, however, it does. Latin verbs typically have at least two, and frequently three, different stems: the infinitive stem, which forms the nonperfect tenses and some non-finite forms, and the perfect stem, which forms the perfect tenses and other non-finite forms. The case, gender, tense, person, number, mood, and/or voice suffixes are added to the appropriate stem. But each stem is formed from a basic root, for each verb. I.e, the metaphor is that the root is the base, and there are several stems growing out of it, all covered with fully inflected leaves formed by adding suffixes. For example, in Latin am- is the root for 'love', with infinitive stem amã-, perfect stem amãv-, and participial stem amãt- vid- is the root for 'see', with infinitive stem vidē-, perfect stem vīd-, and participial stem vīs- aug- is the root for 'help', with infinitive stem augē-, perfect stem aux-, and participial stem auct- cap- is the root for 'start', with infinitive stem capi-, perfect stem cēp-, and participial stem capt- More details are available here and here."}
{"doc_id": 146649, "author": "", "text": "I thought to quote from two websites that aided me, but to facilitate reading, I edit slightly and eschew blockquotes (>). The first quote is written with plainer and simpler diction and so ought to be read before the second with more formal diction. 1 of 2 quotes Bases, stems, and roots are the main components of words, just like cells, atoms, and protons are the main components of matter. In linguistics, the words \"roots\" is the core of the word. It is the morpheme that comprises the most important part of the word. It is also the primary unit of the family of the same word. Keep in mind that the root is mono-morphemic, or made of just one \"chunk\", or morpheme. Without the root, the word would not have any meaning. If you take the root away, all that you have left is affixes either before or after it. Such affixes do not have a lexical meaning on their own. An example of a root is the word \"act\". Now let's look at what is a stem and a base and apply them to the root \"act\" so that you can see how they differ and interconnect to transform a lexical word altogether. The stem occurs after affixes have been added to the root, for example: Re-act ↝ Re-act-ion Hence a stem is a form to which affixes (prefixes or suffixes) have been added. It is important to differentiate it from a root, because the root alone cannot be applied in discourse, whereas the stem exists precisely to be applied to discourse. A base is the same as a root except that the root has no lexical meaning while the base does: \"to act\" is the infinitive of \"act\" and is structured with the base \"act\". In many words in our language, a word can be all three: a root, base, and stem (eg: \"deer\"). They differ in how they are applied during discourse (stem, base) and whether, on their own, they have any lexical meaning (stem, base) or no lexical meaning whatsoever (root). An example of root, base and stem joined together is the word \"refrigerator\": The Latin root is frīg, which has no meaning in English on its own, and which requires a change in spelling for suffixes. ⟹ refrigerāre = Latin prefix + root + suffix, with no meaning in English of its own yet. ⟹ re- + friger + -ate + -tor = prefix + root + 2 suffixes. The 2 suffices now produce lexical meaning = stem; spelling changes are required for suffixes. [The links included with the answer contain the Glossary of Linguistic Terminology for further information.] Sources: http://www-01.sil.org/linguistics/GlossaryOfLinguisticTerms/WhatIsABoundRoot.htm http://www-01.sil.org/linguistics/GlossaryOfLinguisticTerms/WhatIsAStem.htm 2 of 2 quotes Root, stem, base Taken from: [...] English [W]ord-[F]ormation [...] by Laurie Bauer, 1983 (published by Cambridge University Press). ‘Root’, ‘stem’ and ‘base’ are all terms used in the literature to designate that part of a word that remains when all affixes have been removed. A root is a form which is not further analysable, either in terms of derivational or inflectional morphology. It is that part of word-form that remains when all inflectional and derivational affixes have been removed. A root is the basic part always present in a lexeme. In the form ‘untouchables’ the root is ‘touch’, to which first the suffix ‘-able’, then the prefix ‘un-‘ and finally the suffix ‘-s’ have been added. In a compound word like ‘wheelchair’ there are two roots, ‘wheel’ and ‘chair’. A stem is of concern only when dealing with inflectional morphology. In the form ‘untouchables’ the stem is ‘untouchable’, although in the form ‘touched’ the stem is ‘touch’; in the form ‘wheelchairs’ the stem is ‘wheelchair’, even though the stem contains two roots. A base is any form to which affixes of any kind can be added. This means that any root or any stem can be termed a base, but the set of bases is not exhausted by the union of the set of roots and the set of stems: a derivationally analysable form to which derivational affixes are added can only be referred to as a base. That is, ‘touchable’ can act as a base for prefixation to give ‘untouchable’, but in this process ‘touchable’ could not be referred to as a root because it is analysable in terms of derivational morphology, nor as a stem since it is not the adding of inflectional affixes which is in question."}
{"doc_id": 146650, "author": "", "text": "A root is the form to which derivational affixes are added to form a stem. (Sometimes stems are formed by derivational processes other than affixation.) A stem is the form to which inflectional affixes are added to form a word. In English, many words have no affixes, in which case there is no point in distinguishing word from stem or stem from root. Derivation, at least for English suffixes, is conditioned by part of speech, while inflection is conditioned by syntactic environment."}
{"doc_id": 146667, "author": "", "text": "Root, stem, base are used in various linguistic sectors with slightly different meanings, so in each case you have to get information how a special linguistic field or a special author uses these terms. In Latin grammar root and stem have one meaning, in the IE field another, so a general definition of those three terms brings nothing."}
{"doc_id": 153231, "author": "", "text": "These two pictures can assist."}
{"doc_id": 142570, "author": "", "text": "Technically it belongs to Lexicology, but since this last one in turn belongs to Linguistics, you could say that Etymology is part of it as well. Plus, Lexicography is actually under Lexicology. See the following scheme: N.B. I put the major disciplines for Linguistics. I might have forgotten some of them, though. If you notice some major discipline that is missing, I can edit the image."}
{"doc_id": 142571, "author": "", "text": "When etymology as such is covered in a linguistics curriculum, it is usually in a historical linguistics class, and I think what current research there is (not so much as there used to be, for sure) is often done by people who would consider themselves doing historical linguistics of some kind. Much of this kind of research is now under the heading of \"semantic change\", and is more theoretically motivated than traditional \"etymology\". I.e. the interesting questions aren't what are the histories of particular words, but rather, what explains the attested/possible patterns of semantic change. I think there is also a certain amount of less historically oriented work on semantic change in pidgins/creoles and that kind of thing. In general you may get the impression that it isn't part of linguistics because though it is a part of linguistics, that part is more or less outside the mainstream as represented in the USA. (Contra Alenanno's answer, I'm not really aware of any researchers, courses, or conferences that cover \"lexicology\" as any kind of coherent subfield in the way that the other things in that chart are.)"}
{"doc_id": 142574, "author": "", "text": "As a first approximation, one could probably claim that etymology is the diachronic dimension of lexicology but that would probably be too reductive. This is because to fulfil its role, etymology draws on many other disciplines of linguistics. For instance, etymology relies heavily on phonology to justify successive forms of words because it needs laws to justify how phonemes morph into one another or even disappear altogether. This in turn even allows the reconstruction of bygone pronunciations and unattested word roots. It also builds on morphology to explain how people evolve the words they need from the ones they have and semantics because meanings shift no less than phonemes. It is thus both a fundamental component of and an indispensable contributor to historical linguistics and works in this respect hand in hand with hydronymy and toponymy. Because historical linguistics adds the time dimension to synchronic linguistics it is one of the litmus tests allowing us to prove or disprove theories aiming at explaining how things are the way they are today as much as at predicting how they might turn out tomorrow. As such I shall venture that etymology is not only a part of linguistics but even a central one."}
{"doc_id": 142583, "author": "", "text": "A linguist who devotes their time to studying the phonetics (i.e. a phonetician) of an under-documented language, is still a linguist, despite their narrow focus. Likewise, a linguist who devotes their time to documenting the lexical environment of a language (i.e. a lexicographer) is still a linguist. If a question were posed to a phonetician about the occurrence of an alternation between two phonemes in language X, the question would be legitimate and relevant to linguists, even if the audience is small. So I am inclined to say that questions about specific word etymologies in a language X, would be well within the realm of linguistics to provide an educated answer, and hopefully expound on the example with interesting/relevant information."}
{"doc_id": 142584, "author": "", "text": "Words and concepts don't necessarily correspond for social reasons. The word 'etymology' is about individual word histories, which is definitely a part of linguistics. But that doesn't mean there is a sub-section of academic linguistics departments with faculty members who primarily call themselves 'etymologists'. Dictionary makers might have specialists in etymology (which would require all sorts of linguistics and particular language training). The people who 'do' etymologies might have more training in multiple specific languages. It doesn't seem likely that they would be a member of a Linguistics department's faculty (at least not in the US). Philology might be a label for a faculty that culturally etymology falls under elsewhere."}
{"doc_id": 147012, "author": "", "text": "\"Kingdom\" a higher taxonomic level, higher than \"phylum\". You can probably say \"superlevel\", if you desperately need it in one word, but I would rather say \"higher level in the hierarchy/taxonomy/classification\"."}
{"doc_id": 145956, "author": "", "text": "The fundamental difference is that generative grammar purports to be a model of mental processes and (quasi-classical, non-Sapirian) structuralist linguistics denies that or is agnostic. Technically, GG is a perfectly explicit description of the competence of the ideal speaker-hearer (Aspects p. 4), but then there isn't much GG around, given the \"perfectly explicit\" desideratum. Also, bear in mind that many people equate GG with \"Chomsky's current theory of syntax\", which is a misunderstanding of the concept."}
{"doc_id": 145961, "author": "", "text": "You cannot really compare structuralist and generative linguistics directly. In broadest terms, generative linguistics is one way to study and model language structure. It is therefore a part of the broader structuralist program (see for instance F Newmeyer). In particular, generativism shares these structuralist commitments: Language as an interconnected hierarchical semiotic system Distinction between competence and performance (languge/parole) Meaning and function expressed through distinctive features Language susceptible to syntagmatic and paradigmatic analysis Specific concepts such as phoneme, morpheme, lexeme Language units as symbolic pairings of meaning and form Linearity of language Etc. While many of the generativist approaches grew out of a critique of the structuralist orthodoxy of the time, they still stayed firmly within the broader structuralist framework. In some way, generative linguistics could be seen as a contribution to a structuralist theory of syntax."}
{"doc_id": 154427, "author": "", "text": "1- particular grammar of a particular language which, in a purely mechanical way is capable of enumerating all and anly the grammatical sentences of that language. Generative grammar in this sense was introduced by Noam Chomsky in the 1950s 2- Any theory of grammar which has as its goal the construction of such grammar Generative grammar has its roots firmly grounded in the structuralist tradition. Generativists share with structuralist the idea that \" the grammar of a language is a statement of the systematic structural interrelationships holding between linguistic elements \" New Meyer 1992:46"}
{"doc_id": 146431, "author": "", "text": "French and Spanish are indeed members of the Romance branch, but French is an oddity within it. If you speak only English, the phonetics of Spanish are probably much easier than French sounds, so you'd probably make a quicker start in Spanish. On the other hand, if you learn French first, Spanish would then be relatively easy."}
{"doc_id": 146441, "author": "", "text": "The question \"how similar are these two languages\" can't be easily answer. There are many levels at which you could make a comparison, and it's not obvious how you want to quantify that. If we're talking just about lexical similarity then the answer is very similar, you can take a look here."}
{"doc_id": 148245, "author": "", "text": "I mean that if I have mastered one of them, how much easier will it be for me to learn another one? First, knowing two languages instead of only one already makes it easy to learn a third one. At the very least, you won't take the quirks and irregularities of your first language for granted, and will be prepared to expect something different. Second, learning a language of a given linguistic family definitely makes it easier to learn a further language of that family. This effect is incremental: if you know two languages of a given family, it is even more easier to learn a third one. You will be acquainted to at least part of the lexical entries one needs to learn. Third, specifically, no, French and Castillian are not particularly similar. If you master Castillian, you will probably be able to read a text in Portuguese or Catalan and understand most of it; but this is not true of French. However, English has borrowed so much lexical entries from French, that a person whose first language is English, and learns Spanish, will probably be in a better position to extract a basic comprehension from a French text than a person whose first language is another one, even another Romance language. So, in short, learning one of them won't preclude the need of specialised classes to learn the other. Those classes will be somewhat easier, especially if you learn Spanish before French. Think of the relation between both like the relation of English to German. There is a similarity, but the languages are far from mutually intelligible."}
{"doc_id": 219176, "author": "", "text": "The debate about whether curved or straight swords are superior is as old as warfare itself. Cultures all throughout history have had different preferences. Generally the European cultures (Greeks, Romans, Medieval) preferred straight, while the Asian cultures (Mongols, Japanese, Saracenes...) preferred curved. The main difference is that curved swords are better at slashing while straight swords are better at thrusting. This is mostly relevant if you are fighting enemies who wear armor. If the enemy is wearing chainmail or plate armor, then slashing attacks are unlikely to hurt them. You then want a straight sword which allows you to do a thrusting attack which breaks through their armor. Straight swords also give you more range for the same weight. This is useful for fighting in close formations where every cm of range counts. That's why traditional close formation armies (like the Roman legion) used straight swords."}
{"doc_id": 219177, "author": "", "text": "As always for weapons, there are no weapon better than another, if one of them would be useless, we wouldn't have any historical exemple of it. It's just different combat style. Depending on how curved the blade is, the slash damaged would not be the same: not as deep, or as big area. Another thing to keep in mind is that sometimes, weapon can keep stuck in bodies, a slightly curved blade can prevent this. As the other answer says, straight blade are better to thrust weak point of ennemies. However, a curved blade can be harder to parry. With a sword like the Khopesh, you can bypass a shield. The Dacians also win some battles against romans thanks to Dacian falx, hitting over roman shields. Romans then redesigned their helmet, and crushed the Dacians. It's a great exemple, as romans had straight short blades: there is no blade better than another, you just have to have a blade depending on the ennemy equipment."}
{"doc_id": 219186, "author": "", "text": "The difference is because of function: This is a pretty simple difference. They are shaped differently because they are used for different kinds of enemies and combat styles. Curved sword are generally better at cutting. This is because they have a longer surface area of blade that generally follows the motion of your cut and is therefore in contact with your target longer. This can create a deeper cut. With curved swords you generally try to do what is called a Draw Cut. This is when you cut something and slide the entire length of the blade against the target. By doing so you can slice much deeper than just by hacking. Curved swords generally appeared in areas or ages where the contemporary enemy was lightly or unarmored. This is because the earlier mentioned deep cutting is very effective against bare human bodies, but not so much against chainmail or plate mail. This is was this weapons selling point was and is why we see them more in the Middle East, ancient Egypt etc where and at times when the enemy would have been more lightly armored. Straight Swords were generally designed to have some capacity for piercing. While not every straight sword was necessarily designed to pierce, and by all means straight swords can cut, historically the great increase in the use of straight swords in for example Europe was in response to a greater need for piercing ability. Piercing weapons I believe are more effective against armored opponents. This is suggested in Half-Swording and other techniques that emphasized using the point of a blade to bypass armor. While every sword seeks to be the most optimal tool, balancing cutting and slashing often at a tradeoff, in Europe it seems that the need for thrusting overtook the need for cutting at the onset of the high Medieval ages. This is probably why straight swords became so popular around this period. As we move into the renaissance and age of guns, when armor again became minimal you will notice that there was a return to curved blades in the form of sabres. Changes in sword design across history tended to follow the changes in what was optimum for combat in the region and period where it was used. Blacksmiths knowing these differences and military practitioners wanting to survive, naturally picked the best designs for whatever kind of combat they were facing. This is how these different sword came to be designed, and why they were used and became popular. Purely for best function (usually at the trade-off of cut vs thrust) depending on what was needed. Some light reading for you, why curved for cutting: http://raynfall.com/2539/the-physics-of-the-cut/ Why straight swords overtook curved swords in Europe during the high medieval ages (armor prevalence): http://www.thearma.org/essays/thrusting_vs_cutting.html#.WtDq0LpFy70 This is not really official, but is a good conversation on this topic: https://www.reddit.com/r/AskHistorians/comments/1w4gbt/why_did_knights_still_use_swords_after_the/ Why some straight swords were not pointed: https://www.youtube.com/watch?v=gVrYt5A3VyA"}
{"doc_id": 176590, "author": "", "text": "Dinosaur genetics is not crazily different from bird genetics and mammal genetics... Humans currently could still mate with neanderthals who are different form us by about 800.000 years of evolution, i.e. 34000 generations... I'd say that's quite a high limit on maximum generations of an advanced vertebrate between mating, it depends how fast the creature evolves, birds and reptiles do actually evolve rather fast in between generations same as mammals. Biologically speaking, that's a fair estimate of the kinds of limits of generations in prior to total species division of advanced vertebrates, 50k generations, 100k even. Interesting to know: Crocodiles are very slow changing dinosaurs, there are dinosaurs from 110 million years ago that are nearly the same as today, we can suppose that speciation of dinosaurs can take many millions of years. Along with pterosaurs and dinosaurs, crocodiles were an offshoot of the archosaurs, the \"ruling lizards\" of the early to middle Triassic period (needless to say, the earliest dinosaurs and the earliest crocodiles resembled one another a lot more than either resembled the first pterosaurs!). What distinguished crocodiles from the first dinosaurs was the shape and musculature of their jaws, which tended to be much more deadly, as well as their relatively splayed limbs (as opposed to the straight, \"locked in\" legs of theropod dinosaurs)."}
{"doc_id": 176591, "author": "", "text": "No. Allosaurus and T. Rex may look superficially similar, but they're separated by close to 100 million years of evolution. Genetically, they were probably significantly more different than humans and chimpanzees."}
{"doc_id": 176592, "author": "", "text": "No They Could Not, And Here Is Why If our current knowledge about the classification of dinosaurs is correct, then no, Allosaurus couldn't mate with T. Rex even if both lived in proximity. The reason for this is that Allosaurus belongs to Carnosaur group of predators (which contains African and South American predators such as Carcharodontosaurus and Giganotosaurus) while T. Rex belongs to Tetanurae group (which contains raptors, birds and other feathered beasts). You can say that if Allosaurus is modern day polar bear, then T. Rex would be jaguar. Both are carnivores, both are mammals, both live at the same time, but a polar bear cannot mate with a jaguar even if you keep them both in the same cage for years and years. Should You Really Care If They Couldn't? The real question should not be whether Allosaurus count mate with T. Rex but do you want to have that in your story/movie/game? If you want to do that, you don't need to find excuses for that. Just do it. Your readers/viewers would know that you are writing science fiction and not a scientific article or documentary. So as long as the plot is interesting and fun to read/watch, it's all cool. Heck! When Jurassic Park 4 could have a 40 feet high, camouflaging, thermal sensing monster (actually it would be extremely hard to even get a 40 feet high predator work at all!), your idea is much more believable and fun!"}
{"doc_id": 176593, "author": "", "text": "T. rex and Allosaurus are both theropods, but they're members of completely different families--Tyrannosauridae and Allosauridae. I don't know very much about genetics or taxonomy, but the first comparison that comes to mind is a dog mating with a cat. Canidae and Felidae are both in the same order, but they're completely different animals and wouldn't be genetically compatible. As far as I know, the false killer whale and the bottlenose dolphin are the only known animals capable of producing fertile hybrids across genera, and even then their offspring rarely survive into adulthood. And those are two animals in the same family. I do think it's possible that a T. rex might care for a baby Allosaurus as if it were its own. Lots of animals \"adopt\" members of different species for some reason."}
{"doc_id": 176594, "author": "", "text": "Breed may be defined as having fertile offspring, or simply offspring. Consider the mule, generally infertile offspring of a donkey and a horse. Ligers, on the other hand, may be fertile. So if you are screwing with the natural evolution of your planet enough to make Allosaurus and Tyrannosaurus contemporaries, feel free to make them related closely enough. Just don't call it science-based ..."}
{"doc_id": 242705, "author": "", "text": "It's basic trigonometry. In this case, the rule is known as the law of cosines: c² = a² + b² - 2ab cos(C) You already know distances \"a\" and \"b\", so all you need to do is measure angle \"C\", which is a simple observation from Earth, in order to calculate distance \"c\". From Math Is Fun."}
{"doc_id": 242716, "author": "", "text": "You need to measure the angle between the two stars, as seen from the Earth: To do this, you can use an inclinometer, which measures the angle between the ground and where you are looking. Find the angle to the first star, then the angle to the second star and subtract the smaller of the two from the larger to find the difference. Once you've done that, the distance between the two stars is: $$ \\sqrt{a^2+b^2-2ab\\cos{(x)}} $$ Where $a$ is the distance to star A, $b$ is the distance to star B, and $x$ is the angle in between them. This can be in degrees, radians or any other unit as long as you use the appropriate angle mode when calculating the cosine. The unit for the distance is the same as the unit of distance between the stars and Earth. Or, without the notation: Square the distances to each star and add the squares together. Multiply the distances together and then multiply the result by 2 times the cosine of the angle between the stars. Subtract the result of step 2 from the result of step 1. Take the square root of the result of step 3. This is the distance between the two stars. This works because of the law of cosines, which demonstrates how to find distances and angles in non-right-angled triangles."}
{"doc_id": 242722, "author": "", "text": "Sorry this has taken a while to bring up to Hard Science standards but here we go. If you want to calculate the distance between a set of stars you're interested in first you need to create Sol relative XYZ co-ordinates for all the stars you're interested in. This requires the distance from Sol, the right ascension, the declination and some trigonometry, once you have those co-ordinates you can create a simple table that compares the various co-ordinate sets and gives relative distances between any given pair of stars. Assuming you're working from a data table like this one from Wikipedia the first thing you need to do is convert the Right Ascension and Declination into usable digital figures, for most software based computation that's actually Radians but degrees are an important intermediary step in my opinion, because the conversion is relatively simple and so errors are easier to spot. Right ascension is tricky-ish to convert at first but basically one hour of Right Ascension is 15° so divide the seconds(s) by sixty, add that to the minutes(m) divide the result by sixty again and add it to the hours(h), finally multiple that by 15 and you have the digital degrees equivalent for the Right Ascension figure. Declination is easier divide the seconds of arc(\") by sixty add to the minutes of arc(') divide the result by sixty and tack it onto the degrees(°) that gives you the digital degrees of Declination. To get the Radians needed for Microsoft Excel or similar divide those degrees numbers by 2π. The distance from Sol can be in LY or Parsecs at the discretion of the mapper. To get the XYZ co-ordinates from this data requires further working, in these equations RA is the Right Ascension, DEC is the Declination and D is the distance from Sol. With that in mind: X= D x cosRA x cosDEC Y= D x sinRA x cosDEC and Z = D x sinDEC Once you have a data sheet of XYZ co-ordinates you can build a table of the relative distances between all the stars you're interested in. This table compares the relative position of the stars it does this by taking the square root of the sum of the products of the differences between the individual co-ordinates. In other words each cell of the table has the following equation: √ ((X1-X2)2+(Y1-Y2)2+(Z1-Z2)2). Where 1 is the co-ordinate for the star entered in the left-hand column of the table and 2 is the data for the star on the horizontal header for that particular cell, the table is similar to this one showing distances between towns in New Zealand's North Island: But with entries for Alpha Centauri, Proxima Centauri, Barnard's Star and so on. To find the distance between any two stars all you need do is reference them by name in the correct column and row in the table. I have such a table for all the stars out to 25.5LY from Sol and the working data floating around somewhere, let me know if you want it."}
{"doc_id": 242775, "author": "", "text": "The easy way: USE WOLFRAM ALPHA Simply type \"distance between [star A] and [star B]\" and it provides the answer in a second (see screen dump below). Incidently, the distance between Sirius and Capella is almost exactly 12 parsecs - the shortest way to do the Kessel Run, according to Han Solo."}
{"doc_id": 214663, "author": "", "text": "The descriptions of foods you mention above sound to me a lot like a Paleo diet. If nuts and seeds don't count as edible wood, then you wouldn't even have to modify the diet much at all; you may need to take out avocados and their oils for the lycanthrope, and (possibly) some of the very green and leafy vegetables with high mineral content (because of the possibility of silver) but most of it should be fine. For the sake of argument though, let's assume that we can modify the Paleo diet by taking out nuts and seeds, as well as getting rid of spinach and avocado to be sure. We'll call it Occult Paleo. Black pudding can be eaten under such a diet, excess meat doesn't seem to be a problem and most of the vegetables are going to be fine (sans spinach and avocado). You can't do chocolate under this diet which works for the lycanthropes, although to be sure cocoa should be avoided altogether. Bottom line is that Paleo is a good starting fit, with a few extra restrictions to cover off the foods that are bad for dogs especially. If nothing else, it's a food program that's already mostly understood by some restaurants and caterers, so modifying it for your vampires and werewolves wouldn't take as much effort as starting from scratch."}
{"doc_id": 214666, "author": "", "text": "I sincerely could not think of any food containing silver, so I had to Google it out. Turns out that eating silver is a thing, especially in Southeast Asia: Vark, also called varak (also silver leaf, German paper), is super fine filigree foil sheet of pure metals, typically silver but sometimes gold, used to decorate South Asian sweets and food to make those look more appetising. The silver and gold are edible, though flavorless. (...) Estimated consumption of Vark is 275 tons (according to BWC-Beauty without cruelty data) annually (c. 2016). However, it seems that not only werewolves and vampires would be badly affected by it (though regular humans could at least tolerate larger amounts of it): Concerns have been raised about the ethical acceptability and food safety of Vark, as not all of it is pure silver (sometimes aluminium is used, which is toxic), nor hygienically prepared. Every responsible dog owner knows that there is one human food that is poisonous to the vast majority of dogs: chocolate. Check this site for the reason, data, and a toxicity calculator. Supposing a werewolf weights at least as much as a human, they should be able to tolerate more chocolate than a dog, but they will still get intoxicated. Maybe the calculator helps you with ideas for how they would feel. The active component of chocolate that harms them is called theobromine, by the way. Pet owners also know that caffeine is extremely dangerous to dogs, but it should not be so for werewolves. The reason is that the lethal dose of caffeine is about the same for dogs and humans: around 150mg per kilogram (about 2.2 pounds) of body weight. Pets usually suffer caffeine poisoning because they weight so much less than us. However, do notice that when caffeine is metabolized by a human, around 12% of it turns into theobromine (see the link I mentioned before), so I would expect a werewolf to at least suffer a little from a few cups of coffee. Finally, most dogs hate citrus/citronella. Many dogs don't, though. I believe it's due to their sense of smell being so much more sensitive than ours. I would expect werewolves to avoid it like we humans avoid rotten food. So, long story short: food products containing theobromine should be labeled so in order to protect werewolves, and both werewolves and vampires should avoid vark."}
{"doc_id": 214667, "author": "", "text": "They would need to be forced to learn how to order \"steak, blue or rare, no garnish\". And how to cook. They don't need to adapt to anything. Just go there and ask for a \"burger, hold the garlic\". I'm looking right now at a take-away menu and there is disclaimer \"If you have ANY allergy to ANY products used in our menu please let us know. We will prepare your dish in allergen free pots\". I don't know how about your local shop but mine hasn't carried mandrake since 1450. So that is one problem less. But now we can call butcher that will deliver half of ox to your door in 30 minutes. No questions asked. Because why would you want to ask why somebody need half an ox? The main problem you would need to address is why they need human blood. Main difference in human vs. animal blood is protein pockets. But the proteins are transported in the blood, you don't need to consume it to be able to have your proteins. And if you answer would be along \"because I says so\" then you can go and just say that no one needs to change anything. Your vampires and lycanthropes just learn to read ingredients on the back of the chips."}
{"doc_id": 214679, "author": "", "text": "A dog/wolf's poisoning due to consuming chocolate is called Theobromine poisoning and is due to more than just chocolate. The linked article lists a few other foods. Theobromine poisoning, also wrongly called chocolate poisoning, is an overdosage reaction to the xanthine alkaloid theobromine, found in chocolate, tea, cola beverages, açaí berries, and some other foods. I would recommend you read the article. It's fairly short and has some good discussion about toxicity levels based on the animal's weight, etc. You might have some lyncanthropes in your world that eat chocolate or drink soda in spite of the risks and are only moderately affected based on their level of consumption and regularity. Maybe a parallel to humans drinking alcohol despite the negative short-term and long-term affects."}
{"doc_id": 158175, "author": "", "text": "No. At these speeds, we have to take special relativity into account. The relativistic kinetic energy formula is $$KE_r=mc^2 \\left(\\frac{1}{\\sqrt{1-v^2/c^2}}-1 \\right)$$ Taking the limit as $v \\to c$, we see that the kinetic energy becomes infinite. So, actually, I was wrong before - there is no limit to how much kinetic energy a particle can have! However, a particle would have to have a lot of energy to destroy even one star in the manner you described. It would need to have a kinetic energy upon impact of the star's gravitational binding energy: $$U=\\frac{3GM^2}{5R^2}$$ where $M$ is the mass of the star and $R$ is the star's radius. For a star like our Sun, with $M=1.989 \\times 10^{30}$ and $R \\approx 696342000$, the gravitational binding energy would be $$U=\\frac{3 \\times 6.676 \\times 10^{-11} \\times (1.989 \\times 10^{30})^2}{696342000} \\approx 1.38 \\times 10^{42} \\text{ Joules}$$ Which means it would have a velocity of . . . something very large. The relativistic kinetic energy formula gives me something extremely close to $c$. If you want to really shake things up, you have to rip the constituent quarks out of each proton and neutron - which is also impossible. The vast majority of the proton's rest mass is due not to the rest mass of the quarks, but to their energy. However, the strong nuclear force is a real - uh, tough opponent, so put it one way, and it's the strongest of all the fundamental forces. You can't really beat it, though you can try it. Now do that for all of the hundreds of billions of stars in the Milky Way, as well as all the other bodies in the galaxy. However, all of this is still possible - you'd just need a lot of energy."}
{"doc_id": 158177, "author": "", "text": "No. Any particle at such speed will very heavily interact with anything in the universe, from CMB photons and protons from distant stars to gravitons of gravitation waves, all these particles will be hugely spectrum-shifted for the propagating particle. Note that there is a lot of CMB photons around us (they are the most common particles in the universe), but we do not notice them because they have very small energy. The smaller energy, the more photons there is. So the cosmic vacuum as we see it will be a very dense medium from the particle's point of view at such speed. Even passing any particle without direct collision will produce substantial gravitation waves. This means the fired particle will be slowed down, lose its energy to radiation (both photonic and gravitational) and produce a lot of new particles. The interaction will start shortly after the particle leaves the gun, may be some meters apart from it. This will produce a huge explosion with creation of a lot of matter-antimatter pairs and radiation. Some of it will be radiated away into every direction, not necessarily the same as the original one, the rest will produce a huge plasma fireball. This fireball after travelling some thousands years towards the target galaxy will cool down and slow down. And inside it proto-stars, planetoids and may be, black holes will appear. Upon reaching the target galaxy the result will be like a collision of a young galaxy with an older one. Even if the total energy of this bullet would be enough to desintegrate the target galaxy, by the time it reaches the target the most energy will be either radiated away or in the form of stellar bodies, stars, planets and gas, moving at quite moderate speeds. They rather will merge with the target than disintegrate it."}
{"doc_id": 158178, "author": "", "text": "(Incorrect ideas from kinetic energy formulae edited out. Thanks to comments from HDE 226868 and BartekChom. Remainder applies, however.) I would say that even if you had something with extremely high energy that would hit interstellar dust, it won't create a \"beam\", because the dust will 1) be destroyed and 2) still have its original velocity component, and go sideways, and so spread out. Moreover, by conservation of energy, I wouldn't think this would add any energy to your attack, though it would spread it out, making it more likely that some of it would hit something. Which pertains to another difficulty: even if you did have a particle that somehow had crazy-high momentum, you might need to somehow aim it very carefully, or it might most likely just cruise through a galaxy without contacting any significant body. Even if you do hit a large star with a ridiculous amount of focused kinetic energy, I know I don't know what that would do. Maybe a good theoretical question for a physicist. Even if you cause some sort of high-energy supernova of the star you hit, I don't know if that would result in a star's worth of also-super-powerful matter flying outwards, or what. Finally, I think the main reason this probably would never happen would have to do with mechanics of how this much energy could be put into one particle, whether the particle wouldn't just change into something else during the process, or decay to something else along the way, and on top of that, how you would ever get that much energy in the first place."}
{"doc_id": 158179, "author": "", "text": "Yes. There is no limit to the amount of energy you can put into a mass-bearing particle by accelerating it. No matter how much energy you put in, the particle still won't reach the speed of light, so you can still add more energy. Assuming it will take a finite amount of energy to destroy your target galaxy, this means your idea is theoretically possible. But practically, where would you get all the energy to put into your particle? Of course you can get energy from matter that exists in the universe. But I imagine you would need to consume the matter from very many galaxies in order to get the energy to destroy just one [citation needed]. In which case, it might just be easier to use your existing galaxy-consuming technology instead of the extravagant single-particle method."}
{"doc_id": 158180, "author": "", "text": "No. It might do some damage initially, but it can't keep doing damage indefinitely. On the first collision, no matter how fast the particle is moving, it will impart energy to the other particles. To break the bonds holding planets and stars together, it will need to convert their potential energy (stored in the bonds) to kinetic energy. Keep in mind that total energy must stay the same, meaning that both particles will now be moving slower than the original particle's speed. This means that your \"particle avalanche\" will be continuously slowing down as more and more matter is added, and it must slow down rapidly to get other particles moving fast enough to escape a high amount of potential energy (in the bonds). This means that the destructive power of the particles will decrease accordingly, since individual particles will not have enough kinetic energy to continue destroying bonds. Additionally, such high energy collisions tend to do things other than just smash stuff apart. Consider fusion: high energy collisions of hydrogen atoms results in conversion of some matter to light (releasing photons on a wide range of the spectrum) and actually overcoming the necessary energy boundaries to form a nuclear bond. This actually reduces the overall kinetic energy of the matter by converting a lot of the energy into photons and potential energy. So the speed of the particles will not stay high enough to cause that level of destruction on a galactic scale. Even if it were possible, it would be ridiculously impractical. Our galaxy is over 100,000 lightyears across. This means that it would take well over 100,000 years for the damage to propagate across, and probably longer for things to actually settle down. I imagine that natural processes would just begin anew in that time, with new nebulae and stars and planets forming eventually (though not within 100,000 years). And this is one of the smaller galaxies out there. By the time it was done, the inhabitants of the galaxy would have plenty of time to react or possibly retaliate (assuming practical space travel). They might even find a way to stop it. This also ignores the feat of imparting that much energy to a single particle in the first place without blowing yourself up first, which is far less practical than firing off a large number of very high energy particles."}
{"doc_id": 158182, "author": "", "text": "No, a galaxy is too big and too low-density Consider what you're trying to do. You're trying to disrupt an entire galaxy! (As a note: I am assuming you will not consider \"moved the galaxy 4in to the left\" to be destroyed. You want to see structural changes) You obviously need to impart kinetic energy into the galaxy. You can only really consider movement which is fast enough to cause serious damage. If your particles get too slow, they wont be disruptive enough to do any real effect to the structures held in place by gravity. As a first attempt, lets try to play a game of billiards. Lets try to hit a planet, so we can send it careening into a sun, and try to make the sun a billiard ball. Right away, we see a huge problem: acceleration. Imparting all of your particle's energy into a planet is not easy. Consider a particle going just shy of the speed of light (rounded up to $c$ just to keep things simple). If that planet were Earth, the particle would go from one side of the earth to the other in 42 milliseconds. To do any real damage with our new billiard ball, it needs to be going pretty darn fast. Lets say we want to make the sun travel 1km/s in a direction of our choosing by hitting it with the Earth. For perspective, that's not even fast enough to escape the moon's gravitational pull, but we'll use these little numbers for now. The sun is 330k times more massive than the earth, so the earth is going to have to be going roughly 330k times faster than our final speed of the sun. We need to impact the earth to make it go 330km/s. This should raise some eyebrows, because that's faster than the speed of light. The only way to add 1km/s to the velocity of the sun requires accelerating the Earth to relativistic speeds (where the kinetic energy effectively adds mass). Now we see the real problem. Lets pretend we were okay with accelerating the earth to JUST 100km/s. No biggie. We only had 42 milliseconds between particle hitting the earth and passing through. That corresponds to 243 kilo-G's of acceleration on the earth. This is an unimaginably high number, and the very imaginable outcome will occur: the particle will break through the Earth, rather than taking the mass with it. This problem occurs whenever we need to accelerate a mass to a very high speed (such as 20% the speed of light). Whenever we try to do this, we need to do it slow. Doing it fast will just cause the moon/planet/sun to shatter, distributing your energy in all directions, bleeding off your momentum. This will occur whether you hit your moon/planet/sun with a particle or another planet. Whenever you need to accelerate other particles to relativistic speeds, you'll find you just bust straight through them. And this is what makes a galaxy invincible to such activity. Until you start sending high velocity black holes through, the galaxy wont even notice. And even they won't have the massive galaxy-shattering effects you are looking for. Consider we believe we have a black hole in the center of our galaxy, and its a reasonable sized one! Now what if you really wanted to make this work. What if you were a god which knew where every particle was at all times, so you could cast your relativistic proton like a pool shark, and clear the table? Your best bet would be to start millions of light years away, if not further. Line up your shot to collide with a bunch of intersteller hydrogen. But don't hit each one head on. Your goal should be to create a pressure wave of hydrogen moving forward. You want each collision to perfectly impart half of its momentum to the next particle to build the chain reaction. Take your time, and make sure to chalk the tip of your cue first. Eventually you could create a wave which would act like a tsunami when it finally hit the galaxy, tossing it around like a crab ship in a stormy sea. Of course, this was assuming you could predict everything. If you could do that, you could always just play the lottery. I hear the Betelgeuse pick-70 lottery has a really strong payout!"}
{"doc_id": 158186, "author": "", "text": "No, for all the reasons everyone else has said, plus you'd tear the particle apart and everything around it long before you poured the necessary energy into it. Here's some limits to the energy you can put into a particle. 2 x 10^12 K is the Hagedorn Temperature where hadrons (protons, neutrons, electrons) melt into quarks and gluons. So you can't use a proton, it has to be something smaller and more fundamental like a quark. And you have to hope that quarks don't break down into something else. The LHC regularly achieves these energies and no galaxies have yet to be blown apart. 1.41 x 10^32 K is the Planck Temperature above which our current physical theories break down. At TP the wavelength of the object is the Planck Length and we don't know what happens then. Since the energy needed to tear apart a single star is 1.38×10^42 Joules, and you're dealing with a miniscule amount of mass, and you want to tear apart billions of stars in a galaxy, it's fair to say you'll hit the Planck Temperature before you can get enough energy into your particle. Though it would be interesting to see someone do the math. Since you'd need to accelerate your particle to very, very, very close to the speed of light, there's practical issues with how you'd accomplish that acceleration once it's in the 0.9999c region. Simply put, your particle will outrun your attempts to pour more energy into it. It has so much energy that curving its trajectory in a particle accelerator would prove unfeasible. You'd have to make a cosmic railgun on a galactic scale. Again, it would be interesting to see someone run the numbers on this. And pumping too much energy into too small a space has an annoying tendency to form a black hole. VSauce has a nice video about \"absolute hot\" called How Hot Can It Get with many sources in the description."}
{"doc_id": 158190, "author": "", "text": "What you're talking about here is basically the same thing as a cosmic ray air shower, except that it would have to take place in intergalactic space instead of in the atmosphere, and the amount of energy involved would have to be unimaginably higher. There are two factors that make this event rather different from an air shower: There are fewer particles to hit in intergalactic space This would require the energy released from the formation of an entire galaxy to somehow be concentrated in a single particle Reason #2 is enough, on its own, that this would never happen in practice. But since the premise of the question appears to be that, somehow, that reason has been bypassed, let me go through the relevant calculations. First of all, the amount of energy in the particle needs to be enough to cancel out the binding energy of the galaxy and all the stars and planets inside it. From this presentation, slide 10, suppose the galaxy's gravitational binding energy is $M(10^{-3}c)^2$, which works out to roughly $10^{53}\\text{ J}$ assuming $M \\approx 10^{12}M_\\odot$. This would be the amount of energy required to separate the galaxy into individual stars. Then, let's approximate the amount of energy required to separate all the stars, planets, etc. into atoms as $10^{42}\\text{ J}$ per solar mass, which gives another $10^{54}\\text{ J}$ total. So the incoming particle will have to have $10^{54}\\text{ J} \\approx 10^{73}\\text{ eV}$ in the galaxy's rest frame. (Actually a little more because it needs to transfer some energy to the remnants of the galaxy as kinetic energy, but this excess is something like a factor of $10^{-6}$ smaller and thus negligible.) So suppose we have a particle of energy $10^{73}\\text{ eV}$ somehow propagating through the universe. Now, we know nothing about how a particle with such a tremendous amount of energy would actually interact with ordinary matter. Such a high energy is firmly into the domain of (beyond-)nstandard-model physics. For purposes of a science fiction story, you could make it do all sorts of weird things. But, sticking to the current science for the sake of argument, let's say you naively extrapolate the known behavior of high-energy scattering to this $10^{73}\\text{ eV}$ cosmic ray. The next thing to figure out is the probability of the cosmic ray scattering off the particles it meets. And the relevant parameter to characterize this is the squared center-of-mass energy, $s$. For a collision between a massive particle in motion, with mass $m_1$ and energy $E_1 = \\gamma_1 m_1 c^2$, and a massive particle at rest, with mass $m_2$ and energy $E_2 = m_2 c^2$, this is $$s = m_1^2 c^4 + m_2^2 c^4 + 2E_1 E_2$$ Alternatively, for the same massive particle and a photon which has energy $E_2$ and is approaching the moving particle at angle $\\theta$ (with $\\theta = 0$ being a head-on collision), assuming $E_1 \\gg E_2$, the CM energy is $$s = m_1^2 c^4\\biggl(1 - \\frac{E_2}{E_1}\\cos\\theta\\biggr) + 2(1 + \\cos\\theta)E_1 E_2 + \\text{negligible terms}$$ So $s$ for an interaction between the cosmic ray and a massive particle is a fixed, very large value. Interactions of this sort generally get less likely as $s$ increases, so a particle with $10^{73}\\text{ eV}$ is basically going to pass right through matter as if it doesn't exist. But for an interaction between the ray and a photon, $s$ varies depending on the angle. It goes as low as $s = (m_1 c^2)^2$, when $\\theta = \\pi$ (the photon and the cosmic ray are traveling in the same direction), and goes all the way up to more than $4E_1 E_2 \\approx 10^{89}\\,\\mathrm{eV}^2$. This is important because the interaction between two particles is most likely at a resonance, a center-of-mass energy which corresponds to the mass of some intermediate particle. For example, the delta baryon has a mass of $1232\\,\\mathrm{MeV}/c^2$, and therefore interactions between charged particles and photons are particularly likely when $s = (1232\\,\\mathrm{MeV})^2$. The cosmic microwave background (CMB) provides an ample supply of photons traveling in all directions, and thus any charged particle with enough energy to achieve $s \\ge (1232\\,\\mathrm{MeV})^2$ in a collision with a CMB photon will be very likely to do so quickly. Particles with such high energy simply do not propagate very far through space. This effect is known as the GZK limit, and the associated energy cutoff is on the order of $10^{19}\\text{ eV}$ (in the rest frame of the CMB). The exact order of magnitude varies by the type of particle involved, but regardless of what type of particle it is, anything that has $10^{73}\\text{ eV}$ will be well over it. In fact, a high-energy cosmic ray with enough energy to destroy a galaxy will hit not only the delta resonance, but the resonances of every particle in the standard model, and any unknown particles that may exist with higher masses up to a very high threshold. (Higher than the Planck mass, thus underscoring the need for a theory beyond the standard model to explain what happens.) Anyway, the gist is that a particle with this huge amount of energy will pretty much immediately produce a shower of other particles of all sorts, with energies rapidly dropping as the shower progresses. This is actually perfect for your scenario, because it spreads out the immense amount of energy from one particle (which, as I mentioned, passes right through matter) to a broad swath of particles which is rather well distributed for galaxy destruction. One might then consider the question of how close to the galaxy you have to produce the highly energetic particle in order to make this work. The answer to that depends on the characteristic length scale of the shower, which in turn depends on the scattering cross section and some complicated math that I don't want to get into now. If I figure it out later, I'll come back and add details, but for now, my conclusion is that if you ignore the main reason this could never happen, it actually seems quite plausible."}
{"doc_id": 223434, "author": "", "text": "A human cannot live more than 3-4 days without drinking, especially in a broiling environment like a desert. Their immediate chances of survival depend on the ability to find an oasis, where they can gather water, within 3 days at most. Then comes the issue of clothing: a warm desert is a nasty place, one need to be protected from the sun and heat during the day, and from the freezing cold during the night. No, the fancy bermuda and short sleeves shirt are not going to do a lot for them, it would be better if they would be dressed like the Tuareg, with wool to insulate them from the environment. Last but not least, the problem of food: finding food in a desert is pretty difficult, but a human can survive some week without food as long as water is available. But if they manage to find an oasis, they might also find some food: birds, plants, rodents, insects. Of course, the above figures are valid for healthy people in decent conditions. If they are older or younger, the survival span may drastically shorten."}
{"doc_id": 223439, "author": "", "text": "Yes. But they need a guide. http://www.zbrushcentral.com/showthread.php?38562-Lizard-Man Random humans in the desert will die, as prior answers lay out. But your castaways encounter a native who shows them how to survive. I envision this native as without speech, or at least speech the humans understand. Its motives are unclear but it becomes very clear that this alien desert is dangerous, and without the help of the guide the humans will rapidly die. I like the idea that the alien guide is paid by the humans in song. The humans know a lot of songs. We all do."}
{"doc_id": 223468, "author": "", "text": "Remember the \"hierarchy of survival.\" Humans can survive for 3 minutes without oxygen, three hours without shelter (meaning naked in any extreme climate, hot or cold), 3 days without water and three weeks without food. Based on that hierarchy the heat could kill them in a few hours, and lack of water in a few days at most. Even if they overcome both of those issues, in whatever way, they're probably going to starve to death within weeks unless someone has the expertise and equipment to do some hunting or foraging. You need to describe your setting well because depending on what kind of desert they're in they're possibly going to die a lot faster or last a lot longer; something like the Sonoran is far more survivable than the Empty Quarter's sand seas."}
{"doc_id": 209610, "author": "", "text": "There are two aspects to the questions: On the one hand, there are more than enough humans fearing fire and any intelligent creature in the wild is likely to be wary around fire because of its destructive capability. Any wild animal in regions where fire can occur naturally as bush fires etc. would be extremely unintelligent not to fear it. On the other hand, pets like cats, dogs, horses or live stock animals can be trained to be around fire without showing any signs of fear. Because they are socialised to not fear fire, they do not fear it. In the same way, wild animals which are domesticated, like wild sheep, goats or mustang horses etc. have been trained to 'live with human fire'. Examples where wild animals have learned to not fear fire are grizzly bears who learned that humans provide a vast source of food. The ones which realised that the fires around human campers are typically well-controlled, stopped fearing them. Therefore, it is a matter of learning the 'gains' of fire which out of human control is typically extremely dangerous to wild animals as it may destroy their whole living environment. The aspect of not fearing fire, because you can reason its origin or create it on your own, requires an extremely high level of awareness and intelligence. You can read up that chimpanzees are capable of the first steps in this direction."}
{"doc_id": 209611, "author": "", "text": "Self preservation comes way before intelligence. In the wild a fire is always dangerous. A wild animal not afraid of a fire would quickly become a dead animal, with no chances of transmitting its genes along the generations."}
{"doc_id": 209612, "author": "", "text": "In terms of intelligence, the animal in question has to be intelligent enough to be socially conditionable. What this means that it has to be intelligent enough to be taught things."}
{"doc_id": 209614, "author": "", "text": "Nothing inherently fears Fire Only when you are burned (or feel the excessive heat) do you become conditioned to avoid it. Because we humans (tool makers) have figured that we can use fire to achieve a goal, we embrace it. Another psychological aspect most fauna employ, is not a fear but an aversion to an unknown element. Treating the unknown with caution is a natural survival mechanic. So in the movies when you see arctic wolves attacking survivors. They are hesitating because they've never seen fire before and don't know what it can do. And/Or, they got whacked in the face with a torch or hot branch and already learned that that sucks."}
{"doc_id": 200257, "author": "", "text": "They all deal with some form of desire and the need to fill some gaping emptiness in one's life. However the dividing point is based on what you want to do with the thing you desire once you have it. Greed You want something, and then you want to keep it. Forever. Greed deals with hoarding simply to have things and many of them. Gluttony You want something, but you want to consume it. All of it. Gluttony is about the desire to consume things, to the point that one's hunger is never truly satisfied. This differs from greed in that, once something is acquired, it is otherwise destroyed or consumed with no consideration for the toll on either the body, the environment, or other people who may not have what they need. Lust You want something, but you want it solely for the physical pleasure that it brings you; furthermore, the selfish pleasuring of yourself is all you can think about. The desire for pleasure is all-consuming. Whether or not you end up keeping it is irrelevant, you just want it to make you feel good. Often this is pointed at another human, turning him or her into an object of pleasure. Sometimes, this isn't the case. Regardless, the focus with lust is for the end goal of self-pleasure without regard for the person or thing in question."}
{"doc_id": 200259, "author": "", "text": "http://tvtropes.org/pmwiki/pmwiki.php/Main/SevenDeadlySins I'll be using that as a reference, but I'll also explain how they differ here. To quote that page, this is what each of those sins are defined as, Gluttony: Desire for Excess. In pop culture, this sin is almost always associated with overeating, which is a start, but theologically it applies to overconsumption of anything. Taking more than your share is a key part, as is wasting the excess. It has also been equated with any kind of addiction in modern times. The virtue to this vice is Temperance. Greed: Desire for Things. Often simply referred to as greed, but avarice includes spending money pointlessly as well as hoarding it. Greed is also very commonly associated with ambition. The virtue to this vice is Charity. Lust: Desire for Pleasure. It's the desire to know someone Biblically, but traditionally included all other sins of physical desire or luxury (such as drug addiction), not just sex. The virtue to this vice is Chastity. (Whose original definition did not mean \"abstinence\" but was closer to \"monogamy\".) ... With definitions and antonyms out of the way, let's compare and contrast. All of these sins represent a desire of some kind that will lead a person to death (if left unchecked, thus \"deadly\"). However, each of these represent very different desires. Gluttony differs from Greed in that Gluttony resolves around a personal behavior. Gluttony differs from Lust in that Gluttony focuses on \"more\". Greed differs from Gluttony in that Greed revolves around a social behavior. Greed differs from Lust in that Greed is selfish. Lust differs from Gluttony in that Lust focuses on pleasure. Lust differs from Greed in that Lust is shareable."}
{"doc_id": 181272, "author": "", "text": "If you allow the third kingdom (after plants and animals), namely fungi (mushrooms and their many, many cousins), then yes. Fungi will take care of the oxygen surplus, using it up and releasing CO2 for the plants to breathe. For pollenation: There are plants that use the wind for this, and other plants reproduce through non-sexual reproduction - strawberry offshoots, old willow trees breaking apart and new ones growing from the parts, root networks sprouting new aboveground plant parts, potatos and onions creating tubers or child-bulbs underground... Fertilization: See fungi. Plants die, Fungi (and bacteria) break apart the plant matter, rinse and repeat. Animals actually take care of only a relatively small part of the \"Plant matter to fertilizer and CO2\" conversion, with fungi already doing the bulk of the work. Without our contribution to pollination and spread of seeds etc., we could actually consider us animals superfluous for the ecology as a whole ;) Edit: Found a book (trilogy) I was thinking of when writing this answer - \"Of man and manta\", from Piers Anthony. Sci-Fi, features a planet with no animal life where fungi evolved into mobile, and IIRC sapient, beings."}
{"doc_id": 181275, "author": "", "text": "Many plant species shed branches and leaves as well as many which shrivel and whither away seasonally. Wild fires happen frequently, turning plant matter back into CO2. There are diseases which kill plants. if your planet had immortal plants it might be in trouble. As @Syndic said fungi do a great job of converting plants into CO2. The simple fact is that the mass of CO2 in the atmosphere far exceeds that the Earth's plant biomass. You can expect this will be the same on your planet. Although its atmosphere would have a higher oxygen composition. That helps wild fires too. Also, plants release CO2 at night when they're photosynthesizing. This helps maintain the balance. After all, night lasts almost as long as day. In essence, the CO2 won't be used up. There are enough mechanisms to convert plant matter back into CO2."}
{"doc_id": 181278, "author": "", "text": "Pop-culture bad science reporting says Humans and Bananas have about 50% shared genes. Not easily verifiable but some similarities must exist. Photosynthesis evolved about 3.5 Billion years ago. Multicellular organisms only evolved about 800 million years ago. About 450 Million years ago land plants appeared. 230 MYA dinosaurs. So you've got huge opportunities for evolutionary branches. Kingdom Plantae and the Evolutionary History of Plants would be worth investigating."}
{"doc_id": 181283, "author": "", "text": "Plants existed before animals ever evolved, and if all animals were to disappear, plants would continue to exist a million years from now. Just mostly different species. Respiration. Plants produce both CO2 and O2. Without animals, there would be a higher concentration of O2. If you wanted, you could easily evolve plants that have internal processes that take in environmental oxygen at a higher rate than they use CO2, and perhaps use it to increase mobility and growth. Otherwise, it's likely that bacteria would take up any slack, assuming you just didn't re-evolve animals (Oxygen is a great energy source, if it doesn't kill you first). Pollination. Insects and other animals are only one mechanism by which plants are pollinated. Wind dispersal is an older method, and one that most pine trees use effectively. The plants that cause most spring allergies rely on wind distribution of their pollen. Soil fertility. In a compost pile, worms are famous for doing the work of breaking waste vegetation into new soil. But this happens even without animals. Instead, soil bacteria and fungus do the job. I wouldn't even suggest that the process needs to slow down. Seed dispersal. There are lots of plants that rely on animals carrying their seeds, either as food (eaten or stored) or by burrs attached to fur, feathers, or skin. But there are many, many plants that rely on other strategies. Fluffy feathery seeds floating in the wind are common and maple trees with their helicopter seeds are two examples. Seeds in flood areas can use flooding both as a means of being carried away from their parent, and as a signal that it's time to germinate. More unusually, there are even plants whose seed pods explode (video), propelling the seeds many feet away. Competition and predation. With or without animals, plants need to defend against predation and competition. Dodder and mistletoe are both parasitic plants that get their energy by tapping into the sap of host plants. They don't usually kill the host, but they can certainly weaken it. Strangler figs are a species that germinate on a host tree, then as they grow, they wrap around and choke the host to death. Oak leaves (and others) contain tannins that poison the soil at the tree base, making it harder for other plants to grow there. It's a violent world out there. If you also eliminated fungi, many, many plants would struggle because they rely on fungus at their roots to increase their nutrient uptake. Without that, plants would probably be limited in size, and many existing species would die."}
{"doc_id": 181289, "author": "", "text": "The short answer is \"yes\", since plants came into existence some 3 billion years ago and were quite content until the Cambrian Explosion some 550MY ago. Longer answer is that plants will most likely continue to inhabit the oceans for the vast majority of the time until they produce enough oxygen to create an ozone screen and can move onto land. Plants can propagate themselves without animals or insects, the simplest solution being to allow pollen to drift into the ocean or the wind in enough quantities that the corresponding pistels will be fertilized. In your world, there might be enough pollen being released to seriously cloud the waters and fill the air, since that is the only way for plants to carry out sexual reproduction. Plants will also develop interesting ways to project seeds, ranging from dispersion through the air and water to developing \"catapults\" or other ways to throw seeds to needing fire to germinate (much like trees in the boreal forest). Other methods of reproduction seen in modern plants will also be developed, such as using rhizomes. Other ways to propagate may develop depending on the conditions of your world."}
{"doc_id": 181290, "author": "", "text": "Without kingdom animalia you would still have kingdom fungi as well as domains prokaryota and archeae. These are all perfectly capable of respiration (in fact plants themselves can do it too) and would expand to fill in the new niche of oxygen consumption. Fertilization is already done by microorganisms such as nitrogen-fixing bacteria so no problem there either. Pollination will be an issue. Those plants that are pollinated by animals would likely go extinct. The other plants which are pollinated by wind or water will take over their ecological roles. If animals were to vanish overnight, many ecosystems would undergo crises, but ultimately a rich flora (different though it may be from what we are accustomed to today) would persist on the planet."}
{"doc_id": 181321, "author": "", "text": "No one has addressed Fertilization yet; this is the topic I shall address. Fertilizer is composed of vitamins, minerals, and nitrogen. Vitamins can be produced by plants and minerals aren't ever destroyed, so those two aren't a problem. Manure, on the other hand, is composed of mostly nitrogen. Nitrogen is used to produce amino acids, which are used to produce protein - and you can't live without protein. Most plants can't produce Nitrogen, but certain legumes (like beans and stuff) can \"fix\" (fancy word for produce from) nitrogen from the air (remember, the air is about 78% nitrogen). Lightning and certain symbiotic bacteria can as well. And remember - nitrogen is never destroyed - it has to either decompose into the soil or into the air."}
{"doc_id": 181356, "author": "", "text": "Could plants survive without animals? Sure. Plants can respire, keeping oxygen and CO2 levels stable. Fertilization is largely based off of nitrogen fixation, which certain plants (legumes) can do. While many modern plants did evolve to utilize animals for pollination there are countless other solutions (airborne spores or seeds being the most obvious). However, evolution being what it is, there is a pretty good chance that in the absence of animals keeping their population down, plants would evolve to kill each other in order to acquire more resources, and many would learn to eat other plants. Many of these would probably lose the ability to photosynthesize and become something similar to fungi in our world."}
{"doc_id": 246020, "author": "", "text": "No Screens these days are sheets of LED lights so that might work as a solar panel but not as a camera. At best you might measure ambient light levels perhaps but no camera. That said TV these days are coming out with built in cameras and microphones and will get worse as time goes by. Things like Google Assistant and Alexa will come built into a lot of household devices."}
{"doc_id": 246025, "author": "", "text": "Even IF - Why would you want to take a photo with zero (or near few mm) field of vision and a lot of blur? Because TV screen would (could) act like a photosensitive paper (or if you are old enough camera film). There would be no lens to focus, no apertures to set the amount of light, no pinholes that would allow you to point to certain area. Screen would take light from everywhere that is not behind the TV. And what would be behind the TV would (could) still have ambient light. So a photo taken with such screen would be very bright (white) on border going to greyish in the middle. Just put a normal camera in the TV. Speakers can work as microphones anyway."}
{"doc_id": 246035, "author": "", "text": "A camera needs a lens to work. Screen has no lens in front of it so cannot be used as a camera. This is a single, important thing. The rest would not matter that much. A proper electronics can pick external brigthness reading from the LED no problem. If to put a tiny lens in front of each LED, it may work as a camera at the focus distance of this lens, given enough LEDs to compose a picture (\"insect eye\"). Some discrete (\"individual\") LEDs may have lenses in front of the light emitting diode, made on purpose or happening by chance. Hence low resolution banner has more potential for this kind of application. I do not think that IC leds have anything similar. An interesting thing for the story, a water droplet on the screen may act as a lens, suddenly allowing the part of screen under the droplet to \"see\". I am not sure about the focus distance however."}
{"doc_id": 246037, "author": "", "text": "Given enough time, a very low resolution image could be recovered by changing the displayed image such that the liquid crystal layer allows light in through only a single pixel (or a small group of pixels). This would correspond to a black screen with a small white spot. By moving this white spot around the black screen and measuring the voltage at the backlight LEDs, some amount of spacial detail could be recovered (depending on the physical dimensions of the screen and the depth of the liquid crystal layer). The size and thickness of the spot will cause light rays to be rejected by angle based on the spot position. The minimum transmission area will be limited by the total amount of light transmitted by the black pixels, so your resolution will effectively be limited by the screen's contrast ratio as the white spot must be large enough to transmit more light than the black pixels over most of the screen. This would only be suitable for taking still images due to the time required to capture one frame, but the necessary mask frames could be hidden between frames during the display of a normal video on the screen."}
{"doc_id": 246042, "author": "", "text": "Yes! ...But not a very good one. Cameras have some combination of mirrors and lenses to focus the incoming light into something recognizable, and without that, you're not going to see much. An LED will return the light level it can see; unfortunately, the light level an LED on the bottom left corner and the light level of an LED on the top right corner are going to be mostly the same, unless you are pressing your face against the screen. A television on the wall across the room won't be able to see anything useful. However, a simple fix can improve the view considerably! Modern televisions have the LEDs right at the screen, or behind little bubbles that spread the light out, so as to produce the maximum view-able angle. But, there exist privacy screens that essentially limit the view angle to a point directly in front of the screen, at a set distance. These screens work by blocking light from escaping in any direction except a very small angle. Beyond that, the screen will look black, because the light has been blocked. And lucky(?) for us, that effect works in reverse! The LEDs will only see the light in a narrow band, instead of the whole room at once; the image will still be fairly blurry, but as long as someone is sitting in the \"sweet spot\" to view the monitor, the monitor will be able to see them, too! Even without that, your monitor will need a lot of custom hardware to read the pixels; it's a lot easier and cheaper just to squeeze a tiny, fully functioning camera into the TV's frame - say, right next to the infrared sensor?"}
{"doc_id": 246043, "author": "", "text": "Yes, with interferometry based synthetic aperture imagery, but... It would take several orders of magnitude improvement in digital signal processing technology for it to work at the scale of a video screen. The sensors will need to record at a very high data rate, on the order of petahertz. Every single pixel will be receiving all of the light that is in its field of view, and none of the light will be focused. Fortunately, light isn't just photons, it's also electromagnetic waves. The sensors will behave like antennas, rather than eyes, passively collecting the radio waves at the wavelength of visible light. Radio signal sources can be tracked quite easily, by measuring the timing of a wave reaching multiple antennas. The same applies to all sorts of waves; an earthquake's epicenter is calculated by measuring the timing when the waves arrive at seismometers. We even have the same types of sensors built into our own human bodies. : Our ears can taking a wave and determine the direction that it came from, based on changes of intensity and timing. We have already proven the concept using vast antenna arrays to collect unfocused photons, to create a focused image. We imaged the supermassive black hole M87* using the Event Horizon Telescope. Of course, when astronomers use the term \"photon\" they don't just mean visible light; they mean any coherent electromagnetic wave. This image represents the peak of current engineering feasibility for synthetic aperture imagery. The EHT uses a Very Long Baseline Interferometry, which works in 450 GHz, using very narrowly calibrated equipment designed specifically to tease out the glow of the accretion disk at the wavelengths to detect event horizons around black holes. In order to get meaningful data, though, your sample rate needs to be at least twice the frequency of the signal rate, preferably more than 4 times the signal rate, or you start getting downsampling errors called aliasing. In order to record visible light, which has frequencies between 405 THz to 790 THz, you will need a sample rate that is at least 1.58 PHz. Due to limitations from the speed of light, and the time that it takes electrons to pass through silicon and copper in computers, this is just past the fastest speed that we can record data meaningfully. We would have to pass the data from several sensors in order to build up a meaningful synthetic aperture image from interferometry. We would need specialized recording technology that we just don't have yet. And, there's also the problem that LEDs aren't designed to collect light, even though they're capable of doing it... just as sound coming out of a microphone would sound terrible, and sound recorded from speakers is also low quality. It would take several generations of iterating on the current science in order to use an LED-based computer monitor to record what's happening it a room, and it will always require specialized systems to just record the data in a meaningful way, much less process it into an image. It took several petabytes of data and 3 years of processing in order to build the EHT's image of M87*. It was faster to hand-carry the hard drives from the telescopes around the world to the datacenter, than it would have been to send the data over high speed internet links. It would take a lot of iterative work to miniaturize the chips necessary to do the calculations, but the technology just barely exists. Such a screen would be prohibitively expensive, as it's much easier to just put a lense in front of a cluster of photodiodes (i.e., a webcam) and hide that in the corner of the screen, but those leave physical evidence... you can see the lense if you take the screen apart. It could be possible, with non-digital interferometry, to construct such an image of the room in real time... but that equipment barely fits in the basements of large telescope observatories, and requires cryogenic cooling. You wouldn't be able to collect it clandestinely, and would be much cheaper (and higher quality) to use a webcam."}
{"doc_id": 246059, "author": "", "text": "Somewhat. What is marketed as \"LED TV\" is LCD with a LED backlight. There are very few elements and they're placed behind (or to the sides of) a diffuser, so they won't be able to sense anything more than the room's general illumination level. Some high-end LCD TVs do have a full-array backlight with enough elements to potentially make out a bit more information than just general illumination. These are generally even more rare. OLED TVs have individual pixel elements, and they already have some means for recording the aging of individual elements as it occurs. Still, any image you get would have very little fidelity due to the lack of any means of focusing the light. You wouldn't be getting a video. But, if every current from the backlight was recorded, it's probably enough data to forensically distinguish between the patterns of incoming light when someone is in the room watching the TV, and when they're not. After all, isn't that something the dystopian society would care about? Of course, current TVs could already be recording you with good resolution by simply replacing the IR sensor, used for the remote, with a pinhole IR camera. It would keep working as an IR sensor, so no one would know the difference unless they disassembled the TV and closely inspected the components, specifically looking for a surveillance device."}
{"doc_id": 246118, "author": "", "text": "Yes, Samsung produced the SUR40 Surface Table for Microsoft. The panel contained imaging sensors used to produce a picture of anything placed on the table. Note that without a lens of any sort the range of images it can take is very limited. Also it suffers terribly in certain lighting condition especially involving overhead flourescent lights. This was from 2011 https://www.engadget.com/2011/01/06/microsoft-and-samsung-unveil-sur40-the-surface-2-0-experience/"}
{"doc_id": 10317243, "author": "", "text": "You aren't alone - far too many people get Belief and Knowledge confused. A Truth is something grounded in reality - demonstrable either directly or via sound rational progress from direct evidence. Any concept one considers to be true (which is not the same as a Truth) is a Belief. When that concept is a Truth, then that Belief is Knowledge."}
{"doc_id": 10317244, "author": "", "text": "Broadly speaking, knowledge is objective truth while belief is subjective truth. That is, knowledge is typically thought to be that which is true independent of circumstance; it is universally true (non-contingent). Belief, however, is an idea or concept which is held as true to the individual who holds it, and not necessarily to anyone (or everyone) else. It is, however, entangled with many other ideas and notions in philosophy and as such there is no simple definition that will wholly suffice in answering your question. See Epistemology {SEP}{Wiki}, PhilosophyOnline's article on knowledge and belief, Analysis of Knowledge from SEP as Joseph also points out, and (philosophical) Hermeneutics."}
{"doc_id": 10317249, "author": "", "text": "Suppose I flip a coin and don't look at it. I have no knowledge that the coin landed heads up. But I may choose to believe it landed heads up if I want to. Interpreting your diagram: Agnostic atheist: \"I do not believe that a god exists. God might exist or might not, I don't know. Perhaps evidence might make me believe in the future, but right now, I don't.\" Agnostic theist: \"I choose to believe that a god exists by faith. I wouldn't consider this knowledge though, as I don't have rigorous evidence or proof.\" Gnostic theist: \"I know that a god exists. I have proof/evidence that I consider rigorous.\" Gnostic atheist: \"I know that no god exists. I have proof/evidence that I consider rigorous.\""}
{"doc_id": 10317250, "author": "", "text": "Knowledge, of the kind you're asking about, I think, usually requires evidence and reasoning. In extreme cases where such knowledge doesn't require both evidence and reasoning, such as in parts of symbolic logic, knowledge requires only reasoning. On the other hand, belief doesn't require any reasoning or evidence whatsoever. If I know that the sun burns at, or around, a certain temperature, then either there exists some perceptual data as evidence of this, or some perceptual data exists which, along with reasoning, implies the sun as burning at, or around, that certain temperature. So, a claim of the sun burning at, or around, that certain temperature comes as sufficiently grounded. On the other hand, if one believes the sun burns at a certain temperature, there might not exist any evidence or reasoning which grounds such a claim. One could believe something in one's sleep quite easily. Unless you believe dreams provide us with empirical information about the sun, I think this indicates beliefs as not needing evidence or reasoning. This isn't to say that no beliefs can get grounded via reasoning or evidence. Plenty of knowledge, also is believed (I know I have a hand, and I believe it too). However, no belief purely as a belief need get grounded via reasoning or evidence to qualify as a belief. Knowledge does need at least some sort of ground, and if a claim is not grounded via reasoning or evidence, then it comes as a strongly believed speculation at best. Unfortunately I don't have any \"atheist\" or \"agnostic\" literature citations here, but as I recall reading \"atheist\" and \"agnostic\" literature they do seem to use the terms at least somewhat in that way. The Wikipedia on \"Descriptive Knowledge\" says this: \"The difference between knowledge and beliefs is as follows:. A belief is an internal thought or memory which exists in one's mind. Most people accept that for a belief to be knowledge it must be, at least, true and justified.\""}
{"doc_id": 10317252, "author": "", "text": "Strictly speaking I believe definitive knowledge is never obtainable, as Karl Popper has convincingly argued. Simply put; Karl Popper argued that there can always arise occasions where that, that which we hold to be confirmed knowledge (truth), will be falsified by a new observation. In other words; what we accept as being knowledge is actually merely belief with a certain degree of perceived certainty. I say perceived certainty, as Popper argued that it holds no actual certainty value at all; it can merely be perceived as propositions that have consecutively been corroborated by evidence. But as stated before: only one observation that contradicts such a proposition, believed to be knowledge, could be enough to falsify it. Therefor, I think we'd be wiser to classify different gradations of belief (and disbelief for that matter) on imaginary scales: Irrational belief1--|--|--|--|--|--|--|--Rational belief2 Irrational disbelief3--|--|--|--|--|--|--|--Rational disbelief4 1) Belief despite the lack of corroborating evidence 2) Belief due to overwhelming corroborating evidence 3) Disbelief despite overwhelming corroborating evidence 4) Disbelief due to the lack of corroborating evidence Knowledge then, I think, should be considered that part of the first scale that leans towards the right end of the scale (rational belief), while keeping in mind that this knowledge is never definitive. edit: Perhaps this image, somewhat in line with your image, better demonstrates what I mean:"}
{"doc_id": 10317253, "author": "", "text": "Knowledge is a particular kind of belief, one that has (or has more) evidence, and justified at that (of course there is the classic Gettier problem with this definition). The picture you gave shows two axes, one from theism to atheism (the subject matter about what one knows/believes), and an orthogonal one or gnosis to agnosis, or what I take it, to be the degree of belief with gnosis being knowledge (certain belief) and agnosis being... Well, that's the problem. What is that axis 'measuring'? Is it the certainty (which would presumably go from 'sure' knowledge to ...unsure. Is unsure knowledge the same as belief? I think of knowledge as one kind of belief, a very sure kind of belief, rather than in opposition to belief. For the diagram, I'd say that the a/gnosis axis is really trying to quantify 'certainty'. At one end one is -very- sure of one's belief that a god exists (or doesn't). At the other, one is completely unsure of the statement. My problem with this diagram is that is seems perverse to say 'I believe that X, but I am completely unsure of X'. Those seem contradictory. If you are completely unsure of X, then I would say you can't believe it. I guess one could be a theist and be unsure about it, but if one were -completely- unsure of it, then that wold just be an agnostic, directly in the middle, rather than being an agnostic theist. The meta-lesson that I learn from this diagram is that a nice clean diagram does not necessarily exhibit coherent or consistent concepts. Is the 'gnostic/agnostic' axis about a continuum between proof and faith? That might be a more orthogonal and coherent thing, but there's no evidence in the picture that that is the case."}
{"doc_id": 10318166, "author": "", "text": "Knowledge What is knowledge? Knowledge could be a part of particular truth or universal truth. When we are aware of knowledge from other source, we consider it as valuable facts that may be adapted for our own purpose relevantly. We consider knowledge as facts that have possibilities to be useful for us. Failure on Knowledge If such knowledge unable to be implemented successfully to support our purposes, this will not be our knowledge and we disbelieve it, but still there is a chance it will be valuable for someone else, and we can share it to someone else. Knowledge may be a particular truth, therefore knowledge may be shareable. Belief What is belief? Belief must be considered as a part of a universal truth. Belief is our assertion to knowledge. Belief is knowledge as universal truth that we accept. We accept a knowledge as a belief and we share a belief as a knowledge to someone else. Failure on Belief Since belief is a part of universal truth, therefore if our belief was proved to be wrong, it shouldn't be shared as knowledge to someone else. The points are: Knowledge is what possibly useful facts for us, Where, a belief is an assertion of usefulness of a knowledge. May be a knowledge is someone's belief, but what i believe for sure is a knowledge (knowledge is not always a belief, but a belief is always a knowledge)"}
{"doc_id": 10321036, "author": "", "text": "Knowledge is useful or explanatory information. An item of knowledge need not be believed by anybody. For example, there is lots of knowledge in books, computer programs and even genes that nobody knows. The information is just as valuable and just as much in need of explanation as knowledge that happens to be believed by somebody. See \"Objective Knowledge\" and \"Realism and the Aim of Science\" by Karl Popper for more on this issue."}
{"doc_id": 10321040, "author": "", "text": "Knowledge is based on evidence whereas belief does not need any evidence. I think the \"Knowledge hierarchy\" is interesting in this case: So let's get through it: Data is only symbols / signs. Data comes from sensors. A simple example is the output stream a visual sensor produces. This sensor might be your eye and the data comes in the form of electrical impulses. Information is data with context and interpretation. In the eye example that could be some structure: Your brain knows that the data it gets is grouped. What arrives at the same time is related; things that are closer together are related. From the different signals of single rod cells an image is formed. So information can only exist with data, but it is more. I would define knowledge as an extrapolation of information. So you try find patterns in information with context. In the eye example I would say that knowledge is the following: You see the following image: Source: https://commons.wikimedia.org/wiki/File:Puppy.JPG But without knowledge that has no further meaning. But you have seen that pattern before. And you have seen the pattern what happens next before: People say sweet You feel happy You're tempted to +1 this post ( :-) ) You got this knowledge by combining lots and lots of information: You have seen the pattern \"fur\", \"eyes\", \"mouth\" before You have seen the combination before In your first years you were told many times \"that's a dog\" so you derived that \"dog\" is what these patterns mean. And as you know these patterns, you have compresses a lot of information and even more data: You can predict what can happen and what can't. Your predictions might not be always right, but they have to be right most of the time. How do you know they are right? Well, that's another question. One questions that I think is interesting is: Is knowledge always correct? Well, I think here you will get problems with terms. I would call one of them universal knowledge and the other personal knowledge. The universal knowledge does fit to every information all the time. In that sense it cannot be wrong. But it is rather a theoretical construct. We only have personal knowledge as we only have limited data and therefore limited information. As our capacities to detect patterns are limited we also accept errors. So our knowledge has not to fit all information we get. We sometimes simply ignore information (Was that fish just talking? Nah, I have never seen that before. Let's ignore it.) or we actively try to get more information (Did the fish talk again? Probably somebody tried to fool me. Lets seek for hidden cameras.) Humans have developed amazing abilities to get and share knowledge. A good strategy for checking if knowledge is useful / valid is falsification. For everything you know, there has to be some information that had the possibility to change your mind: \"If XY happens, then my knowledge about Z is wrong.\". So much about knowledge. But what is belief? Belief does not need any data / information / knowledge. When you ask religious people what you could do that would make them not belief in god any longer, you will get one answer: Nothing. No data / information can \"remove\" belief. Colloquial meaning: I belief Sometimes people say I belief when they are not sure about something. But that's something different."}
{"doc_id": 10334073, "author": "", "text": "tl;dr - deconstruction is something specific (usually from Derrida less commonly from Heidegger). Post-structuralism is a near synonym for late 20th century French philosophy and is a type of \"post-modernism.\" Post-modernism is a term which means anything after modernity -- no idea what it means without context. Postmodernism is a grab bag term that applies to many different things that come ... after modernism. It's hard to know what someone means when they say this term as it gets bantered about (usually by people who are opposed to something they call \"postmodernism\" or by people who thinks it's the best thing ever). One reason it's hard is that what is modernism is not as easy to answer as it appears. In general, in philosophy, modernism refers to the period from Descartes and Locke down to the period of Kant ... and maybe Hegel, and further maybe Kierkegaard, and Nietzsche (depending on the speaker and their interpretations of these philosophers). Post-structuralism is one of the things that comes after modernism. The name \"post-structuralism\" gives it a firmer meaning than \"post-modernism\" and locates it within a French tradition. It's in many ways quite similar to the project of structuralism. Structuralism is a project that says meaning exists in systems (\"structures\") not in sentences or individuals. Post-structuralists keep the structures but often drop out the idea that there are meanings beneath this that could be found or could exist. It's hard to articulate quite what they mean by this (because part of their point is to attack both \"they\" and \"mean\"), but the basic idea is that our naive concept of things where speakers are subjects that have wills, intentions, thoughts, and values is wrong and what's actually happening is that ideas move in their own force. For this reason, it's not really clear if the people called post-structuralists (e.g., Foucault, Lacan, Butler, Kristeva, Derrida, and others) are engaged in a fundamentally different project or just an extension of the original structuralist agenda. This is because they accept the main point of the structuralists -- that the structures are primary; they reject or possibly amplify the structuralists diminishing of the subject. (How exactly and what exactly will vary depending on who we are talking about). Deconstruction is a term for a method that appears originally (not necessarily meaning \"first\" but meaning as the origin of the method) in Heidegger that refers to showing how the concepts we have doesn't work the way we often think they do (see this answer). For Heidegger, Destruktion (unlike it's English counterpart destruction) means both a tearing down of the old and a building up of the new (SEP; n.b. my German is not good enough to judge whether Heidegger's usage of the term is normal or singular). Derrida is probably most famous for this both as a method and as a name for a type of post-structuralism. Derrida also uses the term Bricolage for the same basic idea with the meaning that when we tear down concepts we think are clear, we will discover they are not so clear. The wikipedia articles in English and German are helpful here. One thing that muddles things even more is that the definitions I'm giving you above are some light off-the-cuff definitions coming from philosophy. Post-modernism also refers to a movement in art and architecture that has some ideological overlap. And the usage of these terms in literary criticism is non-identical (note I didn't say completely different)."}
{"doc_id": 10340391, "author": "", "text": "In her essay on Post-Modernism, Linda Hutcheon states that \"Poststructuralist discourse paradoxically contests, yet unavoidably inscribes, the very preconceptions it seeks to challenge.\" in her A poetics of postmodernism(55) From what I gathered in literary theory, and correct me if I'm wrong Post-structuralism in its attempt to deconstruct structure, it falls into the trap of being structural itself as it took the other position For Hutcheon, Post-Modernism affirms both, challenges both and revels in its own contradictions answering question by questions as she adds to correct Lyotard's definition of postmodernism \"Let us inscribe and then challenge totality; let us (re)present the un(re)presentable; let us activate differences and admit that we thus create the honor of the name and the name itself.”"}
{"doc_id": 10320768, "author": "", "text": "Kant raises a distinction between what he calls perfect duties and imperfect duties in the Groundwork of the Metaphysics of Morals and again in the Metaphysics of Morals: Doctrine of Virtue. You have the basic definition in hand: a perfect duty is one which one must always do and an imperfect duty is a duty which one must not ignore but admits of multiple means of fulfillment. Kant specifies two imperfect duties: the duty of self-improvement and the duty to aid others. To understand why Kant thinks of these as imperfect duties, we need to first understand the nature of duty for Kant. The literature on this is vast, so I'm going to skip over some parts of the mechanics and summarize it as follows: a duty is something that we are obligated to by the Categorical Imperative. In other words, it is something that that we can see as a universal rule for all of humanity necessary for a morally just society (mixing together all three major types of formulations of the Categorical Imperative). It's difficult to come up with completely non-problematic duties due to some issues related to the basis on which we act \"maxims\" and the means through which we universalize these, but I will skip over this for the purposes of this question (If interested, I suggest reading Allen Wood's Kant's Ethical Thought). Let's say that I want to lie to someone. If we universalize this, then every rational creature will lie whenever it is convenient. This will turn out to be self-defeating because no one will believe what anyone says. Since we have a constant need of truth in our dealings, this is something we must practice at all times. (i.e., we cannot add an exception \"except when telling the truth is inconvenient). This makes this a perfect duty in the Kantian system. Most perfect duties turn out to be negative duties -- i.e. don't do X. On Kant's system, every rational being is obligated by perfect duties. Imperfect duties reflect the nature of human rational existence. We are born weak and frail, we cannot do everything by ourselves, and we die. These realities create interesting non-rational features of our reality: I needed someone to feed me when I was a baby. I need someone to help me when my car is stuck. I need a surgeon when my liver fails. These needs are not universal either in time or duration nor are they purely rational laws. To make these desires moral, Kant needs us to universalize them. Thus, we transform I need help at times into every [limited] rational creature has a duty to help other rational creatures at times. Thus, I have a requirement to aid others at times reflective of my own need for help at other times. This is one of the two imperfect duties for Kant. The second imperfect duty is to perfect myself. This duty arises because when I need help, I need experts. Thus, the only way that rational creatures can have their needs met is if rational creatures are developing their talents. So, I too have a need to develop my talents in order to create a universalizable rule that would make it so aid is available when I need it of sufficient ability. Moved to the level of the particular, imperfect duties are things like: study chemistry, practice the violin, learn Japanese, volunteer at an orphanage. These are duties I don't need constantly and that I somewhat pick from among. Thus, they are imperfect duties since they are not constant obligations, but they remain obligations. What Kant does not answer is how often. He puts this question somewhat in the Doctrine of Virtue. You can read more about this in Creating a Necessity out of Virtue by Nancy Sherman. I also gave a more thorough argument for this account in my dissertation."}
{"doc_id": 10331793, "author": "", "text": "I would suggest that religion is an application of philosophy to a given faith or history. And science itself is at root the religion that arises from a faith in naturalism. We do not disown theologies in philosophy, within their own cultural constraints they are valid philosophy. And all of our very early philosophies were very much associated directly to given quasi-religious notions. We consider Platonism a philosophy, but beefed up just a little into Neo-Platonism, it is a religion. Was some specific addition between these two somehow the straw that broke the camel's back? No, Platonism is already a sort of personal religion. It just arose out of a single man's head, rather than a cultural tradition. And it is oddly compelling in a way that makes it easy to clip some corners and render it \"consistent enough\" with many other religions to make it worth keeping around. Everything that happens does so in some historical tradition, with some basic set of ultimately un-analyzed assumptions. So in essence systematic philosophy is, as a whole, a collection of ad hoc theologies. The constraints that separate where one or the other theology applies do not involve the whole religion, just its central principles or the observed 'facts' of its interpreted history. All the rest of any given religion is a collection of theologies, or involves more basic philosophy like logic, ethical analysis and ontological exploration applied within the frame of some theology of the religion. Even the raw experience of the religion in mysticism or personal investment is really an aesthetic engagement attached to a theology that shapes its details. The amalgam of all Hindu theology is Hinduism. The amalgam of all Christian theology is Christianity. The basic facts cannot be analyzed philosophically, and make up the 'core faith' of the religion, which sets it apart from other religions. But even the process of analyzing and isolating that core is a philosophical endeavor. Not all Christians or Hindus take the same axiomatic base to their overall religion, and not all of those with disjoint core notions would exclude others from the 'big tent' of Christianity or Hinduism. So theologies can be tied together in complex ways and they can overlap or include large parts of one another. Science is something that we seem to be able to share across most religions. But that is because all religions have to enclose a philosophy consistent with some contact with natural reality. If the religion just consistently disowns reality (like raw Buddhism), it can get along, but variants of it will arise that do not do so, and those who stick to the anti-realist standards will unconsciously adopt one of those variants to get through life, even while considering it ultimately incorrect. But at root, there is no reason to have faith in our experience of nature. We just do, because we are animals and animals are natural beings. So the philosophical explorations that arise from that root faith are not in essence different that those of a 'real' religion. They are just much more likely to be consistent with some pocket in each other faith."}
{"doc_id": 10331800, "author": "", "text": "The distinctions can be made in virtue of how much dogma and empirical evidence are involved, but philosophers have pointed out that the lines of demarcation are rather blurry, like the distinction between bald and not bald. Philosophy involves very little dogma and very little empirical evidence; it is the art of rational conjecture. Philosophers seldom agree with each other because common premises are few and far between. In the realm of opinions freedom should be absolute because the opinion doomed by one philosopher may be perfectly acceptable by another philosopher. Bertrand Russell preferred a quarrelsome atmosphere in philosophy and refused to play the authoritative role at the height of his career; he was generous to his attackers and lavished compliments to anyone who demonstrated some understanding of his philosophy. Science relies heavily on empirical evidence. Nevertheless philosophers have pointed out that there are hidden faiths in scientific knowledge; the empiricist creed that all knowledge is derived from sense-experience is itself a dogma. Because of these all-agreed-upon common criteria, scientific methods are widely accepted in scientific community, and peer review is a reliable procedure to ensure the quality of scientific work. Hidden faiths in empirical knowledge do not license other groundless faiths; empiricism too is not immune to doubt - this is what faith implies. Both philosophy and science are highly tentative, subject to revision based on new evidence. Religious pillars are dogmas. Paradoxically and by the same standard, scepticism can also be called a religion. The dogma that is fundamental to scepticism is this: It is undesirable to believe a proposition when there is no ground whatever for supposing it true. (Russell. On the Value of Scepticism) Bertrand Russell was the kind of philosopher who sifted through systems of beliefs and pointed out what were implicitly assumed a priori, then went on to reduce the number of postulates to bare minimum. This pattern of thinking recurred in his Critical Exposition of Philosophy of Leibniz, his mathematical philosophy and his theory of knowledge."}
{"doc_id": 10331810, "author": "", "text": "Science is a systematic method of increasing knowledge involving testable hypotheses and replicable methods. Philosophy is sometimes known as the \"mother of sciences,\" it considers questions outside the reach of currently accepted reliable scientific methods. Not all philosophies do involve the rational exploration of truth, which would better be described as a philosophical approach, rather than the philosophical approach. Philosophies can be highly ordered and elaborated, but they are not generally verifiable in an objective fashion (this does not mean they are worthless or wrong). Successful philosophical consideration of a realm of inquiry can produce a science (for example, physics, astronomy, logic, psychology), which is why the founding figures of various sciences were often considered philosophers in their day. Religion is a set of rituals, practices, structures and beliefs, typically connected with a theology, which is a philosophy about God (although there are some non-theistic religions). If theology was a science, religion would be a technology."}
{"doc_id": 10320052, "author": "", "text": "As a starting point, I can tell you that, generally: - the term \"objective\" refers to things which we deem as true/existing independent of our observations of them, and - \"subjective\" refers to a things which we deem as true/existing contingent on our observation of them. The cake and apples example in the other user's answer below is an example of this, although it fails to take into account the fact that our count of the apples is itself subjective (maybe there's 100's of apples in a big bowl and we miss counting one). True, the actual amount of apples on the plate is an objective fact (by 'fact' here I simply mean a 'piece of knowledge that I hold as true'), and if we were able to calculate this without error then sure, the example works. But I think a better example is the idea that my mind (myself) exists — this is more readily and more universally agreed to be an objective fact. Under no circumstances can we conceive of a situation where the thinking self (\"I\") does not exist, for if we are thinking we must necessarily exist. Whether this thinking self is \"smart\", however, is subjective — to an ant I may be considered smart but to a super-intelligent alien species I may be considered as unintelligent as the ant is to me. My smartness thus is a subjective fact. The problem however is that even what we might call as objective facts we arrived at only through the processing of our own minds, which are inherently subjective. This is the problem with the apples example, as well as for mine. Objectivity for us on a human-level, it seems, is not actually intrinsic objectivity (which we can't seem to know) but rather a form of collective subjectivity. Just because everyone agrees on the \"truthness\" of a fact doesn't mean that it's automatically an objective fact. As you can see these concepts quickly become complicated, which is why I originally wrote that this question bordering on \"too broad to be reasonably answered\". Subjectivity and objectivity mean many things to different philosophers depending on their particular views on a wide range of concepts. A good, solid answer in my opinion would really touch on all the major lines of thought that deal with it, but that's perhaps too much work for a single question. There are many important concepts that come into play, pretty much everything in philosophy of mind, but specifically concepts such as: - reality - phenomenology - perception (see also Quale) - substance theory (see also materialism/physicalism, dualism) - consciousness (see also mind-body problem) - object/properties (see also bundle theory, universals, noumena) Note, I linked only Wikipedia articles above. Do also check out the equivalent articles at the Stanford Encyclopedia of Philosophy."}
{"doc_id": 10320061, "author": "", "text": "These terms might be used in different contexts with different meanings, still, there is a core meaning for them as visualised below: (and as it is said in the question itself)"}
{"doc_id": 10320707, "author": "", "text": "(new account, not enough rep to comment reply to Michael's follow up question) There are four apples and therefore the objectively correct answer is four. The third observer is wrong to claim there are 5 apples when there are four. Of course, it might be that there really are 5 apples and that the third person is correct and the first two wrong. Another possibility is that they are all wrong and there are six apples. Regardless, this is a situation where there is one correct answer. The subjective question is about personal taste. Since it is about taste there is no conflict inherent in having different answers to the question. It is a known property of humans that they differ in their tastes. The question \"Is the cake yummy\" really should not be taken literally that way. The question is actually reinterpreted by people to me \"Do YOU find the cake yummy\". Which when asked of different people will generate multiple objective questions, one for each person. In this case lets call the girl Sue and the boy Joe. The subjective question, \"Is it yummy\" converts into two objective questions, \"Is it yummy for Sue\" and \"Is it yummy for Joe\". The respective answers are yes and no. If Joe were to turn to sue and say \"No it is not yummy\" then he could be making one of two mistakes. He is either claiming that Sue thinks the cake is not yummy. In which case he is wrong. Or, he thinks that Sue was claiming the cake was yummy for everyone and was disputing that because he doesn't like it. Depending on what Sue meant he could be right or wrong, but only about his interpretation of what Sue was saying. After all if Sue meant it was yummy for everyone she is objectively wrong. Another distinction that can be made is whether something is metaphysically or epistomologically subjective or objective. Metaphysically objective things are things we call can see. For example the apples. Things that exist in the real world independent of any one person. Metaphysically subjective things are that which only each person experiences and can verify against each other. Exactly how Joe experiences the taste of the cake is not something Sue can experience nor directly verify. This is usually referred to as qualia. This exists in the real world but only to a single person. Hallucinations are metaphysically subjective and NOT metaphysically objective. That is they exist subjectively but not out in the real world. Metaphysics is about what exists. Certainly hallucinations exist, and the objects of the hallucinations don't exist or they wouldn't be hallucinations. When speaking epistomologically we are talking about truth claims. Those truth claims that can be decided by metaphysically objective facts are epistomologically objective. Those claims that can be decided by metaphysically subjective facts are epistomologically subjective. The statement \"There are four apples\" is something all parties can determing by counting. The statement \"This cake is yummy\" is something that is determined subjectively by tasting it. I could tell if my cat finds foods yummy by whether she eats them. That's an objective standard for measuring a subjective experience."}
{"doc_id": 10320708, "author": "", "text": "Objective- an unchanging truth not dependent on perception. A universal truth. Subjective- how you understand something to be. It is your interpretation."}
{"doc_id": 10329976, "author": "", "text": "The terms are somewhat ambiguous as to whether the sense of their use is ontological or epistemological. Professor John R. Searle explains as much in his paper, \"Consciousness\" Here is the ambiguity: We need to distinguish two different senses of the objective-subjective distinction. In one sense, the epistemic sense (“epistemic” here means having to do with knowledge), science is indeed objective. Scientists seek truths that are equally accessible to any competent observer and that are independent of the feelings and attitudes of the experimenters in question. An example of an epistemically objective claim would be \"Bill Clinton weighs 210 pounds\". An example of an epistemically subjective claim would be \"Bill Clinton is a good president\". The first is objective because its truth or falsity is settleable in a way that is independent of the feelings and attitudes of the investigators. The second is subjective because it is not so settleable. But there is another sense of the objective-subjective distinction, and that is the ontological sense (“ontological” here means having to do with existence). Some entities, such as pains, tickles, and itches, have a subjective mode of existence, in the sense that they exist only as experienced by a conscious subject. Others, such as mountains, molecules and tectonic plates have an objective mode of existence, in the sense that their existence does not depend on any consciousness. As for objective and subjective \"value\" roughly yes, but this also depends upon how \"value\" is being used. For example, \"the old worn sock puppet may not have been worth it's weight in cotton, but its sentimental value to the child was beyond measure\" is a subjective value; \"the arithmetic expression evaluates such that x has a value of seventeen\" is an objective value. In the second example you may want to also look into distinguishing observer-independent and observer-relative (see section 1) as the computation of the arithmetic expression as done by a (non-human) computer is observer-relative, tho when the arithmetic expression is evaluated by a human, it is observer independent. For example, a map my show the directions from where you are to where you want to go - from point A to B. The \"information\" (also an ambiguous term) on the map is relative to an observer that can interpret it. The map does not know how to go anywhere. This is distinct from you actually knowing how to get from where you are to where you want to go - the information \"in your head\" is not relative to an observer, the conscious thought is psychologically real, actual and observer-independent. From \"Theory of Mind & Darwin’s Legacy\" by Searle: Related to the distinction between objectivity and subjectivity is the distinction between those features of the world whose existence depends on human attitudes and those features that exist independently of anyone's attitudes. I call the former “observer relative” and the latter “observer independent” or “absolute.” Observer relative phenomena include money, property, marriage, nation states, universities and summer vacations. Observer independent phenomena include mountains, molecules, galaxies and tectonic plates. In general the natural sciences deal with observer independent phenomena; the social sciences deal with observer relative phenomena. The observer relativity of a phenomenon introduces an element of ontological subjectivity into its very existence. So the existence of money and language, for example, is observer relative and consequently contains an element of ontological subjectivity."}
{"doc_id": 10319925, "author": "", "text": "The quote about facts gets it pretty right. A fact is, for many philosophers, a part of reality (Russel, for example). So as there are people and tables and chairs in our world, there is also the fact that I am sitting on the chair. It is as real as the chair itself. You often see some kind of brackets when someone speaks about fact, so for example: < I am sitting on a chair> converts to \"The fact that I am sitting on a chair\". Truth is a property of sentences, propositions, utterances, whatever you like. Facts can therefore not be true, in the same way as a chair cannot be true. Stating a fact, however, and depending on your opinion, has a truthvalue. I think the second quote about truth is a bit problematic. It sounds as if good arguments alter reality. But arguments cannot be true, they can be valid, and they can be truthconserving. So if I have an argument for the existence of god, it is at best valid. That does not mean, however, that suddenly, in virtue of the good argument, god came into existence. Edit: More on truth So on one common view those things that can be true are propositions. So a meaningful exression would be: The proposition that snow is white is true. If you believe that sentences are the things that can be true, then this would be an example: The sentence \"Grass is green\" is true. Most people believe that facts cannot be true: They think that \"(The fact that grass is green) is true\" is a weird thing to say. (I use brackets to make clear that the predicate \"is true\" refers to the fact. Because otherwise there could be a second reading about the (fact that grass is green is true), if there is such a fact) To conclude:(i) There is the fact that grass is green, and (ii) the proposition that grass is green is true. Also it is worth pointing out that there are philosophers who say that there are no facts, because facts are weird ontological things and maybe you can do without them. So this is just one way to answer this question."}
{"doc_id": 10320098, "author": "", "text": "A fact is a perception of reality. A truth is a perception which matches reality. There is a nice parallelism with [axiomatic] formal systems: An axiom is a building block for possible worlds. A theorem is a statement about certain possible worlds. Ok, that didn't turn out quite as well as I hoped. I was trying to establish the analogy: fact : truth :: theorem : axiom There is a weird asymmetry: fact ↔ theorem truth ↔ axiom At least, I expected it to work the other way around. The convention in this thread has facts being possibly wrong; we are much more used to axioms being possibly wrong, for we only call something a 'theorem' if it has been logically demonstrated to flow from the axioms. Then again, if we are trying to approximate the world with a formal system, we are essentially searching for axioms that generate theorems which match the facts. In pictogram format: (fact) observation < —— > theorem ∧ ∧ | | ∨ ∨ truth axiom Excepting tautologies, truths are unknowable except by approximation; we must remember that science models reality, but it does not say what reality is. Models are made up of axioms & theorems. To the extent that our theorems match our observations ('the facts'), we think that our axioms are [close to] truths. P.S. The word 'reality' in this answer can be replaced with 'possible world'; what is true in a fictional world may not be true in our actual world. Assuming there is an objective reality, of course. :-)"}
{"doc_id": 10334977, "author": "", "text": "I want to make some general points about the OP. Firstly, you appear to be asking for how the words truth and fact are used, but you capitalize these words. That already tends to obfuscate the issue, suggesting there is some very special, possibly metaphysical, usage you are alluding to. Secondly, in asking for the meaning of individual words, you are suggesting that the unit of meaning is a single word. This is not true, as any cursory look in a dictionary will demonstrate. There are multiple entries for both truth and fact, not in the the least because the meaning of the words is modified by their context, and that therefore truth and fact can have multiple meanings in different contexts. Now, it happens to be the case that one such dictionary entry for truth is \"conforming to the facts\" and for fact \"a particular truth known\". This is from the Oxford Dictionary, but I assume any dictionary would have similar definitions. This only goes to show that in one important sense truth and fact are interchangeable."}
{"doc_id": 10323753, "author": "", "text": "Remember Kirk and Spock. Spock is the man of logic. But Kirk is the man of reason. Kirk always figures out the right thing to do in any situation; even when Spock raises an eyebrow and says, \"But Captain, that's illogical!\" Reason is not the same thing as logic. As is so often the case, Wikipedia supplies valuable context. Reason is the capacity for consciously making sense of things, applying logic, establishing and verifying facts, and changing or justifying practices, institutions, and beliefs based on new or existing information. As we see, logic is one set of tools we apply after doing a lot of other homework ... not only establishing facts, but also putting them in the context of our own preferences and desires. If logic was all that existed, commerce could not exist. A seller holds a good, a buyer holds currency. By \"logic,\" one is greater than the other, and no trade can occur. But each using their own reason, the seller values the money higher than the good; and the buyer holds the reverse values. They trade and they are both happy. If pure logic prevailed, this could not happen. One party would have to lose and the other win. But preferences are subjective; even though logic is objective. Thus reason encompasses more than logic. One could give many examples. There's a story reported in the newspaper. One person believes the story, another doesn't. They then take rational actions based on their information and beliefs; but they each take different actions. Two machines given the same programming will produce the same output for a given input. Two humans often do something different given the same inputs. Reason is much more powerful than logic. If there is insufficient information, or contradictory information, logic is powerless. But reason can always find a sensible course of action. As I noted in my comment above: We use reason to make rational choices in situations where logic fails. Isn't that pretty much the common everyday experience of everyone? You go to get your morning coffee. Is coffee bad for you? Maybe. It's a mildly addictive stimulant. Is it good for you? Maybe. Some think it forestalls diabetes. It's full of antioxidants. Are those good for you? I guess so, I saw it on the Internet. But what about the beans? Are they fair traded? Was someone unfairly exploited in the production of those beans? How much do I care? How much can I know, even if I spent all my time and energy trying to find out? And what about the milk? Whole or 2% or none? Are the cows being mistreated or are they happy? Pros and cons to every choice. Sugar or artificial or none? Does your brain lock up in an infinite spiral of confusion? For some people whose rationality is compromised, that's exactly what happens. For normal people, we are aware of all that information but we put it in its proper perspective: things we can't do anything right now. We have our coffee. Or not, if we prefer not. Either way we're using our rationality to cut through a mass of facts and rumors that can not possibly be analyzed to their last detail. Where is logic? If a perfectly logical being existed, it could not function. It couldn't even get a cup of coffee in the morning. But you and I function just fine. Our reason allows us to put all that overwhelming and conflicting information in proper context ... which is that in the scheme of things, it's all not very important. Unless the individual chooses to make those things important. Maybe you refuse to drink beans that aren't organic and fair-traded. Maybe I order a cup and don't think about it much. We're both being rational. Neither of us is being logical. Logic is hopeless in deciding whether and how to have your morning coffee. Only reason can save us. Logic's useless. Reason lets you ignore all the inputs that simply don't matter. Reason is what lets you function in a world of conflicting information, where logic fails."}
{"doc_id": 10340816, "author": "", "text": "So many ways to get after it: Logic is deductive/Reason is inductive Logic is formal/Reason is common sense Logic is systematic/Reason normative Logic is form/Reason is function. And depending on your metaphysical leanings, they may be the same!"}
{"doc_id": 10340817, "author": "", "text": "Logic regards necessary relations among abstractions. Reason regards whether they are good abstractions."}
{"doc_id": 10340821, "author": "", "text": "Logic & reason Gil Harman has worked at this distinction and his views, usefully summarised below, provide at least some materials for reflection. Difference 1 - reasoning, logic and belief revision Reasoning is a procedure for revising one's beliefs, for changing one's view, for modifying one's intentions and plans. This must not be confused with argument for, or proof of, a conclusion, which is a matter of proceeding from premises via a series of intermediate steps. In fact Harman thinks that there is a category difference between reasoning (reasoned change in view) and argument or proof. His point is that a rule of argument is a principle of implication which tells us that propositions of a certain sort imply other propositions of a certain sort, but tells us nothing about belief-revision, so that 'rules of argument are not by themselves rules for revising one's view' (Harman, 1986, p. 3). The rules of logic are permissive rules licensing the deduction of propositions. By applying such rules, we can generate more and more logical consequences from an initial set. Now, in reasoning, Harman points out, we not only add to our beliefs but sometimes subtract from the beliefs held in store and this, he claims, illustrates one vital difference between logical proof and reasoning. Another difference, he suggests, is that, unlike logical principles, principles of belief revision are defeasible - they don't hold in all instances. Consider, for example the following principle of belief-revision: Tf one believes p and also believes if p then q then one can infer q. Unlike modus ponens, this principle does not always hold. Harman constructs a counter instance: Mary believes that if she looks in the cupboard, she will see a box of Cheerios. She comes to believe that she is looking in the cupboard and that she does not see a box of Cheerios. At this point, Mary's beliefs are jointly inconsistent and therefore imply any proposition whatsoever. This does not authorize Mary to infer any proposition what? soever. Nor does Mary infer whatever she might wish to infer. Instead she abandons her first belief, concluding that it is false after all. (Harman, 1986, p. 5) In other words, if a logical consequence of some of the propositions we believe is absurd or otherwise unacceptable, then the thing to do is to revise or relinquish some of our beliefs, rather than to accept that logical consequence. Difference 2 - logic and the infinity of conclusions Again, principles of logic permit the drawing of an infinite number of conclusions from a set of premises. The premises p and q imply p & q, (p & p) & q, (p v q) & q & q and so on. But it can hardly be a principle of reasoning that we should accept (i.e., explicitly embrace) a vast number of useless consequences of our beliefs, for that would be to clutter the mind with useless trivialities (Harman, 1986, pp. 5-6, 12-13, 41-42). Harman makes much of this point, and advocates a Principle of Clutter Avoidance which is 'a metaprinciple that con? strains the actual principles of revision. The principles of revision must be such that they discourage a person from cluttering up either long-term memory or short-term processing capacities with trivialities' (Harman, 1986, p. 15). For Harman, to have an explicit belief is to have a representation - some sentence-like entity - in the brain (Harman, 1973, 1977, 1986, pp. 12-14, 32) so clutter-avoidance is a serious matter of preventing the brain bursting. Difference 3 - logic, reasoning, and logical inconsistency Finally, Harman objects to the idea that the relevance of logic to reasoning is captured by the suggestion that, in reasoning, one seeks to avoid logical inconsistency. Harman invites us to observe that we sometimes discover that we hold inconsistent beliefs but, unable to find a satisfactory way of revising them, we retain them 'while trying not to exploit the inconsistency'. (Wittgenstein once took exactly this line, but later changed his view.) Harman also points to situations where one believes that not all one's beliefs could be true. He comments that here one might well be justified in continuing to believe that and each of one's other beliefs as well (Harman, 1986, pp. 15-16). Likewise, according to Harman, the proper, rational, response to semantical paradoxes such as the Liar is to retain the Biconditional Truth Schema '\"p\" is true if and only if p' as a kind of 'default assumption', while recognizing that, when paradoxical sentences are substituted for 'p' in the Schema, contradiction ensues. So the Schema holds 'normally', but not without exception. Harman comments: 'This does not seem to be a satisfactory solution from the point of view of logic, since we take logic to require precise principles with precise boundaries, not principles that hold merely 'normally' or 'other things being equal'. But in ordinary life we accept many principles of this vaguer sort (Harman, 1986, pp. 16-17). Harman concedes that we do have the disposition to avoid embracing propositions we see to be inconsistent, but it is not just logical inconsistency that we are so disposed to avoid. For example, we don't accept both 'X is y's brother' and 'X is female' nor both 'Today is Thursday' and 'Tomorrow is Saturday' (Harman, 1986, pp. 17-19). So there is no special relevance of logic. (Laurence Goldstein, 'Logic and Reasoning', Erkenntnis (1975-), Vol. 28, No. 3 (May, 1988), pp. 297-320: 297-9. References Harman, G.: 1973, Thought, Princeton University Press, Princeton, New Jersey. Harman, G.: 1977, 'How to use Propositions', American Philosophical Quarterly 14, 173-76. Harman, G.: 1979, '\"If\" and modus Ponens', Theory and Decision 11, 41-53. Harman, G.: 1982, 'Logic, Reasoning and Logical Form' in T. W. Simon and R. Scholes (eds.), Language, Mind and Brain, Erlbaum, New York. Harman, G.: 1984, 'Logic and Reasoning', Synthese 60, 107-27. Harman, G: 1986, Change in View, MIT Press, Cambridge, Massachusetts. ,"}
{"doc_id": 10333130, "author": "", "text": "In a classic theory of knowledge, there are typically 3 conditions that must be fulfilled in order for something to be considered \"known\": Belief - pretty self-explanatory, you believe that what you think is true. Example: I believe an elephant has grey skin. Justification - you have a reason to think you belief is true. Example: I heard someone say elephant has grey skin. Truth - an after-the-fact kind of justification, you can say an objective true statement about your belief. Example: you saw an elephant, and you saw it actually has grey skin, so you can say objectively \"elephant has grey skin\". Now keep in mind that every theory of knowledge build upon these 3 conditions and expand (because, as noted over the years, they are not enough - in the example, who says I didn't saw just a specific kind of elephant with grey skin, but there are actually more elephants with pink skin? Here comes Hume's famous induction problem, and that's just an example for the problem with keeping only those 3 conditions as they're presented here)."}
{"doc_id": 10333133, "author": "", "text": "These words are often used very loosely in philosophy and cause much trouble. The reason is the common idea that ''truth' is the same as 'belief' where the belief is 'justified'. But a justified belief is a tentative belief and not a known truth, and if a truth is not known then we cannot call it a truth. Descartes saw this problem and chose his axiom accordingly. A proper answer would be quite long and technical, but for now I'd just keep it simple. A belief is what we believe. A belief may be justified or unjustified depending on the strength of the evidence for it. If the evidence is strong then it may be considered a justified belief but cannot be considered a truth. A truth is something we know to be true such that we require no further evidence or justification and could not be wrong. Much trouble comes from equating justified belief with truth, as in the phrase 'justified true belief'. I would ban this phrase from philosophical discourse as an unhelpful muddle of words, but regrettably I'm not in charge. ."}
{"doc_id": 10323006, "author": "", "text": "To argue that the ethics and morality of Taoism are similar to the Christian is a rather simplistic argument. All the great religions teach similar ethics and morality. Taoism is a non-dual tradition, very different from the monotheistic tradition of Christianity. David Loy writes in Nonduality: A Study of Comparative Philosophy: \"The first section [of Chapter 3] argues that the Taoist paradox of wei-wu-wei (the action of non-action) is a description of such nondual action. It is highly significant that the same paradox is found in the other two nondualist traditions, clearly enunciated in the Bhagavad-gita and more fully developed in the Buddhist account of the Bodhisattva's path.\" Western academics consistently try to show non-Christian religions as either 'pre-Christian' in their ethics and morality, derived from Christian influences, or not up to snuff with Christian ethics and morality. There is a good book on the distortions that are done by Western academics. The book is called Invading the Sacred: An Analysis of Hinduism Studies in America. THe book is available as a free download here - http://rajivmalhotra.com/books/invading-sacred/"}
{"doc_id": 10323017, "author": "", "text": "Speaking from personal experience as a Christian with Taoist sympathies, although it's far from a mainstream line of thought in either Christian or Taoist circles, the cause of reconciling the two is well known, and has been taken up by a number of different thinkers. Part of the motivation may come from the fact that Taoism is largely non-theistic, which makes it possible to incorporate aspects of Taoist philosophy into Christian practice (as well as many other different religious traditions), without the burden of also taking on incompatible claims about God or gods (compare Hinduism or Shintoism). Taoism also functions largely without any central prophet or Messianic figure whose authority could be considered by Christians as trespassing on that belonging to Jesus (compare Buddhism or Islam). There is also the fact that the Tao translates most naturally as \"The Way,\" a term which, in the Christian tradition, is claimed by Jesus (\"I am the Way, the Truth and the Life\"). This fact may seem coincidental, but has been taken as significant by some writers."}
{"doc_id": 10326428, "author": "", "text": "Taoism teaches that a way of living exists that is compatible with health and happiness. There is a great emphasis on not explicating the way, but to use intution and experience to impliment or activate it. Christianity does the same thing but can point to a incarnation of the right way -- the life of Jesus."}
{"doc_id": 10327920, "author": "", "text": "Taoism is quite different from christianity, specially in morality and ethics (i.e. considering what is right and what is wrong), since: In Taoism every concept appears immediately with their opposite. In christianity, there is God, and be apart from God. Most of the concepts in the Bible arise from the idea of being apart from God. An example of this is the first two chapters of the Book Of Wisdom in the bible, when referring to the people with no God. Have this quote: Thus being and non-being produce each other, and try telling a christian that the Devil creates God (the opposite is true). I can teach you how to block punches. You will need it. In Taoism, as stated in the very first \"chapters\" in Tao Te Ching, Tao cannot be defined (this includes: there is not intrinsic ethics arising from the main concept). Tao morality looks then no more than a guideline (subjective note: which for me and other taoists I know, is a GOOD thing; with this I remark that this is not a despective comment about the practice, but considered as a feature). In Christianity, God is Good, Benevolent, Fair, bla bla bla. Opposite concepts arise from being apart from God. In Taoism, there's the paradoxical concept of \"no action\". In Christianity, there's a constant referenct to acting, good deeds, God's plan or predestination (depending on schools). In Taoism, in particular chapter 38, having hope, believing, and waiting for the future is stupid (depending on the translation, the word stupidity, ignorance or confusion is used; [...] Abides in the real, and does not dwell on the flower [...]). In Christianity, a huge part of the belief is about the future rewards and punishments (Book of Wisdom chaps 1-5, and almost whole book of Apocalipsis). Most of the rewards are somehow related to power or rank (over nations, over angels, ...). Edit: The flower concept is a reference to the promise, the future. Meanwhile, Christianity (and mostly every abrahamic school, say) is deeply based (and tied to) the concept of prophecy. There's a high honor to be considered a prophet (and gained the gift of prophecy). In Taoism, you die and you are part again of Tao. In fact, you never stop being apart from Tao in a \"literal\" sense. In Christianity, you can fall apart from God. You have no commandment in TTK to attend a Taoist temple, or consider any Taoist church as a sacred organisation. You have no commandment to pray, but you are encouraged (or informally recommended from practitioners) to learn from the whole bunch of metaphors. Reality is one in Christianity. Reality is not fixed according to Chuang Tze metaphors of dreaming about being a butterfly. For this point I have no concrete source (others have their source in TTK and Bible as I stated), but experience: Christians often reject non-contemplative or non-God-based meditation, including activities like Qi Gong and Taiji Quan. Here you have a source. This is always the main taoism source (it is a must-read), regardless other authors. They can have common points, but as a practitioner I can tell it is quite different in essence."}
{"doc_id": 10331688, "author": "", "text": "\"Why does everyone love the Tao so much when they first find it? Isn't it because you find what you seek, and know that your sins are forgiven?\" When I first ran across this bit from the TTC, as penned by my teacher, Gia-fu Feng in his beautiful translation (the first available in the West that was done by a native Chinese speaker), I was quite stunned by the similarity to Christian thought. You could just swap out \"Jesus\" for the word \"Tao\" there, and someone who didn't know any better might think that was a quote from the New Testament. I was particularly struck, of course, by the \"and know that your sins are forgiven\" bit. I asked Master Feng about this, saying, \"I wasn't aware that the concept of sin existed in Taoism\", to which he wittily replied, \"Sure, we do bad things sometimes, too.\" :) Ultimately, Master Feng and studying Taoism actually helped me with a number of issues that ultimately led me to becoming a Christian. For example, I was one of those people who was always a bit rankled by what I saw as God's arbitrary rules. Feng helped me to see things such as the Ten Commandments not as rules but simply as instructions for living life in harmony with Tao. In other words, one should see things such as the commandment, \"Do not commit adultery\", as merely informing you that adultery is not in harmony with the Way of Life - if you're committing adultery (or stealing or killing), then you're swimming upstream against the flow of Tao. Just a thought (or two). Jack Maverick"}
{"doc_id": 10325068, "author": "", "text": "Like you I see more similarities than dissimilarities between empiricism and logical positivism. In a broad sense, empiricism also covers logical positivism. In a narrow sense both differ by the historical time and the opponents of each school of thinking - as Mauro explains in his comment. A main topic of logical positivism from 20th century was the question, how to separate science from metaphysics. In the end, it was Karl Popper who against logical positivism detected, that the difference is the possibility of falsification - not verification. This insight of Popper is considered by many as a breakthrough in epistemology. In any case, also empiricism must concede that knowledge cannot be deduced from experience alone. Science get's it life from the interplay of experience and theory. And theory often uses concepts which are freely created by man, e.g., Hilbert space for quantum mechanics, Riemann manifold for the General Theory of Relativity, gene for Biology."}
{"doc_id": 10325087, "author": "", "text": "somehow on contrary there is much difference ,sometimes even on the sense which these two are used. (\"logical\") positivism (so called \"logical empiricism\" or \"scientific empiricism\"), a movement associated with Vienna Circle. Their mission was to 'unite' the science (especially as opposed to metaphysics) and give a correct sketch,description of scientific method(to dispute metaphysics). Their task concern both scientific theory and language.This movement can be seen as empiricism (which states for example that they are in agreement with the methodology of natural science, and in that they believe that the source and origin of all knowledge is experience) and with a very important role for the (formal) logic \"to describe the structure of permissible inference\"(linguistic and logic).They are in agreement with empiricist that origin of all knowledge is experience but they also seek for a \"logic of science\" with a very unique \"authority\", which in it's turn leads to conformation theory. To be direct, positivism states that 1)the only authentic knowledge is scientific knowledge[a uniqueness statement](but empiricism is about source and origin of knowledge, which states that origin of all knowledge is sense experience, and of course scientific knowledge is included). 2) this authentic knowledge come from a special and unique method(scientific method)\"\" that is empiricism + specific principles of logic\". It asserts something about authentic knowledge and the method of achieving this authentic knowledge, empiricism is about ,what is the basis of all our knowledge, it's emphasizes on experience and evidence(especially sense perception) to acquire knowledge, to use Kant, the \"only knowledge we can have\" is a posteriori( based on experience)."}
{"doc_id": 10325442, "author": "", "text": "Empiricism is the thesis that knowledge comes from experience. Logical positivism is the thesis that the meaning of a sentence is the set of conditions under which that sentence could be verified to be true. (This implies that any sentence which cannot be verified empirically is either meaningless nonsense, or a tautology.) All logical positivists are empiricists, but not all empiricists are logical positivists. For instance, Aristotle would agree that all knowledge comes from sensation. (To my knowledge, he is actually the first to make this claim.) However, Aristotle might well reject that a sentence is meaningful even if there is no way for us to tell whether it's true or not. (I can't think of any examples of sentences that Aristotle would take to be like that, but I also can't think of any reason he would have to deny this either.)"}
{"doc_id": 10329661, "author": "", "text": "From the Stanford Encyclopaedia of Philosophy entry on Logical Empiricism: The term ‘logical empiricism’ has no very precise boundaries and still less that distinguishes it from ‘logical positivism’. It is therefore hard to map. [...] It does not, however, distinguish logical empiricism from logical positivism, and it is doubtful that any principled such boundary can be drawn along doctrinal or sociological lines. The most that one can say is that if a distinction is to be drawn, logical empiricism is the wider term."}
{"doc_id": 10339311, "author": "", "text": "From \"Introduction\" of The Cambridge Companion to Logical Empiricism: Throughout this book \"logical empiricism\" is understood to be synonymous with \"logical positivism,\" or even \"neopositivism,\" unless it is clear in context that a distinction is being drawn. Some logical empiricists thought the names had different reference, but most did not; in any case, by the middle of the 1930s, \"logical empiricism\" was the preferred term for leading representatives of both camps. Thus, we have chosen it rather than the more well-known but more misleading \"logical positivism.\""}
{"doc_id": 10320968, "author": "", "text": "Generally speaking, no (in either school), especially when the God is question is some kind of being. As a Westerner its very difficult to not see the Tao through a Western lens especially when translators translate it as God in an attempt to make it more accessible. Generally speaking, it is translated as \"way\" not because the translators are being overly literal, but because \"way\" is a really good word for what it describes. I think it's probably easiest if we go with the English word \"way\" as a guide to the meaning of Tao. Not because it is exactly correct, but because it's close enough for our purposes. Is God a \"way\" in any reasonable sense. Not really. The Christian religion might be called this (in fact it is: the way of Christ). But the Christian God Himself is not usually thought of in this manner (Let's assume John 14:6 means something more like \"I can show you the way\"). SEP says no also, I've highlighted the most important bit - the quote is from a long article which is probably worth reading if you are interested in the topic: Dao [...] was the center of Chinese philosophical discussion. It occupies the position at the center of thought that in Western philosophy is filled by terms like ‘being’ or ‘truth’. The centrality tempts interpreters to identify dao with the central concepts of the Western philosophical agenda, but that is to lose the important difference between the two traditions. Metaphysics and epistemology dominated early Western philosophy while ethics, politics and philosophy of education/psychology dominated Chinese thought. Although it's insightful to say humans live in dao as fish do in water, the insight is lost if we simply treat dao as being or some pantheistic spiritual realm. Dao remains essentially a concept of guidance, a prescriptive or normative term. In the late Classical period, dao paired with devirtuosity to form the Chinese term for ‘ethics’ “dao-de.” Dao is the pivot of Chinese philosophy—but it still translates as ‘way’, not ‘being’."}
{"doc_id": 10320969, "author": "", "text": "Assigning a meaning to the term 道 (dao or tao [both spellings are acceptable due to an oddity in the differences between the T and D used by English speakers and the appropriate consonant in contemporary 普通話 (pǔtōnghuà) = Mandarin) is extremely difficult. There are several reasons for this. First and foremost is that the meaning of 道 (dao) is part of the contested matter in early Chinese thought. In other words, the main schools are arguing about what [道] (dao) is. Why? At the simplest level, the character refers to a road or a path or a way. It also has other meanings such as to speak or to be the doctrine of a school of thought. The question of whether the 道(dao) is a deity or something like that will vary depending on both the school of thought, the thinker, and the meaning of deity. I will start with the last question and work through the others depending on our definition. If we define a deity as a sentient being with particular personal thoughts, then I don't think any classical Chinese school of thought would have viewed the 道 as a deity. If we define deity as the source of all power and the guiding force of reality, then I think that both Confucians and Taoists would believe this of the 道(dao) and it could be considered a deity. If we add to this definition a belief that this guiding force is guiding towards a particular goal, then we will still have the Confucians but not the Taoists as believing in a deity. This is because for Confucians the 道(dao) is guiding towards a well-ordered society in which the relationships are in harmony under the true sage king who has the mandate (命[ming]) of 天(tian). (More about 天[tian] later). The Daoists are lost, because they don't see the world as having a productive trajectory. While the overlap is not perfect, there's definitely some anti-government thought going on in Taoism and the belief the harmony of the 道(dao) is already there and we just need to tap into it. Thus, one of the more famous parts of the 莊子(Zhuangzi) is a consideration of a butcher who is in contact with the 道(dao) and thus does not loose the sharpness of his knives because he cuts everything perfectly without thinking (無為 [wuwei]). So then, this could also be seen as a type of deity or at least a \"spiritual energy\" that a self can be in contact with. Perhaps to qualify things a bit, the Daoists are not necessarily \"lost\", but the 道 gets you in touch with nature 然, but it's not at all clear nature has a direction or goal. If anything, many passages suggest either an endless cycle of recurring or some sort of joyous vibrant energetic chaos. To simplify with a chart Op: Omnipotent Pe: Personified GP: Guiding Principle PS: Possessing Spirit On: Omniscient Bn: Benevolent | Op | Pe | GP | PS | On | Bn Christian God | ○ | ○ | ○ | ○ | ○ | ○ (omni?) Greek gods | × | ○ | × | △ | × | × Dao in Daoism | △ | × | ○ | ○ | × | △ [not consciously] Dao in Confucianism | △ | × | ○ | × | × | △ [not consciously] Hegel's God | × |△ 1 | ○ 2| ○ | △3 | △4 1: God=us, 2: as Nessesity, 3: Eventually?, 4: if we are The term also refers to schools that are built around trying to understand the \"way\" of something. Thus, 武士道 (bushido) = the way of the samurai [yes, its Japanese but the origin is the same]. So then in short the 道(dao) is a god to some but not to others. Definitely as Taoists mean it, it is no the Christian God, but they might want to have mystical encounters with it in the religious forms. As promised, I will mention 天(tian). Part of why 道(dao) is not referring to a personal god is that 天(tian) might in fact be doing so. The history is long and complicated, but it looks like one very early Chinese dynasty believed in 天(tian) as their personal god. This people then got conquered by another dynasty who chose to usurp the authority of the first group. Thus, instead of just wiping out their gods, they actually chose to promote this deity and make it the \"heavens\" or a kind of fate like entity. Whether any scrap of belief in its personality remains is a matter of contest, but the majority of scholars think that by the time of Confucius, it was no longer conceived of as a personal deity (the minority view seems to be held primarily by religious thinkers like Kelly James Clark -- who is a Western trained philosopher rather than expert in sinology)."}
{"doc_id": 10320971, "author": "", "text": "I ask myself: what is the difference between \"Primordial essence or fundamental nature of the universe\" and \"nature\" itself? None that I can think of. So I'm translating Dao as Nature. The gods usually represent the different forces/effects of Nature, so they are pieces of Nature. Dao De Jing 4 says that Dao \"looks older than the gods\"."}
{"doc_id": 10317405, "author": "", "text": "The terms are not synonyms. You should note that religion always refers to a specific set of beliefs, i.e. Christianity, Islam, Buddhism, etc. The term philosophy can be used in two senses: In the general sense it refers to: the rational investigation of the truths and principles of being, knowledge, or conduct. (source) In a narrower sense, you can use the word to refer to a specific set of philosophical theories, such as stoic philosophy or Kantian philosophy. It would be these narrower subsets of philosophy which might be considered parallel to a particular set of religious beliefs. With that cleared up... How do you make the distinction between philosophy and religion? Answer: philosophy in general is the rational investigation of truth, whereas religion often makes the same kind of truth claims but doesn't claim to base it on reason or rationality, but instead it is based on other things like faith. The key difference is that they are different epistemological positions — philosophy has a system of logical principles in place to arrive at conclusions whereas many religions (such as Christianity) allow for other sources of knowledge (i.e. faith). Are there some philosophies/religions that are hard to categorize as being one or the other? Answer: In principle, it could be a problem to categorize them but in practice it is not very difficult. Why? Because virtually all religions with even a modest following carry with it a set of traditions and rituals which philosophies do not. Put simply, religions have practices and philosophies do not. Theoretically, you could have what might be considered a religion without any practices, but it's not very common."}
{"doc_id": 10317408, "author": "", "text": "Taking the latter part first: yes, there are definitely religions/philosophies which have proven difficult to categorize. The government of Australia, if I understand correctly, is currently trying to decide if Buddhism qualifies as a religion; the government of the US has decided that Scientology counts as a religion for tax purposes, while the government of Germany has decided the opposite. In Norway, the second-largest \"faith/life-view\" organization (after the state Lutheran church) is the Humanist-Ethical Society (i.e., a group of atheists) which qualifies as a \"religion\" for many purposes (including tax status.) The fact that this confusion exists in practice shows that there isn't any clear, reliable indicator one can point to. The obvious candidates (such as the belief in a deity, or a soteriological path, or the presence of rituals or dogma) break down when you actually try to apply them in practice: exceptions abound. So, that being the case, I would turn the question around: why does it matter? What benefit would be gained by drawing such a distinction? If we could answer that, perhaps we'd have a clearer way to finding an appropriate criterion."}
{"doc_id": 10317421, "author": "", "text": "Thomas Aquinas makes the distinction that philosophy is based on human reason alone, but religion also includes some kind of divine revelation."}
{"doc_id": 10317428, "author": "", "text": "One of my favorite statements of Wittgenstein's is this. \"At the foundation of well-founded belief lies belief that is not founded\" (§253, On Certainty). So, you can believe whatever you can believe. –An experience. –A scientific belief. See: it is of the system in which we claim knowledge of this or that. (I believe this or that statement, but I know my beliefs.) If this were not the case, we'd never know anything. (For me), philosophy is the \"system builder.\" (And Charles Sanders Peirce would seem to agree, as he called science inquiry, and the system that puts everything together philosophical rather than scientific.) But, this makes sense. Think of the scientific use of the word \"energy.\" Physics, chemistry, biology, geology all have their own form of energy, but it is not science that connects them (well, it's actually language), but it seems it is the job of philosophy to systematize. For example, we call physics and chemistry sciences, and yet, they fundamentally see different aspects in the same thing. There is no atom in chemistry. –Only molecules. The differences between the points of views of chemistry and physics really helps show the discontinuity of the modern sciences. It is a wonder of the world to listen to a particle physicist discuss atomic theory with a chemist. What connects the seemingly infinite divide between our scientific facts is an holistic view (one that philosophy provides). Now the reason I brought up science as opposed to religion is that religion also can fit into this holistic view. That is, it can sit beside chemistry, perhaps between chemistry and biology, or physics and biology. (And there is no contradiction.) There isn't a contradiction because philosophy is what gives us the whole. That is, philosophy lets us put incommensurable systems under a whole system. I say philosophy, but really it could be your parents, or teachers, or friends who glue different points of view onto the same whole. In that, I would say that philosophy uses religion, as it uses chemistry or physics."}
{"doc_id": 10318903, "author": "", "text": "I think the two are difficult to disentangle easily. For example: Buddhism is often taken to be more philosophical rather than a religion, but I think this comes from viewing religion from a Christian perspective. Also Byran Magee (British philosopher) remarks in his autobiography that Kants philosophical thought can be seen as a rationalisation of the pietist tradition he was brought up in. Acharya Sen, an expert on indian medieval literature and hinduism points out that hinduism includes the lokyata/carvaka tradition which was empirical & scientific. He also writes that rational inquiry is an investigation into the impersonal side of Brahman. Pythagoras is said to have declared the universe is number and geometry in mystical reverie, this is still current today in the popular imagination, for example the Higgs boson referred to as the 'God' particle, and physicists and mathematicians seen as the high priests of a technocratic world (look at how they are referred to within science fiction). David Wallace an american novelist writes that human beings are worshipful beings, they have no choice in this - what they do get is the choice of what to worship."}
{"doc_id": 10329041, "author": "", "text": "I think it's a great question because it's quite clear, but I think many of the answers fall for a problem in the way the question is posed. I take it the question is meant to be along the lines of What's the difference between a fish and a giraffe? In other words, there's two categories of things and something can at most belong to one of the categories. Unfortunately, I don't think that's what we have here. I think we have something more like: Is a vegetable a root vegetable or sweet? First off, both religion and philosophy are terms that have had a large number of differing definitions. Looking just at the philosophy side, the epicureans lived together in communities, had parties, and prostelytized. Similarly, there were groups like the pythagoreans, etc. Moving past the Greeks and Romans, there was the medieval arrangement where philosophy is the hand maiden of theology. And then later philosophy is a name for natural science, etc. etc. (Further, what happens when the term philosophy refers not just to the Western inheritors of the Greek tradition but to ideas from India, China, Africa?). Religion, too is a term that has rather varied definitions (as several answers highlight). This clouds things significantly and makes the question harder to answer. So I think it's better to see there as being four possibilities: Some view is neither a religion nor a philosophy Some view is a religion but not a philosophy Some view is a philosophy but not a religion Some view is both philosophy and religion Let's say we define a religion as something that has ritual communal practices and moral views. Let's say we define a philosophy as something which attempts to give a fundamental account of metaphysics and epistemology. On such a reading, it would seem to me some forms of Christianity (say catholicism) are 4. Some forms would be 2 (say faith healing pentecostalism). Some forms of philosophy would also be kind of religious (see for instance the Jesuit difficulties in understanding the Confucian practices they saw in China). Moreover, the groups that do \"church\" for atheists or even just have regular discussion sessions in a bar on the weekends about philosophy (but with a commitment to a singular viewpoint or to pure inquiry) might qualify as being both religion and philosophy on this definition. I suspect any resistance to this would be predicated on differing definition of \"religion\" -- which probably for some people means more like \"believing idiocy\" than having ritual practices."}
{"doc_id": 10329625, "author": "", "text": "Religion is philosophy but philosophy is not religion; rather, it could be a form of religion. As the most general explanation, religion is about everything in relation to one cause and one end while philosophy is about different things in relation to no cause and different ends. Another way of stating that, intended to bring its meaning out clearly is, the end of religion is Truth, also known as Beatific Vision, Heaven, Paradise, etc. The relation between man and this end in man's quest for it through prayers, rituals, beliefs, etc, is known as religion. Philosophy on the other hand has many means and NO one end. The relation between man and each of the ends in man's quest for them could be religious but since means may differ and no common end exists for all, as in religion, philosophy is not religion. It is noted, as correction of the definition of philosophy as \"rational search for truth\"---based on which it is argued that religion, which it is said is based on faith is not philosophy---that the definition of philosophy as \"rational search for truth\" is the definition for epistemology and not philosophy. But if philosophy is taken as rational search for truth, St. Augustine's rational search for truth---Trinity---is a particular example of how or why religion is philosophy."}
{"doc_id": 10329648, "author": "", "text": "What is the difference between philosophy and religion? A general account of religion is community, ritual and reverence. Philosophy simply is reverence for obtaining knowledge. This translation from the Greek has stood unchanged for 2500+ years. How do you make the distinction between philosophy and religion? Same way you make any distinction: analysis. For example, as I have described them, note that in this case both involve \"reverence\". Religion, however, merely requires reverence (i.e. a sense of virtue or respect). Philosophy is reverence for something specific: wisdom. Are there some philosophies/religions that are hard to categorize as being one or the other? There is philosophy, not philosophies. There is love (in the sense of initial utterance, read: virtue, respect, reverence) and there is wisdom (read: obtaining knowledge) and there are prepositions to join them (i.e. \"of\" and \"for\"). Perhaps you mean something like, \"according to my philosophy...\" or \"in their philosophy...\" or \"some philosophies suggest...\"? This is using philosophy as misnomer for a way of looking at things. Unlike \"theology\" and \"theism\" philosophy is not an \"-ology\" (a study of...) nor an \"-ism\" (a belief in...). Note that religion requires neither theology, nor theism, I merely bring this up to clarify the distinction between what you can categorize as \"religion\" and what is \"philosophy\". If a religion encourages reverence for obtaining knowledge, it may be said accurately that it is philosophical, but do not confuse this description of religion or the religious with the virtue of obtaining knowledge (read: philosophy). Hope that helps!"}
{"doc_id": 10327648, "author": "", "text": "I do not see a difference between utility and extrinsic value. In particular your example concerning the extrinsic value of money shows, that money is useful to obtain a certain goal. You ask how can emotions be useful? The list of emotions comprises fear, anxiety, pleasure, joy, contempt, disgust, curiosity, hope, disappointment, expectation, exaltation, depressiveness. Emotions are flexible (emotional conditioning): There is no fixed attachment which links a certain experience with a set of emotions. E.g., some people experience pleasure about a certain movie while others feel disappointment. From a biological point of view, one can ask for the function of linking experiences with emotions, i.e. for the utility of emotions. One answer is that emotions serve as a marker of stored experience. As a consequence, in a new situation we have quick access to our stored experience by activation of the corresponding emotion. Hence also emotions are useful. One can discuss whether from a biological point of view intrinsic value exists at all. Of course, the viewpoint of biology is only one possible approach. At least since the Nicomachean Ethics of Aristotle a professional conversation has been conducted about values and the question whether an intrinsic value exist as a final guide of life."}
{"doc_id": 10333545, "author": "", "text": "Utility refers to a thing’s effect on the physical world, while value refers to a thing’s effect on a person’s mind. Handing a car dealer money has no effect on the car you want, but it affects the mind of the dealer, who then allows you to take the car. So in that example the money has no utility, only value. Utility is objective and value is subjective. I believe a mind is the only thing that could have intrinsic value, because it will infer a value upon itself. But it will also be judged with an extrinsic value by others. Because value is subjective, an object can’t have value, it can only be valued. But it’s utility is intrinsic and objective. At least that’s my drunken opinion on the matter."}
{"doc_id": 10322086, "author": "", "text": "Math doesn't describe a universe any more than a hammer describes a skyscraper. It's just a tool. From the mathematician's point of view, if the physicists find it useful, then good for them ... but math doesn't care one way or the other. On the other hand, the greatest of the mathematicians have been great physicists. Newton and Gauss for two. So there's some kind of symbiosis between math and physics. But math doesn't describe \"the\" universe and it also doesn't describe \"a\" universe. If I give you the number 5, that's math. If I tell you 5 miles per hour, that's physics. If I say 5 trees, that's forestry. Math is a tool. On its own terms and in isolation it applies to nothing and implies nothing. Also, math doesn't claim to be \"true.\" In the twentieth century a lot of really smart people used math to study math itself. And from a syntactic standpoint, the best you can do is agree that 1+1=2 follows from the Zermelo-Fraenkel axioms of set theory, with the usual von Neumann interpretation of the natural numbers. But is 1+1 = 2 true? It is, about the world. But in math it's only true because of the way they define the symbols '1', '+', '=', and '2'. So this is tricky business, with the usual suspects having their say, from Russell to Frege, Wittgenstein, Godel, Turing, and all the deep thinkers about what it means to sling around strings of symbols according to rules. As I understand it, I've stated the formalist position. But it seems to me that the math means nothing until you assign meanings to the symbols. It's true that math is inspired by physics. But math isn't physics."}
{"doc_id": 10322088, "author": "", "text": "So I actually really like this view on \"The difference between math and physics is that physics describes our universe, while math describes any potential universe\". Let me try to explain why he said what he did: The idea is that if you pick a physics theory, for instance take the Standard Model, the theory is not all perfect, it describes the behaviour of the strong force, the weak force and the EM. But it doesn't do that just by itself, it needs a few free parameters. These parameters are specific to the world in which the laws of physics operate in. That I think is what he means by physics describes our universe. Now if you talk specifically about the equations that describe phenomenon, they don't all use free parameters. You have to understand that a theory has to be comprehensive in it's explanatory scope, because of that it needs those free parameters. But the equations that describe something don't need the free parameters, so those equations can describe any universe, because when a theory uses them it can then in turn use different parameters."}
{"doc_id": 10322089, "author": "", "text": "What is mathematics? Mathematics is a modeling language/toolbox. What is a model? A set of entities and relationships between those entities. How do we come up with a model? Through observations. We observe a system (eg: Universe), collect data and then figure out entities and relationships. Why we need a model? So we can \"understand\" the system Can we create a model without observation? Yes, in that case you would come up with entities and relations that are not result of observing some system (although the influence could be from various observations from various systems and various existing models), you can say those things doesn't exist in real world. Any example of a model without observation? The game of Chess and many other systems that we humans have designed that weren't already there in the real world. Summary: Physics is about observing the universe we live in and try to create models of it using mathematics. You can use mathematics to create any model that describe a system which doesn't really exist aka potential universe."}
{"doc_id": 10322090, "author": "", "text": "I think what your professor meant by saying; \"The difference between math and physics is that physics describes our universe, while math describes any potential universe\" is that the study of Pure Mathematics provides a deeper and more substantial understanding of reality than physics. Im guessing that your Professor is in some sense a Platonist, as Philosophy of Mathematics is grounded in Plato's theory of the forms. Mathematics and Physics present two distinctively different ways to understand reality, and the distinction is in the different approaches to discerning what reality is in essence by discerning what it is ultimately reducible to. The Physicist will tell you its matter all the way down, that reality in essence is matter, and that all matter is ultimately reducible to a particle which is indivisible; the materialist approach. Physics will give you technical expertise, but tell you little about what reality is in essence as materialism is an assumption. The mathematician will generally describe reality in terms of the forms, that reality is ultimately reducible to principles of logic, the idealist approach. Pure Mathematics will give you a deep understanding of the nature of reality and technical expertise. I believe this distinction between the different intellectual fruits of labour for both fields was at the core of your professors arguement."}
{"doc_id": 10322110, "author": "", "text": "I don't know any physics, but I've heard that a source of some anxiety among physicists is the possibility that the physical laws which obtain in our universe are just a kind of local accident, and that the constants of nature may vary outside of our Hubble volume. This hypothesis is offered as an explanation for fine tuning, for instance, and it has been given even more credibility (I'm told) by recent findings at the LHC. This possibility has always seemed to me like a pretty good argument for math as a career path. I do worry that I am being too glib, however, and I hope any onlooking physicists will correct me."}
{"doc_id": 10322165, "author": "", "text": "The professor's statement is fairly accurate if you restrict physics to the \"current known material universe. The current \"real\" universe is described as having 4 dimensions (3 spatial, 1 temporal). In contrast, with \"pure\" math, you can have an infinite number of universes, with an infinite number of dimensions - each! With the added advantage that one of these universes is the\"real\" universe that physics studies. It is also possible that what your professor was trying to tell you, was that majoring in math would give you a \"broader\" base than majoring in physics. With a math base, you could easily continue into chemistry, biology, engineering, or any other science."}
{"doc_id": 10337669, "author": "", "text": "As with any short statement, it is possible to disagree with it. However, I think you get more benefit by being charitable and seeing what insight the fellow was talking about. I think his main point is that physics DOES pertain to our universe. Math is an abstract system. This is a very cool insight. But it isn't necessarily a point in math's favor. There tends to be some loss of connection to the real world, when you move to the abstract. Physicists get to learn about electricity and things like that that we experience. Math majors don't. Similarly many biologists LOVE their topic because what could be more interesting than learning about our bodies. But if you want to learn the abstractions more power to you. You asked for other writings and I was glad to see the V Arnold comments but surprised nobody mentioned this clip: https://www.youtube.com/watch?v=obCjODeoLVw In addition to the math versus physics, I would be attuned to where he mentions hypotheses based on bias. I.e. that it is OK to have them but realize it and don't be too prejudiced. It's a good philosophical point. Finally in the context of what is a better major, I think that a lot of that comes down to YOU, not to which major is better. Is basketball \"better\" than wrestling. Maybe. You can make actually make arguments. Not definitive, sure, but you can find factors like team sport, notability, etc. to support a stance. However, what is best for YOU will likely have a lot to do with your physique, skills, aggressiveness, enjoyment, etc. So...I definitely think that \"this is better\" field is a very low factor in your calculus. It should be more about what you enjoy, what you are good at. And also, I would be careful to consider what further training will be like and what jobs will be like. You might like freshman calculus and physics equally but find the move to proof mathematics a turnoff in the math curriculum (real analysis doesn't actually help you solve more integrals!) Feynman had a negative reaction to math, his initial major at MIT, when he asked the professors what they did with higher math in later parts of math major and the answer was teaching more people to do that in the future. At first he switched all the way to EE. Then he decided that was too applied for him and settled on physics. None of this is to argue you to physics (I'm even more applied than physics), because if you like math and are good at it, then fine. Also there are a few fields of math (real mathematicians will scoff) like statistics or operations research where you learn and perhaps even research fundamental math in the topic but there's also a pretty strong emphasis on collaboration with practitioners in the real world."}
{"doc_id": 10320496, "author": "", "text": "The difference is essentially the same as that which Gottlob Frege discusses in his seminal work On Sense and Reference. Putnam essentially uses the word meaning in place of Frege's sense, presumably because it seemed more intuitive for Putnam to splinter reference from meaning rather than make meaning a redundant term in our language. The distinction between meaning and reference is simple: meaning/sense is the intension or the word, name or symbolic representation of an object; the reference is the thing to which the intension corresponds in the world—the reference is the object to which you are referring, if you will. For instance, if you think of a battery, you have the word battery and you have symbols for batteries, such as: and yet you also have the battery as a physical object, as an actual battery in the world, an extension. You have that which you mean and your ways of meaning it. The referent and the symbol to refer to it. An interesting parallel between analytic and continental philosophy is Frege's distinction between sense and reference, which is also acknowledged in semiotics as the distinction between signifier and signified. Putnam differs from Frege in where meaning and reference sit. For Frege and others like him, meaning is a social, communicable sign/symbol and reference is in the world. For Putnam, both meaning and reference are in the world, because reference influences the formation of the symbols and names we use for meaning. Putnam sought to prove his argument with his famous Twin Earth thought experiment."}
{"doc_id": 10336290, "author": "", "text": "In \"Paris is the capital of France\", \"is\" is used to mean identity. In \"my pet is a cat\", \"is\" is used to mean predication : my pet belongs to the class of cats. See Ludwig Wittgenstein's Tractatus : 3.323 In everyday language it very frequently happens that the same word has different modes of signification—and so belongs to different symbols—or that two words that have different modes of signification are employed in propositions in what is superficially the same way. Thus the word ‘is’ figures as the copula, as a sign for identity, and as an expression for existence; ‘exist’ figures as an intransitive verb like ‘go’, and ‘identical’ as an adjective; we speak of something, but also of something’s happening."}
{"doc_id": 10336291, "author": "", "text": "'My pet is a cat' involves the 'is' of predication; you predicate - say of - your pet that it is a cat. Your pet has the attribute or property of being a cat. If you say 'The Morning Star is the Evening Star' (Frege's example) then this is the 'is' of identity : the Morning Star and the Evening Star are one and the same object. Put the point like this. The property of being a cat is not identical with the property of being your pet since your pet might equally well be something different - an alligator instead. In contrast, the Morning Star can never be different from the Evening Star : they are identical, one and the same thing with (as it happens) different names."}
{"doc_id": 10336292, "author": "", "text": "The difference between the \"is\" of identity and the \"is\" of predication is the difference between being an object and being the property of an object. As Graham Priest puts it (page 84) We must distinguish between an object and its properties. When we say that you, with a different hair-style, are different, we are saying that you have different properties. It does not follow that you are literally a different person, in the way that I am a different person from you. The reason this can be confusing is the English word \"is\" can be used to refer to both objects and properties of objects. When I refer to the same object saying my cat is Kiki, where Kiki is the name of my cat, this is the \"is\" of identity. When I am talking about a property of the object and claim that my pet is a cat, this is the \"is\" of predication. The property of being a cat is a property of the object, my pet. Reference Priest, G. (2010). Logic: A brief insight."}
{"doc_id": 10336294, "author": "", "text": "The idea that there is a fundamental difference is known as the Frege-Russell \"is\" ambiguity thesis, Corazzon's webpage is a very good source on it. In addition to the is-es of predication and identity they distinguished the is of existence, and the is of subsumption. However, from Aristotle to Frege philosophers did not draw such distinctions. The need for them is transparently linked to the needs of formalizing logic into predicate calculus with quantifiers, where predication and identity are expressed by different means, P(x) vs x=y. There are even more options in set theory, where one can talk of inclusion of classes defined by predicates, and use \"is\" for that. There is no general fact of the matter as to what is/are \"really\" stand for, it depends on context and how one wishes to interpret the claim when formalizing. Here is from Hintikka's Meinong in a Long Perspective: \"This history involves more changes and contrasts than one might perhaps expect. For one thing, we twentieth-century philosophers are wont to approach the notion of being by means of the Frege-Russell ambiguity thesis. As we all know this thesis concerns the notion of being, as codified in verbs for being in languages like the English is, German ist or the ancient Greek estin. What it asserts is that these verbs are multiply ambiguous. We have to distinguish (according to this thesis) from each other the ises of existence, identity, predication and subsumption. Indeed we are in fact supposed to have learned to distinguish them from each other in practice, for we have all been taught to use first-order logic as our canonical notation in logic and logical analysis. It is in order for me to emphasize the word ambiguity here. Every half-way sensitive analyst (or perhaps I should say, every sensible analyst) will grant that verbs for being like the English is are used in different ways on different occasions. What the Frege-Russell thesis does, is to blame these differences in use on the ambiguity of a single word, instead of explaining the difference in use away in some other way, for instance by reference to the context of use.\" Aristotle considered the distinction and rejected it. He followed Greek grammar in treating \"is\" (ἐστιν) always as a copula, i.e. always as means of predication. The other \"meanings\" were treated in terms of force, existential, etc., which may or may not be resent in specific uses of ἐστιν. Medieval scholastics followed Aristotle. In his syllogistic there is no distinction between predication, existence and identity because it does not have means (or need) to express such a distinction. \"My cat is a pet\" is in a typical form of syllogistic premise, but it makes no difference whether this attaches predicate to a subject (Aristotle's preference) or subsumes the pet under the concept of a cat (we have no identity here). The same with \"all cats are animals\" or \"bachelors are unmarried man\" (here we do have identity). In fact, the extension/intension (class/predicate) ambiguity was common in 19th century logic and mathematics and persisted until extensionalization of set theory by Hausdorff. But the unraveling started earlier with Kant, who famously declared \"'Being' is obviously not a real predicate; that is, it is not a concept of something which could be added to the concept of a thing\". Frege and Russell developed this into the use of the existential quantifier, which expresses existence without a predicate. The move is now controversial, see What are the counterexamples to Kant's argument that existence is not a predicate?"}
{"doc_id": 10317513, "author": "", "text": "Religious belief is based on the teaching of a trusted authority -- a church, an ancient teacher or sage, a cultural tradition. Scientific belief is based on experimentation to confirm or falsify hypotheses. In practice, beliefs that are considered \"scientific\" are often actually religious in the sense that I accept what I am told by my science teacher without actually performing the experiment to see the result for myself. So my belief is based on the authority of my science teacher, not my own experience."}
{"doc_id": 10317514, "author": "", "text": "\"Science has proof without any certainty. Creationists have certainty without any proof.\" This half-joking quote from Ashley Montague highlights for me the critical difference between religious belief and scientific belief: the beliefs of the religious are effectively unquestionable, whereas scientific beliefs can generally only be accepted as legitimate if they are questionable. In other words, at some core level religious beliefs seem to require a \"leap of faith\", belief without logical reason or justification, whereas all \"beliefs\" or theories of science can only be said to be valid if they are—on some level—questionable (i.e. falsifiable). Scientific beliefs are never held in any light such that they become beyond reproach, and the scientific method is all about testing and retesting theories, modifying our understanding of the world around us. In contrast, the religious doctrines of the major world religions are largely fixed, the sacred texts are static and unchanging — at best, believer's views and interpretations of the doctrines have changed over time rather than the actual texts themselves."}
{"doc_id": 10317525, "author": "", "text": "Science is a field of study where statement of theories are made, in which there are probabilities or possibilities. These statements can be proven or disproven on the basis of controlled experiments. Religion is a field of beliefs in which statements of certainties are made. These statements are typically not questionable, because they are not subject to proofs or disproofs. Depending on ones inclination and faith, one may only argue that any statement within religion is inherently true, or untrue. Fallacies are beliefs in theories that are not questionable due to how the theory is defined."}
{"doc_id": 10317549, "author": "", "text": "The key differences are as follows: Religion has a conclusion (i.e. deity, after life, creation, etc.) and then finds evidence to support those conclusions. This is most evident by the theological fallacy \"God is in the Gaps\". Science on the other hand has evidence (i.e. an object falls at a contestant rate regardless of weight) and then draws a conclusion from it: Newton's theory of gravity, which was later changed by Einstein and his general theory of relativity."}
{"doc_id": 10319162, "author": "", "text": "While most religious belief systems affirm the existence of certain things, the belief system of (natural) science tends to deny the existence of certain \"not-reproducible\" things. Why would the mere fact that you cannot reproduce something lead you deny a thing existence? I cannot reproduce a Boeing 747. Does this now somehow call into question the existence of such a thing? Nope! Let's look at two examples to make this question more concrete. There are many examples in fiction of interactions of no longer living persons with living persons. But suppose this would happen to you in \"real life\". Wouldn't you try to explain it away (or at least keep it for yourself)? Perhaps you manage to settle for a \"I don't know\" position. Why would you think they did not doubt it Thomas certainly did doubt it. I do not think it was harder for any Apsotle to believe it than what it would be for any humans today. But how clear is the distinction between the two really? It is clear that the domain of expertise of a priest is quite different from the domain of expertise of a scientist. But does that really stop the corresponding belief system from making assertions about things outside of this domain of expertise? (And what is the position of philosophy here? It seems to have an opinion on both science and religion. But is it more part of (natural) science, or more some sort of \"impartial\" observer?) Now this to me is a very interesting question. It was a question that plagued a certain person called Stephen Jay Gould and he got famous in academic circles for the answer he proposed to this very question. He posited that both science and religion have their own valid teaching domain or authority. As long as they are both within the bounds of their own \"teaching domains\" then their should be no conflict between the two. He coined the term Non-overlapping Magisteria or NOMA in reference to this. Now personally I hold to POMA or partially overlapping magisteria. Their are some discoveries in science that hold great discussion in religious circles. If you take the discovery of background radiation for instance. This in essence began the road to a finite universe which was remarkable and religiously relevant discovery. It also sparked a resurgence in Aquinian thought which was for centuries held to be disproved by physicist. Their are some who even hold to a completely overlapping magisteria (Coma) like Richard Dawkins for example. He believes erroneously (to me at least) that the God hypothesis is a scientific hypothesis (As he explains in his books) and you should use the scientific method to ascertain it's validity. You can read more about it here. http://en.wikipedia.org/wiki/Non-overlapping_magisteria"}
{"doc_id": 10333412, "author": "", "text": "While most religious belief systems affirm the existence of certain things, the belief system of (natural) science tends to deny the existence of certain \"not-reproducible\" things. I don't think this is true. Science - in the limited scope you meant here - only deals with things that can be demonstrated by reproducible experiments. It can't deal with what is, by design or nature, not subject to such requirements. Are there invisible fairies in my garden? We cannot know, because the claim is unverifiable by design: the fairies are invisible, and so we cannot see them. Can we communicate with the dead, if we have faith? We can't know, bucause the claim is unverifiable by nature: if we were to make an experiment, we would already lack the necessary faith. It doesn't follow that those things don't exist. It follows that science cannot reach to conclusions regarding them. Collaterally, there is a \"belief\", which can not experimented in a \"scientific\", reproductible way, that things that have inbuilt unverificability clauses, such as invisible fairies or any phenomenon dependent on faith, are either not important, or so rare that they cannot be accounted for in any meaningful way. On the other hand, scientists seem to believe in the Big Bang, the existence of dinossaurs, or dark matter, all of which are unsusceptible of repeatable experimentation. So, while repeatable experimentation is certainly an important part of science, it cannot be all that is in science. Let's look at two examples to make this question more concrete. There are many examples in fiction of interactions of no longer living persons with living persons. But suppose this would happen to you in \"real life\". Wouldn't you try to explain it away (or at least keep it for yourself)? Perhaps you manage to settle for a \"I don't know\" position. There are many real life accounts of interaction with the dead. But, in short, the problem is that either whatever information the dead bring to us can be checked by the living, or it cannot be checked. In the first case, the dead can usually be shaved off by Ockham's razor - they are unnecessary to explain how such information was obtained. In the second case, we cannot know if such information is actual information, or just a fantasy. The dead could, of course, clear up the issue by informing me the numbers of the incoming lotto - but then it seems they have a strict (and convenient) moral code that forbids them from doing that. And when they otherwise talk about the future, they tend to adopt a contorted (Nostradamic) style that makes it impossible to understand what they are predicting. Now let's look at the different belief systems of (natural) science and religion. Is there any need to fit this into a scientific belief system? Isn't (natural) science concerned with \"reproducible\" things, so that science doesn't even need to bother whether certain \"non-reproducible\" things are \"real\" or not? It cannot be so indifferent to such \"non-reproducible\" things, as they will contaminate the reasoning and result in non-verifiable hypotheses. And so, they must be purged. Can the same position also be used for religious belief systems? Probably not, because this touches the kind of questions that religious belief systems are concerned with. But what about the opposite case, for example heliocentricism? That's a scientific theory (or fact) after all, so it shouldn't worry religious belief systems too much. But apparently it did. It did, but that due to certain particularities of the Abrahamic faiths, which imply the inerrancy (at a least to some level) of given, dated, sacred texts - sacred texts that make several assertions about existing or past states of the universe. It is worse for literalists, such as a few fundamentalist Protestant sects, and less of a problem for non-literalists, such as the Catholic ortodoxy, or rabinic Judaism, that can always fall back into an alegorical interpretation (yes, the Bible says God created the world in six days, but who says that a billion years is not just a day for God?) From Philosopher3's answer: Religion has a conclusion (i.e. deity, after life, creation, etc.) and then finds evidence to support those conclusions. [...] Science on the other hand has evidence (i.e. an object falls at a contestant rate regardless of weight) and then draws a conclusion from it. It is not like that. Science starts with conclusions (theories) and only then seeks for evidence. The difference is, science seeks for evidence that contradicts its \"conclusions\" (ie, theories), and only sticks to conclusions that resist such trial. Religion does not seek evidence at all, and tends to reject (or misinterpret) evidence that contradicts its conclusions."}
{"doc_id": 10333413, "author": "", "text": "The most fundamental difference is that religions believe that the mind (in its wide meaning) is the source of creation, the source of all phenomenons, while science believes that their is no mind and that matter itself is the source of everything we see. This is why both dogmas have always been in opposition. While you can stumble upon genuine scientists who seek the truth, the scientific method does not prevent science and scientists from following an ideology that originates from the Enlightenment (century of lights), as shows the persistent paradigms in every fields. Indeed, pick any officially accepted theory and you will notice that it explains a phenomenon by matter being the source and systematically rejects a possible implication of the mind. For details see the work of Rupert Sheldrake https://www.youtube.com/watch?v=JKHUaNAxsTg"}
{"doc_id": 10333464, "author": "", "text": "Let's start with the things they have in common on a conceptual basis: Both Science and Religion have an unfounded a-priori assumption. In Religion this is the belief that the Divine has manifested itself in some discernable way (otherway how would we know about it). In Science this is the assumption that the Scientific Method can be used to make usefull claims about the world. The second might seem more self-evident, living in this technology filled world, but there is no way to prove that this effect is intrinsic and not just happenstance. And what do they have in common in practice? While Science as a concept is fairly different from Religion, in actual practice there are some overlaps. The first and foremost is the concept of Scientism, the \"believe in Science\". At it's most extreme this means a complete rejection of anything that can't be proven by the scientific method. As mentioned earlier, there is nothing aside from common human experience that proves that the scientific method is correct. While this is certainly is enough to make a case for following this stance, it is strictly speaking nothing more than a religion. In addition there is a certain amount of reliance on authority, to overcome the limitations of the human lifespan and lack of resources: While in theory, one could repeat all experiments that are the basis for our scientific knowledge, in practice we have to rely on established institutions, and trust that these institutions have done the work for us. That is to say, that both religion and science rely on authority, but the sources of the trust placed in these authorities differs (or should differ at the very least). Now where do they differ on a conceptual basis? The on thing that Science claims for itself that sets it apart from Religion is repeatability. Anyone can repeat any of the experiments upon which our scientific knowledge is based. One could draw paralels between the inability to reproduce science, and the requirements other Religions place on their believers (e.g. \"you can't hear god because you're not pious enough\", \"god only speaks to the chosen ones\"). The difference is, that the factors that prevent people from repeating scientific experiments are not build into Science itself. And what seperates them in practice? First and foremost, science has a direct and concrete effect on human lives, even if we ignore the secondary effects caused by belief in science. Religions do claim the same but even if we assume that all reported Miracles are actually true, only a negligible ammount of people seems to be affected. On average human experience seems to say that science works, while faith doesn't, to the point where most popular religions concentrate on intagible effects (e.g. rewards in the afterlife, faith gives hope, faith is the basis of morality). There also seems to be a difference in how scientists and religious people approach arguments. While science is much more concerned with the results of it's base assumption, religions seems to have a much bigger focus on proving their base assumption. Again this is because Science proves it's own worth by delivering results, while Religion, due to the lack of concrete (physical) results, has to prove that it comes from a solid foundation. To summarize: Science doesn't have to be true, it only has to work such that everyone can see it's usefullness. Religion has to be true because it's usefullness isn't readily apparent."}
{"doc_id": 10323118, "author": "", "text": "Ethics deal with a set of rules governing conduct that a group of people have agreed upon, while morals deal with an individual's sense of right and wrong. They are intertwined in that each may inherit from the other. However, one may have a \"moral dilemma\" when their own sense of morals is at conflict with the ethics of their group. For example, let's say you are a cashier at a grocery store. You agreed at your hiring not to let customers steal. The act of doing that, therefore, is unethical. However, you notice a homeless family wander in and the children are hungry. They take a loaf of bread and some milk and leave without paying. You believe it would be immoral not to let them have the food they need. Your sense of morals and ethics are now at a direct conflict."}
{"doc_id": 10331623, "author": "", "text": "They're related, rather than being a 'subset' or a superset; take for example, Descartes cogito: I think therefore I am In which the subjective element is related to the objective."}
{"doc_id": 10331631, "author": "", "text": "are objectivity and subjectivity mutually exclusive, or is the objective a subset of the subjective? I would agree with the second option (which I believe aligns with the intuition underlining your question) ... I would also regard it as a failing of philosophy, if in general philosophers are unable to answer clearly what they believe about these questions. Testing the reliability of our current models through such questions as these, I think, is precisely the life-blood of philosophy as a practice of discovering what we think we know about the world, and how we think we know it. The bias towards privileging the value of \"objective\" information over \"subjective\" information, I think comes from modelling philosophy on science, at the expense of not seeing how philosophy is as much like art as it is like science - also, culturally, it seems that science is regarded as an inherently more reliable source of \"truth\" than art - yet, as you have pointed out, objective knowledge could be regarded as a subset of subjective knowledge, which has apparently passed a threshold level of reliability."}
{"doc_id": 10331634, "author": "", "text": "This is a very tricky issue. As you say, the words are often used sloppily or in differing ways. You're right to say that all objective facts are subjective, and one strategy to allow for this is to use the word 'inter-subjective'. People sometimes use 'objective fact' to mean a fact that is inter-subjective, verified by a shared subjective experience. It is hard to see how an observation can be an objective fact. A complication is that a major school of thought denies the reality of the distinction between the objective and the subjective. Schopenhauer for instance, speaks of his 'better consciousness', a level of awareness for which the subject/object dichotomy evaporates to be seen as a perceptual error. This is the common experience of those who meditate. I feel that you're right and that the issues are muddled in much of philosophy. It's not that any one philosopher is muddled, but that the words are used in varying ways and with varying degrees of care."}
